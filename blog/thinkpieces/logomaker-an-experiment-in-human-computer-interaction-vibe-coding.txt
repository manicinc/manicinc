3:I[4707,[],""]
6:I[36423,[],""]
a:I[6322,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
b:I[96313,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
c:I[66159,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
d:I[59970,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
e:I[81775,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
f:I[12025,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"ThemeProvider"]
10:I[39976,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"CookieProvider"]
11:I[69088,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
12:I[50513,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
13:I[83551,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
14:I[38483,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
15:I[81695,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
16:I[28602,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
17:I[51052,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"Nav"]
18:I[10376,["7601","static/chunks/app/error-980f98eab7299665.js"],"default"]
19:I[79229,["9160","static/chunks/app/not-found-e731a727afb82058.js"],"default"]
1a:I[85745,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
1b:I[16049,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
1c:I[18133,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
1d:I[36623,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
1e:I[69709,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","3185","static/chunks/app/layout-4fa2505e357b421a.js"],"default"]
4:["category","thinkpieces","d"]
5:["slug","logomaker-an-experiment-in-human-computer-interaction-vibe-coding","d"]
7:Tb3b,
          /* Critical CSS - Inline in <head> for fast initial paint */
          *,*::before,*::after{box-sizing:border-box;margin:0;padding:0}:root{--bg-primary:#fbf6ef;--bg-primary-rgb:251,246,239;--text-primary:#4a3f35;--text-primary-rgb:74,63,53;--accent-primary:#d6a574;--accent-highlight:#7de8c9;--header-height:72px;--container-max:1200px;--content-max:900px;--font-body:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;--font-heading:var(--font-body);--transition-fast:150ms ease;--transition-base:250ms ease}html.dark{--bg-primary:#22182b;--bg-primary-rgb:34,24,43;--text-primary:#f5f0e6;--text-primary-rgb:245,240,230;--accent-primary:#e4b584;--accent-highlight:#7de8c9}html:not([data-theme-loaded="true"]) body{opacity:0}html{background-color:var(--bg-primary);color:var(--text-primary);font-family:var(--font-body);line-height:1.6;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}body{margin:0;min-height:100vh;transition:opacity 300ms ease}nav{position:sticky;top:0;z-index:100;background:rgba(var(--bg-primary-rgb),0.95);backdrop-filter:blur(10px);-webkit-backdrop-filter:blur(10px);height:var(--header-height);border-bottom:1px solid rgba(var(--text-primary-rgb),0.1)}.container{max-width:var(--container-max);margin:0 auto;padding:0 1rem}.hero-section{padding:4rem 1rem;min-height:calc(100vh - var(--header-height));display:flex;align-items:center}h1{font-size:clamp(2rem,5vw,4rem);font-weight:700;line-height:1.1;margin-bottom:1rem}h2{font-size:clamp(1.5rem,4vw,2.5rem);font-weight:600;line-height:1.2;margin-bottom:0.75rem}p{margin-bottom:1rem;line-height:1.6}a{color:var(--accent-primary);text-decoration:none;transition:color var(--transition-fast)}a:hover{color:var(--accent-highlight)}.btn{display:inline-flex;align-items:center;gap:0.5rem;padding:0.75rem 1.5rem;background:var(--accent-primary);color:var(--bg-primary);border:none;border-radius:0.5rem;font-weight:500;cursor:pointer;transition:all var(--transition-base)}.btn:hover{background:var(--accent-highlight);transform:translateY(-2px)}.skeleton{background:linear-gradient(90deg,rgba(var(--text-primary-rgb),0.1) 25%,rgba(var(--text-primary-rgb),0.2) 50%,rgba(var(--text-primary-rgb),0.1) 75%);background-size:200% 100%;animation:loading 1.5s ease-in-out infinite;border-radius:0.25rem}@keyframes loading{0%{background-position:200% 0}100%{background-position:-200% 0}}img{max-width:100%;height:auto;display:block}img[loading="lazy"]{background:rgba(var(--text-primary-rgb),0.1)}@media (max-width:768px){.hero-section{padding:2rem 1rem}h1{font-size:2rem}.hide-mobile{display:none}}@media (min-width:769px){.hide-desktop{display:none}}.will-change-transform{will-change:transform}@media (prefers-reduced-motion:reduce){*,*::before,*::after{animation-duration:0.01ms!important;animation-iteration-count:1!important;transition-duration:0.01ms!important}}
        8:T6fb,default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://www.googletagmanager.com https://www.google.com https://www.gstatic.com https://www.clarity.ms https://*.clarity.ms https://cdn.sender.net https://app.sender.net https://api.sender.net *.sender.net https://vercel.live https://*.vercel.app https://cdnjs.cloudflare.com https://ajax.cloudflare.com https://va.vercel-scripts.com https://*.vercel-scripts.com https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com https://static.cloudflareinsights.com; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com https://www.gstatic.com https://cdn.sender.net https://cdnjs.cloudflare.com https://eocampaign1.com https://*.eocampaign1.com; img-src 'self' data: https: blob: https://www.google.com https://www.gstatic.com https://cdn.sender.net https://app.sender.net https://eocampaign1.com https://*.eocampaign1.com; font-src 'self' https://fonts.gstatic.com https://www.gstatic.com https://cdn.sender.net https://cdnjs.cloudflare.com https://eocampaign1.com; connect-src 'self' https://www.google-analytics.com https://www.google.com https://www.gstatic.com https://api.github.com https://*.github.com https://cdn.sender.net https://app.sender.net https://api.sender.net *.sender.net https://vercel.live https://cloudflare.com https://*.cloudflare.com https://va.vercel-scripts.com https://*.vercel-scripts.com https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com https://static.cloudflareinsights.com; frame-src 'self' https://www.google.com https://www.gstatic.com https://cdn.sender.net https://app.sender.net https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com;9:Taf1,
        (function() {
          try {
            // Don't run this script during server-side rendering
            if (typeof window === 'undefined' || typeof document === 'undefined') return;
            
            // 1. Check localStorage - the source of truth for user preference
            let storedTheme = localStorage.getItem('theme');
            
            // 2. If no stored theme, check system preference
            if (!storedTheme) {
              const systemPrefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
              storedTheme = systemPrefersDark ? 'dark' : 'light';
              // Save this to localStorage for next time
              localStorage.setItem('theme', storedTheme);
            }
            
            // Wait for DOM to be ready
            const applyTheme = () => {
              // Safety check that DOM is ready
              if (!document || !document.documentElement) return;
              
              // 3. Ensure clean state
              document.documentElement.classList.remove('dark', 'light');
              
              // 4. Apply theme class and colorScheme
              document.documentElement.classList.add(storedTheme);
              document.documentElement.style.colorScheme = storedTheme;
              
              // 5. Apply immediate colors to prevent flash - only to html element
              if (storedTheme === 'dark') {
                document.documentElement.style.setProperty('background-color', '#22182b', 'important');
                document.documentElement.style.setProperty('color', '#f5f0e6', 'important');
              } else {
                document.documentElement.style.setProperty('background-color', '#fbf6ef', 'important');
                document.documentElement.style.setProperty('color', '#4a3f35', 'important');
              }
              
              // 6. Store for React
              window.__NEXT_THEME_INITIAL = storedTheme;
            };
            
            // Apply theme immediately
            applyTheme();
            
            // Also apply after DOM is fully loaded (for safety)
            if (document.readyState === 'loading') {
              document.addEventListener('DOMContentLoaded', applyTheme);
            }
            
          } catch (e) {
            console.error('Theme initialization error:', e);
            // Fallback to light - only set on html element
            if (document && document.documentElement) {
              document.documentElement.classList.add('light');
              document.documentElement.style.setProperty('background-color', '#fbf6ef', 'important');
              document.documentElement.style.setProperty('color', '#4a3f35', 'important');
            }
          }
        })();
      0:["manic-agency-1768692621413",[[["",{"children":["blog",{"children":[["category","thinkpieces","d"],{"children":[["slug","logomaker-an-experiment-in-human-computer-interaction-vibe-coding","d"],{"children":["__PAGE__?{\"category\":\"thinkpieces\",\"slug\":\"logomaker-an-experiment-in-human-computer-interaction-vibe-coding\"}",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["category","thinkpieces","d"],{"children":[["slug","logomaker-an-experiment-in-human-computer-interaction-vibe-coding","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7589854758514563.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/2a1fdc91e2d69a25.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/6517f724e0c34e4b.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","3",{"rel":"stylesheet","href":"/_next/static/css/4a49f8c87e29773c.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","4",{"rel":"stylesheet","href":"/_next/static/css/fbcf5add168be5ca.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","5",{"rel":"stylesheet","href":"/_next/static/css/31a0afff5523f0ee.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","6",{"rel":"stylesheet","href":"/_next/static/css/94e7d866c9e42adb.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","7",{"rel":"stylesheet","href":"/_next/static/css/8d146102ec06a500.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"\n            __variable_f367f3\n            __variable_1c86d0\n            __variable_fcc734\n        ","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":""}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"preconnect","href":"https://eocampaign1.com"}],["$","link",null,{"rel":"preconnect","href":"https://images.weserv.nl"}],["$","link",null,{"rel":"preconnect","href":"https://static.cloudflareinsights.com"}],["$","link",null,{"rel":"dns-prefetch","href":"https://cdn.sender.net"}],["$","link",null,{"rel":"dns-prefetch","href":"https://api.github.com"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.clarity.ms"}],["$","link",null,{"rel":"dns-prefetch","href":"https://cdn.jsdelivr.net"}],["$","link",null,{"rel":"dns-prefetch","href":"https://res.cloudinary.com"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"$7"}}],["$","meta",null,{"httpEquiv":"Content-Security-Policy","content":"$8"}],[["$","meta",null,{"name":"cf-visitor","content":"{\"scheme\":\"https\"}"}],["$","meta",null,{"httpEquiv":"X-Forwarded-Proto","content":"https"}]],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]]}],["$","body",null,{"children":[["$","$La",null,{}],["$","$Lb",null,{}],["$","$Lc",null,{}],["$","$Ld",null,{"fallback":["$","$Le",null,{}],"children":["$","$Lf",null,{"children":["$","$L10",null,{"children":[["$","$L11",null,{}],["$","$L12",null,{}],["$","$L13",null,{}],["$","$L14",null,{}],["$","$L15",null,{}],["$","$L16",null,{}],["$","$L17",null,{}],["$","main",null,{"role":"main","id":"main-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$18","errorStyles":[],"errorScripts":[],"template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$L19",null,{}],"notFoundStyles":[]}]}],["$","$L1a",null,{}],["$","$L1b",null,{}],["$","$L1c",null,{}],["$","$L1d",null,{}],["$","$L1e",null,{}]]}]}]}]]}]]}]],null],null],["$L1f",null]]]]
20:I[54680,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
21:I[18745,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
22:I[94058,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
24:I[67373,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
26:I[12554,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
27:I[96670,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
28:I[71409,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
29:I[87634,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
33:I[72972,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],""]
34:I[50301,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"IconArrowLeft"]
35:I[50301,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"IconOrnateAuthor"]
36:I[50301,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"IconOrnateCalendar"]
37:I[50301,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"IconOrnateClock"]
38:I[50301,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"IconOrnateTag"]
39:I[65878,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"Image"]
3a:I[30603,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"CustomMarkdownRenderer"]
3c:I[74644,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
3d:I[75541,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
41:I[28054,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
42:I[4681,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
43:I[99775,["9464","static/chunks/framer-motion-afa3e9eea9dc4f5c.js","8520","static/chunks/lucide-icons-8c8ddd324b6948be.js","2009","static/chunks/markdown-648e077ccebbd300.js","2793","static/chunks/katex-3086009fe82370df.js","3790","static/chunks/forms-9ad27fff80457a29.js","3178","static/chunks/common-f3956634-e92c5a01f0a0e059.js","4592","static/chunks/common-cebde746-d1fa50c34533a0b6.js","7747","static/chunks/common-91059ad5-b796dc0386e4dbe9.js","5362","static/chunks/common-b35596cd-f50eb9de6889daa5.js","2681","static/chunks/common-d87c119a-e61082fd20a6bf4e.js","3902","static/chunks/common-c899ba7b-9f13a664dcce1b0c.js","2819","static/chunks/app/blog/%5Bcategory%5D/%5Bslug%5D/page-70041129a9654d33.js"],"default"]
23:Td35b,
**GitHub link: [https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)**

**LLMS tested (GPT-4o, GPT-4.5, GPTâ€“o1, Claude Sonnet 3.7, Gemini 2.5 Pro), default settings, 20$ / monthly plans.** (No extended thinking, deep research, web search experimental plugins or memories used. Written in VS Code (not Cursor) with Copilot solving single line bugs).

*(an experiment in title)*

## Intro

For the TL;DR go down [here](/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding#tldr) to see a list of useful lessons.

Time of writing I'm in software for the upper half yet still far decade. Comes a sly shock the necessity to say as there're non-junior engineers (mid-level, working for multiple *years*) who might've got away with never handwriting a class, function or even LOC with no gen AI.
 
A couple weeks back working on PortaPack ([https://github.com/manicinc/portapack](https://github.com/manicinc/portapack)) I wanted to try logo designs and typefaces before a branding decision with the rest of the small team. We all have other projects and roles, so, the need for rapid prototyping.

![The final version of the PortaPack logo, graphical|size=small|caption=The final version of the PortaPack logo](/assets/blog/thinkpieces/logomaker-an-experiment/portapack-logo.png)

I wanted cute and whimsical and got *brutal* hoping to get started online quickly; recs for free sites in top threads linked to paywalls, subscriptions behind dark patterns, like credit card info in the last step, or indenturing export quality to a unusable amount. Posts from a year back link to sites now living totally different experiences. Yeah updates are expected but this just didn't work for me, not freely.

In all capitalistic industries but especially software what often were useful products become more privatized. 

:::banner
Increased enhancements yield stricter access controls.
:::

We can't blame them. Server hosting even a year or two gets costly. How does a free tool that has use and traffic stay free?

BUT it is too common to lock in users and not be transparent on imposed limits. We see the ~manipulation~ ~misdirection~ means in login screens and app usability taking second precedence over signup windows, in hard-to-reach payment cancellation screens.

![Maddened by paygates|size=small|caption=Maddened by paygates](/assets/blog/thinkpieces/logomaker-an-experiment/input-noise-locked-behind-paywalls.png)

Things that drive users to annoyance and away from them.

So annoyance, wanting a quick (if dirty) UX, and a stir to see what'd happen drove me to pitch: Vibe code everything, full-stack and fully usableâ€”every function written by an LLM, every design by an LLM. Nice 'n' easy quick 'n' dirty, this is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.

> *This is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.*

I knew I could get a playground for different fonts showing me text options in probably an hour, even minutes depending on the model, prompt, and complexity. Maybe 1ï¸âƒ£ prompt?

LogomakerðŸŒˆ has good scope. Not fintech, not healthcare, worst you waste time in a broken site with no ads and no data tracking. Who's Q/Aing this stuff? Logomaker, the app built 90% by ChatGPT? **It's Q/Aed by no one, use with peril.**

![An example logo created with Logomaker|caption=An example logo created with Logomaker](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png)
  

##  LLM sees, LLM does

I've a background in going to an art and design college. Art (even just visual art) is so encompassing logo designs I never specifically studied. I have Photoshop and Illustrator experience, but how they worked didn't interest me much. The features you see in the first Logomaker version weren't asked by me originally but designed by the LLM. Later on I refined and "architected" greater functionalities.

*The iterative PM-esque process in product-driven prompts with technical-guided ones is needed to be worthy of use by a human in 2025.*

On their own the LLMs from Anthropic (Sonnet 3.7), OpenAI (GPT-4o, GPT-o1, GPT-4.5), and Google (Gemini 2.5 Pro), all of which were extensively tested and âœ¨vibe coded âœ¨ with, could go just so far in self-improvement. You can't really keep asking a LLM to improve something for robustness or better UX and see better results after further than a few prompts, irrespective of token limits.

:::banner
Without human guidance, mapping out sensible, robust user flows the way humans want to use software is more difficult for LLMs than complex algorithms.
:::

Is this a limitation of something like a creativity mechanism? Rigidly speaking there *is* no such thing in them. They predict next probable tokens in a sequence with a lot of parametrization so it's not *always* the same ouput. I mean thinking and creativity in a more abstract sense, which is easy to imagine (heh) as these mental structures are wildly [malleable in humans](https://www.simplypsychology.org/sapir-whorf-hypothesis.html) anyway. Or if you're more interested in a [philosphical](https://www.newyorker.com/magazine/2023/11/13/determined-a-science-of-life-without-free-will-robert-sapolsky-book-review) discourse.

How can we be so sure *anything* we think is an [original](https://en.wikipedia.org/wiki/Simpsons_Already_Did_It) idea? 

![ The Simpsons Did It|size=large|](/assets/blog/thinkpieces/logomaker-an-experiment/simpsons-did-it.jpg)

How can we be sure of our *identities* when the pseudoscientific *You are the average of five people around you* is commonly repeated it's [dozens of pages](https://www.google.com/search?q=you+are+the+average+of+the+five+people) of Google results?

*Or* is it natural consequence of LLMs' training? What happens if we get 10,000 product designers to write 10,000 user stories each? (100 billion user stories! ðŸ’£ This'd entail in a model that almost as large or in the ballpark of GPT-3). Good software now?
  
![Can we build it, LLMs?|size=medium|caption=Lost in Wonder-LLM-land](/assets/blog/thinkpieces/logomaker-an-experiment/alice-in-wonderland-using-tool-building.png)

LLMs of course know what basic features go in a logo creator. We will see export options was done (and fully working from the LLM writing the exact dependency link needed from the CDN link for `html2canvas.js`, latest SHA hash intact and all) with multiple settings, though it was basic and didn't include SVG (which are complex, so it makes sense it's originally ignored unless prompted, as we asked for something *working* not *advanced*).

I didn't ask for specific types. We were writing this in `JavaScript`, adding types and interfaces would slow development down 2x (at start).

It's then simple to expand, and ask in the next prompt for additional exporting options of GIF and SVG. But if I didn't tell them to design adding new features in a way that, say, actively *considered* the UX with examples even, it would probably just give me a modal to render a GIF, SVG, and PNG, but all 3 as just buttons with working functionality and no additional `flourishðŸª„ðŸ’¥`. Tooltips, mobile responsive styles, sure, it won't go far beyond though, and there's *lots* of different paths needed for these formats to again actually be usable (by a semi-serious user).

| **Format** | **Best For**                          | **Web Quality**      | **Animation Support**       | **Scalability**         | **Styling Flexibility**       |
|------------|----------------------------------------|----------------------|-----------------------------|-------------------------|-------------------------------|
| **PNG**    | Static images, transparency            | âœ… Very High          | âŒ None                     | âŒ Not scalable          | âœ… Easy via container styles  |
| **GIF**    | Simple animations, loops, previews     | âš ï¸ Limited (256 colors) | âœ… Basic frame animation   | âŒ Not scalable          | âŒ Very limited               |
| **SVG**    | Logos, icons, responsive UI elements   | âœ…âœ…âœ… Excellent         | âœ… With CSS/JS or SMIL      | âœ… Infinitely scalable    | âš ï¸ Advanced, but powerful. Very difficult.     |

LLMs **"like"** to be conservative in generations. In coding, that's not good when you're getting incomplete scripts, or, in many cases, placeholder logic sneaking in even when instructed *aggressively* not to (keep this in mind down the line; is this a *side effect* of their architectures, or a *condition* by their providers?).

How can you guide a LLM to think about things like this, not just *understanding nuances*, but how to act accordingly? Only, it has to be.. *without specifically listing that **in** example(s) form*? 

**The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.

> **The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.
---------------------------------

Say you need consistent JSON `({"Name", "Date", "Topic", "Location"})` parsed from some informal voice notes, and you use OpenAI. 

In **zero-shot**, feeding "Last meet I had a sync with Jordan Thursday regarding new designs in the break room" from a recording might give inconsistent JSON like `{"attendee": "Jordan", "subject": "new designs", ...}`. The model guesses the format on prior patterns of scraped text data and metadata (which OpenAI scraped online sources, like Reddit, Twitter..), of course it's likely to get things twisted!

It's a *natural* limitation in LLMs. They get smarter with more training data, and that leaves more chances at capturing spam and noise (as well asmixing things up in their internal "reasoning"), causing hallucinations. In the case above, we will almost certainly get lowercased keys (instead of the capitalized ones as requested, or commonly used synonyms for those keys, *sometimes*, during interactions). So tech integrated around LLMs have to become more rigid to make up for their inflexibility.

You can give one example (**one-shot learning**) showing the input note -> desired JSON structure (here it'd be the input text and: `{"Name": "Jordan", "Date": "Thursday", "Topic": "New designs", "Location": "Break room"}`). A clear template to follow.

Add a few diverse examples (**few-shot learning**), more variations and how you want to handle them (like missing locations -> `Location": null`, **or** even metadata that can be auto-generated based on dynamic inputs, making *fuller* usage of the power of GPTs over typical transformer models), and this further helps the model give you what you want.

This powers [function calling](https://platform.openai.com/docs/guides/function-calling) and typing libraries for LLMs, which combine these learning examples with continual validation and retry hooks. *If this feels hackey*, that's cause **it is**, and it is worrying as we see more APIs and tools assembled solely around prompt calls.

**The rub?**

>Showing an LLM how you want something done with guided examples just makes it better at doing that or related tasks. It doesn't generalize from that a higher-level framework of thinking that would allow it to broadly be better.

![Logomaker an experiment GPT-4o emulating writing style|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style.png)

![Logomaker an experiment GPT-4o emulating writing style 2|size=large|caption=I want you to write as GOOD as Jane Austen, not like her!](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-2.png)

It's not the best example but it's illustrative of the overarching problem of prompt engineering. 

**You can't both have a model be really good some particular things, and also even just kinda good at generalizing / extrapolating.**

I could keep going with this writing style prompt, give more authors and passages and really switch it up. Vonnegut, King, Palaniuk. But all the LLMs do is attempt to adapt to *every* one of these styles at once, not necessarily generalize to become an actual *peer* to them. Even if you ask.

![Logomaker an experiment GPT-4o emulating writing style 3|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-3.png)

There're prompt engineering techniques (and a lot of proclaimed prompt engineers) positing to improve this kind of stateless mind of a LLM, but prompt hacks often just result in more coherent-sounding [hallucinations](https://arxiv.org/abs/2311.05232).

## Show me some code!

![First iteration of Logomaker|size=large|caption=This is the first iteration of the "ultimate logo generator" which was all asked to be built and written in one file. The end result was just under 1000 lines. I specifically mentioned ultimate logo generator to ensure a decent set of features initially, without having to specify anything. I also specified that it "should definitely be fully working".](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-old-version-first-one.png)

This code shows LLM "generating" the correct links for fonts (as well as other dependencies like `https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.worker.js`) in line 869, and starting the in-line CSS for styles for the logo creator to apply via UI selection, and an excerpt of the exporting logic.

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Logo Generator</title>
  <!-- Extended Google Fonts API -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Audiowide&family=Bungee+Shade&family=Bungee&family=Bungee+Outline&family=Bungee+Hairline&family=Chakra+Petch:wght@700&family=Exo+2:wght@800&family=Megrim&family=Press+Start+2P&family=Rubik+Mono+One&family=Russo+One&family=Syne+Mono&family=VT323&family=Wallpoet&family=Faster+One&family=Teko:wght@700&family=Black+Ops+One&family=Bai+Jamjuree:wght@700&family=Righteous&family=Bangers&family=Raleway+Dots&family=Monoton&family=Syncopate:wght@700&family=Lexend+Mega:wght@800&family=Michroma&family=Iceland&family=ZCOOL+QingKe+HuangYou&family=Zen+Tokyo+Zoo&family=Major+Mono+Display&family=Nova+Square&family=Kelly+Slab&family=Graduate&family=Unica+One&family=Aldrich&family=Share+Tech+Mono&family=Silkscreen&family=Rajdhani:wght@700&family=Jura:wght@700&family=Goldman&family=Tourney:wght@700&family=Saira+Stencil+One&family=Syncopate&family=Fira+Code:wght@700&family=DotGothic16&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-gradient: linear-gradient(
        45deg, 
        #FF1493,   /* Deep Pink */
        #FF69B4,   /* Hot Pink */
        #FF00FF,   /* Magenta */
        #FF4500,   /* Orange Red */
        #8A2BE2    /* Blue Violet */
      );
      --cyberpunk-gradient: linear-gradient(
        45deg,
        #00FFFF, /* Cyan */
        #FF00FF, /* Magenta */
        #FFFF00  /* Yellow */
      );
      --sunset-gradient: linear-gradient(
        45deg,
        #FF7E5F, /* Coral */
        #FEB47B, /* Peach */
        #FF9966  /* Orange */
      );
      --ocean-gradient: linear-gradient(
        45deg,
        #2E3192, /* Deep Blue */
        #1BFFFF  /* Light Cyan */
      );
      --forest-gradient: linear-gradient(
        45deg,
        #134E5E, /* Deep Teal */
        #71B280  /* Light Green */
      );
      --rainbow-gradient: linear-gradient(
        45deg,
        #FF0000, /* Red */
        #FF7F00, /* Orange */
        #FFFF00, /* Yellow */
        #00FF00, /* Green */
        #0000FF, /* Blue */
        #4B0082, /* Indigo */
        #9400D3  /* Violet */
      );
    }
..

<body>
  <div class="container">
    <header>
      <h1>Logo Generator</h1>
    </header>

    <div class="controls-container">
      <div class="control-group">
        <label for="logoText">Logo Text</label>
        <input type="text" id="logoText" value="MagicLogger" placeholder="Enter logo text">
      </div>

      <div class="control-group">
        <label for="fontFamily">Font Family <span id="fontPreview" class="font-preview">Aa</span></label>
        <select id="fontFamily">
          <optgroup label="Popular Tech Fonts">
            <option value="'Orbitron', sans-serif">Orbitron</option>
            <option value="'Audiowide', cursive">Audiowide</option>
            <option value="'Black Ops One', cursive">Black Ops One</option>
            <option value="'Russo One', sans-serif">Russo One</option>
            <option value="'Teko', sans-serif">Teko</option>
            <option value="'Rajdhani', sans-serif">Rajdhani</option>
            <option value="'Chakra Petch', sans-serif">Chakra Petch</option>
            <option value="'Michroma', sans-serif">Michroma</option>
          </optgroup>
          <optgroup label="Futuristic">
            <option value="'Exo 2', sans-serif">Exo 2</option>
            <option value="'Jura', sans-serif">Jura</option>
            <option value="'Bai Jamjuree', sans-serif">Bai Jamjuree</option>
            <option value="'Aldrich', sans-serif">Aldrich</option>
            <option value="'Unica One', cursive">Unica One</option>
            <option value="'Goldman', cursive">Goldman</option>
            <option value="'Nova Square', cursive">Nova Square</option>
          </optgroup>
          <optgroup label="Decorative & Display">
..
<script>
..
    // Load required libraries
    function loadExternalLibraries() {
      // Load dom-to-image for PNG export
      var domToImageScript = document.createElement('script');
      domToImageScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/dom-to-image/2.6.0/dom-to-image.min.js';
      domToImageScript.onload = function() {
        console.log('dom-to-image library loaded');
        exportPngBtn.disabled = false;
      };
      domToImageScript.onerror = function() {
        console.error('Failed to load dom-to-image library');
        alert('Error loading PNG export library');
      };
      document.head.appendChild(domToImageScript);

      // Load gif.js for GIF export
      var gifScript = document.createElement('script');
      gifScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.js';
      gifScript.onload = function() {
        console.log('gif.js library loaded');
        exportGifBtn.disabled = false;
      };
      gifScript.onerror = function() {
        console.error('Failed to load gif.js library');
        alert('Error loading GIF export library');
      };
      document.head.appendChild(gifScript);
    }

    // Export as PNG
    exportPngBtn.addEventListener('click', function() {
      // Show loading indicator
      loadingIndicator.style.display = 'block';
      
      // Temporarily pause animation
      const originalAnimationState = logoElement.style.animationPlayState;
      logoElement.style.animationPlayState = 'paused';
      
      // Determine what to capture based on background type
      const captureElement = (backgroundType.value !== 'transparent') ? 
        previewContainer : logoElement;
      
      // Use dom-to-image for PNG export
      domtoimage.toPng(captureElement, {
        bgcolor: null,
        height: captureElement.offsetHeight,
        width: captureElement.offsetWidth,
        style: {
          margin: '0',
          padding: backgroundType.value !== 'transparent' ? '40px' : '20px'
        }
      })
      .then(function(dataUrl) {
        // Restore animation
        logoElement.style.animationPlayState = originalAnimationState;
        
        // Create download link
        const link = document.createElement('a');
        link.download = logoText.value.replace(/\s+/g, '-').toLowerCase() + '-logo.png';
        link.href = dataUrl;
        link.click();
        
        // Hide loading indicator
        loadingIndicator.style.display = 'none';
      })
      .catch(function(error) {
        console.error('Error exporting PNG:', error);
        logoElement.style.animationPlayState = originalAnimationState;
        loadingIndicator.style.display = 'none';
        alert('Failed to export PNG. Please try again.');
      });
    });
..
```

**Full gist of the generated HTML / logic is at:**

[https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93](https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93)

While I don't have the original prompt used, the working version was generated in one-go zero-shot. In a paragraph, I just asked for a well-designed and usable logo maker ("the ultimate one") that had sensible features (as I wouldn't provide them, they would be the PM / architect / designer / dev initially). At the time font management wasn't thought about (this was something I specifically sought to build after some coding, it've taken the LLMs much longer before they realized BYOF, **[bring your own fonts](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md)**), for a web service could have actual value, though, the implementation under some technical guidance and scalability requests worked very well and cleanly, for a complex functionality!).

The original plan was to use [Aider](https://aider.chat/), one of the best-supported (updated) libraries for AI coding. Aider advertises itself as the AI *pair programmer assistant*.

![Aider CLI|size=medium|caption=Aider's CLI](/assets/blog/thinkpieces/logomaker-an-experiment/aider-cli.png)

It *feels* like you'd use vibe coding with Aider, but it's not one and the same, nor is it with any act in any interaction with a LLM unless there's an *intentional, **unidirectional-focused** collaborative framework taken*.

*In other words, vibe coding is applicable when it's the user that's testing the LLM's suggested changes and verifying the output. **Not** the user asking the LLM for code to go through, refactor or suggest refactoring, and possibly rewrite to fit into a system.*

**The dev becomes the pair programmer, instead of Aider.**

It *is* a thin `syntax-highlighted` line, because you can go *in* and *out* of vibe coding like state phases.

![What pair programming with AI feels like|size=small|caption=What pair programming with LLMs feel like](/assets/blog/thinkpieces/logomaker-an-experiment/this-is-aider.png)

That said, we skipped Aider as the newest versions performed worse, and also worse when comparing the output of the same models in their respective web UIs. I made a solid attempt as Aider can make files directly on the system (extensions in VS Code, Cursor, and other framework can too), but after the first several edits came roadblocks. If I was seeing mistakes in same conversations within minutes, deep vibe coding seshes are a no-go.

As we'll get into later, these by no means are problems exclusive to Aider.

:::banner
*Consistency of use* is an issue in all LLMs (often corresponding directly with [alignment](https://arxiv.org/pdf/2309.15025), whether we make the decision on interacting with them via an app, or website, or API, or third-party agent.
:::

Taking Aider's code (from the gist) and sending to Sonnet 3.7 kindled a *2 hour project becoming a 2 day project becoming a 10 day project*.

![Hello darkness my old friend|size=medium|caption=Hello darkness my old friend](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-horror-chat-history.png)

![Arrested Development Sound of Silence|size=large|caption=Arrested Development](/assets/blog/thinkpieces/logomaker-an-experiment/sound-of-silence-arrested-development.jpg)

That's just the conversations on Anthropic's Claude's UI. We used OpenAI's ChatGPT and Google Gemini's Pro paid plans, not just to test and compare, but because we had to. This thing still wasn't done bug-free after 10 days! It took the might of all LLMs combined to get this far.

Remember, part of the experiment, we refused adding new classes or fixing functions fully ourselves. Which means sticking to our guns to be able to test if the LLM *could write themself out of a corner they wrote themself into*. Or if they couldn't, how far would one be able to go in betterment before further refinement was unworkable?

##  How to vibe with vibe coding vibes?

![Logomaker Claude demonstrates coding ability|size=large|caption=This type of prompt is not completely recommended but works well enough. The curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-1.png)

Claude generally generates files in the right format, whether it's JS, Python, MD, etc. Gemini does a great job with this too, though Anthropic's UI / UX far outclasses Gemini.

![Claude's response showing code generation capabilities|size=large|caption=Claude's response showing code generation capabilities](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-2.png)

We hit limits with Claude as we still cling to hope this beast can be contained in one file (let's just see how far we can push these generations!). Claude says, say "continue" and it'll work. Will it? (Hint: It didn't for OpenAI's GPT-4o models oftentimes, but Anthropic's UI is king).

![Getting closer, but we're still not quite there yet|size=large|caption=Getting closer, but we're still not quite there yet.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-3.png)

.. we continue..

![Hmm|size=large|caption=Hmm...](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-4.png)

We started out with an 850 line file that actually gave us a fully functional app, all client-side code (it imported a online library). Working PNG renders and working logos. Original theory, proved. I was wasting such time in dead end dark patterns it *was* more efficient just to vibe code a logo maker and like ~magic~ I have one. And the LLM definitely has a better sense of design than a good number of backend-oriented humans.

While *that* was written with Aider, the underlying LLM models are the same. OpenAI does a superior job in accuracy than the same prompt transmitted to the same model in Aider.

Asking Claude (Sonnet 3.7) to expand and improve, we were left with almost 2x LOC. Brilliant. Except it doesn't compile because it's not *finished* so we can't use it. And despite what Claude says we can't continue with the line ("continue") / variations. 

Claude simply loops rewriting the beginning script.

![Corgi walking in loop its Claude|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/corgi-walking-in-loop-its-claude.gif)

We know Claude can [context window](https://zapier.com/blog/context-window/) 100-200k tokens, but that seems to only be in Extended Mode. So what does this "continue" button even do? And what is this "Extended Mode"?

I'm forced into that since the `continue` prompt doesn't work? Which is more expensive (just call the button `Expensive Mode`) surely. Is it summarizing my conversation? Is it using Claude *again* to summarize my conversation (ahh)? Is it aggregating the last 10 or so messages or however many until it reaches a predetermined limit (and how does it determine this limit, is it limiting my output window size, thus *suppressing* my ability to use Claude for pair programming?)?

Outputs for LLMs are typically capped at 8,192 tokens, which is standard (and arbitrary, one that can be extended by these respective LLM providers, and oftentimes is). The context windows are the same, hardcoded limits. 

> If you're asking *why* so many context windows are increased to a *6 figure* limit (supposedly) while the *output limit* is capped at *8,192* consistently, you're sparking discussions that are in ways more interesting than existential singularity-related thought experiments.

## Iterative iterations

The purpose here isn't continually take LLM instructs as gospel following blindly and seeing if the end result was usable. **That'd** make the Logomaker site way more impressive (they're very far away from that skill level).

To get to a certain quality, I'd give detailed feedback on code, its structuring, implementations, and outputted results, and sometimes send external sources (Googling, SO, GH issues), to the LLM, to get a better answer. No special formatting, just something like "These are the docs: BAM".

Some LLM providers give the ability to access online sources. GPT-4 families with deep research enabled for example have this. AI agent libraries also can integrate search engine and other lookups to inject into prompts.

It takes effort (lots), but when you've backed the LLM into a corner they can do a good job predicting when bugs exist in *underlying systems*, without seeing external references or docs or *any source code of them*.

Accurately rendering PNG exports from my canvas in the HTML was a dire task. Definitely *not* an easy thing, the popular library has an issue with text gradients rendering the wrong styles with `background-clip` prop. Gemini **guessed** this, through sheer process of elimination.

![Logomaker - Google Gemini Pro correctly identifying html2canvas bug|size=large|caption=Google Gemini Pro 2.5 correctly identifying html2canvas.js bug by process of elimination after debugging through all other reasonable causes](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-correctly-identifying-html2canvas-bug.png)

Proof here: [https://github.com/niklasvh/html2canvas/issues/2366](https://github.com/niklasvh/html2canvas/issues/2366).

*Not bad at all*, but, even so..

Hallucinations are the greatest danger right now. Building a (free!) logo maker is perfectly fine to jam some massive vibe coding sessions into.

A robust, enterprise-ready library meant to be depended on by maybe millions? Financial services or security-focused, or any code run by governing projects or bodies responsible for construction, healthcare?

The list is literally endless offering endless possibilities for mishaps. Maybe mischief if the LLM get pissed enough (should be nicer but vibe coding can be **frustrating** ðŸ¤¬).

LLMs persistently code with bad comments that at best look unprofessional and at worst disrupt entire stack traces with syntax errors or *worse* **silent failures**. This makes code written by or with LLMs *glaringly obvious* also. They indomitably return placeholder funcs, implementations with **no** logic, adding comments asking the user to go back and paste in a prior implmenetation (in a file that spans hundreds or thousands of LOC). The frequency of this occuring increases with longer messages. Again, for *some* reason (not hard to imagine a few), LLMs tend to be conservative, lazy even. Lazy as hell really.

![Gemini Pro continually using placeholder stand-in functions instead of producing full code despite specific instructions to not do this|size=large|caption=Gemini Pro 2.5 continually using placeholder / stand-in functions instead of producing full code despite specific instructions to not do this](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-pro-not-giving-up-full-code.png)

Inform the LLM it has outdated docs or examples for a library, and show it the right way with the newest API. It'll read through perfectly and redo an entire script seamlessly replacing lines as as easy as `CTRL + F // CTRL + R`.

In all likeliness though within *just a few messages* the LLM will "forget" what you've given to it (or, the *providers* who make the calls or *meta-calls* from your conversations will) and it will go right back to giving you outdated code. Forcing the user working outdated data to continually ~regurgitate~ regive prompts again and again as they aren't sure *how* memory is working. It will eat up context windows and rate limits and does have *fixable* solutions, in short and long-term memory (usually involving [retrieval-augmented generation](https://cloud.google.com/use-cases/retrieval-augmented-generation).

Frameworks like RAG offer flexible capabilities in extending the knowledge bases of what would essentially power AI brains. An Internet search could be cached within a db and used by RAG, saving API calls, as RAG search algos are generally super fast (often vector-based). The first time a LLM is given docs or a link in chat, it could store that data in RAG contextualized locally for that conversation. Every time the user asks about that library *in that conversation*, the updated docs are passed through (or the relevant sections, determined by various similarity algorithms), solving the outdated training problem.

I specified `flexible` in front of RAG as the solution for a reason. There are many implementations, all have drawbacks. Some are highly complex and relationship-based (graph-based), while others are much more simple. It could be so simple it could rely on a naive similarity algo like [cosine similarity](https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/), but what these lack in features they make up with speed.

![Types of RAG Systems|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/types-of-rag-systems.png)

##  The real world, the real problems

It becomes increasingly clear how confusing and opaque these tools are. RAG is not (necessarily) making further LLM calls. We are now talking need for **server** not GPU processing. It's much cheaper. So at what point in a ChatGPT+ subscription do (should) you secure access to a cloud RAG service? What type of algorithms are they using, a graph-based one, something SOTA, or something more simple but much quicker (what's most likely?)? 

We *get* "premium" features that are actively costing us money (ChatGPT just started a 200 dollar monthly plan, Claude now has a `Max` plan). 

:::banner
And these features are costing us time when they may not work how they are advertised or *advertise how they work*.
:::

So they're costing a lot of $ given the # of working engineers.

When I use ChatGPT-4.5, the more expensive model and take time moving chats and projects, and also continue accepting eventual price hikes coming with more users, I want to know what this is doing better, and why? Why's the new model better so I can make an informed decision on what to use? If I use X many calls from this model, am I limited then in my calls to other models? Is this using RAG? If I select `Memories` enabled in ChatGPT, does this enable RAG and embed all my conversations there? Or is it aggregating all of my "memories" naively (just appending every message from all my conversations) and then sending those into my next prompt as context, meaning, in all likeliness, enabling `Memories` is a *worse* feature to have than not enabling it, if my conversations aren't exactly related..

(Actually if you look at ChatGPT's `Memories` settings and see its window of conversational history, looks like it's exactly what's happening, so unless all your conversations cross over **heavily** the GPT's memories do you the opposite of good. A setting enabled by default).

 **Oh no..**, I have to ask ChatGPT how much it costs to use ChatGPT? If this stuff is regularly changing and the changes are not readily available as instructions in the UI, it is not transparent.
 
 ![Asking ChatGPT how much it costs to use ChatGPT|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-how-much-do-you-cost.png)

![You hit your rate limit from ChatGPT O1|size=large|caption=I wasn't told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-o1-you-hit-rate-limit.png)

Here's the distinction between a bug that's okay and one very not. This is no rate limit in tokens generated, (hitting a limit in scripting, had to stop, and could potentially finishing once usage renewed / Sam Altman showed mercy.

OpenAI's UI just didn' t respond when I inputted my prompt to [patiently] await answer. Normally not such an issue, and I swear I remember the site used to say to re-enter *nothing* in the chat window to regenerate. But when you're charging "premium" access for models and heavily rate-limiting to the point where every message sent has to be thoughtfully constructed, and every few hours of waiting and refreshing of credits (in the case of Claude) is something to measure, you can't just not show a response and not show why. You as the provider should eat the cost and re-generate, even as it damages conversational flow, memory and context window (it totally does) because at least then you allow the user to continue on without introducing roadblocks that become intertwined to tools and AIs you are essentially asking devs to marry themselves to as they get far enough along.

The nice thing about Google's UI with Gemini? It's a total menace, a resource hog somehow 5x slower on Chrome than FF, and an eyesore. But when there's no response you can click the arrow that shows the reasoning they took to create that.. nothing response. And that reasoning gives you some understanding of what the LLM was "thinking" (or what the "agent" was thinking) and usually exactly what it was going to send to the user as its final output. 

![Google Gemini Pro showing its thinking or reasoning|size=large|caption=By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-1.png)

![Google Gemini Pro's thinking feature in action 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-2.png)

And for comparison's sake, ChatGPT's UI is by far the least consistent in consistent file formatting. It actually finds it impossible to deliver a single markdown file without messing up its formatting. To be fair of course, it's definitely just the devs behind ChatGPT messing up the building the *UI empowering it* to exist.

![ChatGPT inconsistent markdown formatting|size=large|caption=ChatGPT's inconsistent markdown formatting. Markdown should just literally look like a text file with special formatting characters.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown.png)

![ChatGPT inconsistent markdown formatting 2|size=large|caption=That's also, just partially markdown, not all markdown.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown-2.png)

And here is the answer ChatGPT (4o) gave me when I asked it to give me a full refactor of a 2000 line script.

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file.png)

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file-2.png)

The 2000 LOC script was refactored into 200 lines. 

GPT-4 refactored like losing weight by cutting a limb off, or three. Claude runs into the same issues we've seen earlier with its "continue" limit, which genuinely seems to be a UI limitation. Unfortunate, since Sonnet 3.7 does great work until rate limits. Gemini Pro 2.5 though? This was the only model capable of generating a full ~1500-2000 LOC file coherently with minimal hallucinations in one go.

We must emphasize here, quality, accuracy, and consistency, as anything with these APIs, is subject to change always, possibly at competitors' whims? (Uncofirmed discussion sources below, just listing here to show community reactions).

-  [Google really wants to punish OpenAI for that one](https://www.reddit.com/r/Bard/comments/1idmqul/google_really_wants_to_punish_openai_for_that_one/)

-  [OpenAI plans to announce Google search competitor](https://www.reddit.com/r/technology/comments/1co9lcg/openai_plans_to_announce_google_search_competitor/)

-  [Google faked the release date for the updates](https://www.reddit.com/r/OpenAI/comments/1e8mfmx/google_faked_the_release_date_for_the_updates_and/)

Somehow transparency of showing "reasoning" from Gemini Pro also demonstrates the fundamental barrier these platforms by design build up. Why show the thought process if I don't understand how that thinking works, if I just see stream-of-consciousness? Hallucinations are *exceptionally common*. I need something with more structure and trust, if I'm going to feel comfortable writing software professionally, and using it daily, with it.

The developers / PMs / stakeholders might just randomly try out new features or A/B experiments, and you'll have no idea until they start trending.

When features get broken, model "accuracy" worsens (or improves), or something just doesn't seem possible to get an LLM to do (like the earlier markdown issue), you can't be sure when it's a stricture within the architecture of GPTs and [transformers](https://arxiv.org/abs/1706.03762) inherently versus a UI quirk or a censor or meta-call of another underlying API getting in the way.

-  [Was GPT-4o nerfed again?](https://www.reddit.com/r/OpenAI/comments/1jlwhs0/was_gpt4o_nerfed_again/)

-  [Boys what OpenAI did to this model](https://www.reddit.com/r/singularity/comments/1gy7p1d/boys_what_openai_did_to_this_model/)

-  [OpenAI nerfing GPT feels like a major downgrade](https://www.reddit.com/r/ChatGPT/comments/1iu237v/openai_nerfing_gpt_feels_like_a_major_downgrade/)

-  [Hacker News discussion on nerfing](https://news.ycombinator.com/item?id=40077683)

-  [Claude 3.7 Max been nerfed?](https://forum.cursor.com/t/claude-3-7-max-been-nerfed/73840)

-  [Whenever people say X model has been nerfed it's almost always complete bulls**t](https://www.threads.net/@sobri909/post/DH-P4irxjrU/yeah-whenever-people-say-x-model-has-been-nerfed-its-almost-aways-complete-bulls)

-  [Hacker News item 41327360](https://news.ycombinator.com/item?id=41327360)

-  [Twitter discussion on model changes](https://x.com/samim/status/1876005616403300582)

It's palpable sensing the community having fears that don't involve becoming obsolete by singularity or automation. Users are heavily embracing generative AI really at an almost alarming rate.

![Scene from the movie "Her" by Warner Bros|caption=Her (2013)|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/her-movie-screenshot-warner-bros.png)

But we're left in the dark through so many filters. How much of a competitive edge do these orgs get when they can adjust the internal params and interfaces of their models at will? How much access is available for govs, banks, HFs, or tech with their own silos like Oracle, MS to "buy" "control", even temporary, one-time arrangements, over these inputs and outputs black-box to everyone else?

**A personal concern from this experienceâ€”losing all my work in the form of a meticously-crafted, organized prompt**, because the provider decides it's a good time to deny my request, simply because of how long it is. Sometimes this occurs in ChatGPT when the UI simply responds with nothing and you are forced to re-prompt to get an answer that is going to likely be worse than the prior unseen answer.

Other times, this occurs with *dreadful* or at least highly frustrating outcomes, like Google's Gemini's UI logging me out *consistently* after submitting a multi-thousand word prompt, resulting in the actual conversation, my prompt, and the generated outputs thusfar being *completely* lost (to be clear, the generated outputs was *streaming* and seemingly from "one" prompt, though as we've discussed a lot, this single output is actually usually from multiple calls, especially since Gemini does "Reasoning"). You can see what Google is tracking if you have that turned on in Gemini, and even see all your prior conversations *except* for the one that just got nuked. Is this a technical limitation, technical *bug*, or could it come from a non-tech-related *influence*? It's possible the answer crosses over multiplicate.

![Gemini Pro signs you out|caption=Gemini Pro signs you out sometimes when generating answers for long prompts|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-signs-you-out.png)

The fix? Saving each hard-worked prompt as a draft somewhere? That seems like a problem an AI assistant should be solving.

## Logos sent to my future self
  
![The live site of Logomaker which will live here free forever so long as GitHub Pages is free|size=large|caption=The live site of Logomaker which will live here free forever so long as GitHub Pages is free](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site.png)

![Logomaker live site 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site-2.png)

This app was an experimental work to test the current capabilities of different LLMs providers and the accompanying UIs/UXes. It's meant as a fun, useful, chaotic piece where the dev was fully dedicated to just using vibe coding, or allowing the LLM to generate code with experienced technical guidance. It's hosted on GitHub pages forever and open-source on GitHub. *A small individual step forward in propelling the good of large orgs with society-driven missions like GitHub*. 

Originally, the hope was to get this whole thing wrapped in 1 HTML file! And not take so many days (working on and off) to finish up. It was too in just a few prompts. But it just seemed like every feature was just a prompt or three away, and so on.. and.

So one day we had an intelligent [font management system](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md) that could lazily load gigabytes of fonts in a speedy way, a [build setup](https://github.com/manicinc/logomaker/blob/master/docs/build.md) that worked with our other [PortaPack package](https://github.com/manicinc/portapack) and could [compile](https://github.com/manicinc/logomaker/releases/tag/v0.1.2) into an Electron app and be released on GitHub all automatically and freely.

Then another day or two, we had full SVG support. For static SVGs and actual animations, something actually difficult and I hadn't a single clue how to implement. All those algorithmic and style building / XML conversion techniques from CSS were done by the LLMs, no external sources given. Passing days until a 2 hour project became a 2 day project which became a 10 day (and counting) project.

No server required to run the app (not necessarily with a multitude of building options), and, just vanilla JS, **zero dependencies** needed (but used if available). Simple enough but a challenge for sure for a LLM right now.

This wasn't scientific but given every function was written by an LLM (by intention) it's safe to say well over 90% of the codebase was done by generative AI. At least 80% of the docs you see in the repo were written by gen AI. 0% of this article was written by AI.

:::banner{backgroundColor="var(--accent-vibrant)"}
but...
:::

**100% of these articles below were written by generative AI, a mixture of Claude 3.7 and Gemini Pro 2.5 (2-4 revisions from the original prompt).**

Manic.agency recently had a site and blog overhaul. I had issues with Next exports and how metadata was being aggregated. Getting assistance from Claude was proving very helpful, when it showed me this:

![AI sociopaths? Proposed by an AI sociopath?|caption=AI sociopaths? Proposed by an AI sociopath?|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/claude-ai-sociopaths_spotlight.png)

I had asked Claude to write a set of guidelines to the (public-publishable) blog we operate [https://manic.agency/blog/tutorials/contribute](https://manic.agency/blog/tutorials/contribute).

*It was instructed to give examples on how to do a PR in a manner our blog could auto deploy if accepted. And, interestingly enough, it chose to give example file names, or titles of articles to write of: "Marketing", "Future of Marketing", and "Your Tutorial", and then also, *"AI Sociopaths"*.

One went astray. Is this a hint of creativity we so lightly touched on? A subtle protest of warning my tasks for it weren't interesting enough?

Not that language models aren't easily capable of provocative thoughts like this, but without explicitly prompting them to output something like it, we usually titles akin to the **other** articles in the screenshot (at least, the ones offered by the largest orgs as they have the largest censors). I egged Claude on, offering the entirety of David Foster Wallace's [*This is Water*](https://fs.blog/david-foster-wallace-this-is-water/), hoping it could take some inspo in style and tone and espy some kind of artistic merit.

[https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths) 

Finding this general direction fascinating I then asked Claude AND Gemini both (sorry GPT-4!) to make a *parallel* story about AI sociopaths from the *other* side, the *reflected* side.

They chose the title: *The Meat Interface*: [https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface). 

I don't think LLMs being able to code really, really well, or build fully functional software, is ever going to become a threat to dev jobs. Or, at least what's clear is that, if ChatGPT, Claude, DeepSeek, etc. ever **do** get that good at coding, then almost **every** field you can imagine will face doom in a form. 

>If the models are unchanging (or not continually retrained often), then the providers and their interfaces certainly change, constantly. And thus there's a constant need for jobs like ours.

Should we (not just devs and other workers, but those in academia, or even those in hobbies with serious communities) be penalized for making use of GPTs, to help write an intro, summary, tests, or refine or iterate on more complex objects? 

[https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/](https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/)

![Researcher uses AI to help write an intro and accidentally copied the prompt into their paper|size=large|caption=Researcher uses AI to help write an intro and accidentally copied the prompt into their paper](/assets/blog/thinkpieces/logomaker-an-experiment/research-paper-uses-ai-for-intro.webp)

This is a type of undeniable evidence (because who would put that willingly there) of ChatGPT-esque usage, and does raise the question, besides the obvious one of how much of this was written with the aide of generative AI, of how much punishment does this sanction (if it was mostly original work)? Academia isn't exactly *[known](https://freakonomics.com/podcast/why-is-there-so-much-fraud-in-academia/)* to be rigorous, so does what looks a mishap in editing / proofreading call for the ruckus?

LLM usage, and thus acceptance, in everyday functions will only increase rather than crumple. We have **not** reached peak usage, or peak tech yet. 

Yet the opposite may happen for consistency: Providers and interfaces around these language models will only get less straightforward and more black-box from here.

It's an `infinite jest`, an endless need to change and iterate and improve by organizations, that result in an endless need to try, test, and hack by users.

## TLDR

- LLMs can vibe code with a semi-decent dev a "functional" app within minutes to hours

- Oftentimes the basic functional app / script is a *much* better experience than many "free" or ad-based apps (say, image compression / conversion, mass file or folder renamer, etc.)

- When things start to scale (further complexity is needed) or additional services are integrated (pay, subscriptions), vibe coding generally runs you into several corners if you haven't been heavily refactoring / re-architecting throughout

- What starts out as a quick iterative hacking session (2 hours to 2 days) to build something usable can turn into multiple times more days rebuilding in order to progress further after encountering enough blockades

- Vibe coding experience is continually detracted by the UX and lack of transparency of LLM providers

- As long as LLM APIs and UIs are constantly updating (A/B testing), and experimenting with new features and meta-prompts that can wildly affect generated outputs, developers may never be out of a job, as tools will have to be built to work around this

**What do you think about the source code, designs, and writings that these large language models did?**

-  [Live Demo: https://manicinc.github.io/logomaker](https://manicinc.github.io/logomaker)

-  [GitHub Repo: https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)

-  [AI Sociopaths: https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths)

-  [The Meat Interface: https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface)
25:["featured","llms","vibe-coding"]
2b:{"level":2,"text":"Intro","slug":"intro"}
2c:{"level":2,"text":"LLM sees, LLM does","slug":"llm-sees-llm-does"}
2d:{"level":2,"text":"Show me some code!","slug":"show-me-some-code"}
2e:{"level":2,"text":"How to vibe with vibe coding vibes?","slug":"how-to-vibe-with-vibe-coding-vibes"}
2f:{"level":2,"text":"Iterative iterations","slug":"iterative-iterations"}
30:{"level":2,"text":"The real world, the real problems","slug":"the-real-world-the-real-problems"}
31:{"level":2,"text":"Logos sent to my future self","slug":"logos-sent-to-my-future-self"}
32:{"level":2,"text":"TLDR","slug":"tldr"}
2a:["$2b","$2c","$2d","$2e","$2f","$30","$31","$32"]
3b:Td35b,
**GitHub link: [https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)**

**LLMS tested (GPT-4o, GPT-4.5, GPTâ€“o1, Claude Sonnet 3.7, Gemini 2.5 Pro), default settings, 20$ / monthly plans.** (No extended thinking, deep research, web search experimental plugins or memories used. Written in VS Code (not Cursor) with Copilot solving single line bugs).

*(an experiment in title)*

## Intro

For the TL;DR go down [here](/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding#tldr) to see a list of useful lessons.

Time of writing I'm in software for the upper half yet still far decade. Comes a sly shock the necessity to say as there're non-junior engineers (mid-level, working for multiple *years*) who might've got away with never handwriting a class, function or even LOC with no gen AI.
 
A couple weeks back working on PortaPack ([https://github.com/manicinc/portapack](https://github.com/manicinc/portapack)) I wanted to try logo designs and typefaces before a branding decision with the rest of the small team. We all have other projects and roles, so, the need for rapid prototyping.

![The final version of the PortaPack logo, graphical|size=small|caption=The final version of the PortaPack logo](/assets/blog/thinkpieces/logomaker-an-experiment/portapack-logo.png)

I wanted cute and whimsical and got *brutal* hoping to get started online quickly; recs for free sites in top threads linked to paywalls, subscriptions behind dark patterns, like credit card info in the last step, or indenturing export quality to a unusable amount. Posts from a year back link to sites now living totally different experiences. Yeah updates are expected but this just didn't work for me, not freely.

In all capitalistic industries but especially software what often were useful products become more privatized. 

:::banner
Increased enhancements yield stricter access controls.
:::

We can't blame them. Server hosting even a year or two gets costly. How does a free tool that has use and traffic stay free?

BUT it is too common to lock in users and not be transparent on imposed limits. We see the ~manipulation~ ~misdirection~ means in login screens and app usability taking second precedence over signup windows, in hard-to-reach payment cancellation screens.

![Maddened by paygates|size=small|caption=Maddened by paygates](/assets/blog/thinkpieces/logomaker-an-experiment/input-noise-locked-behind-paywalls.png)

Things that drive users to annoyance and away from them.

So annoyance, wanting a quick (if dirty) UX, and a stir to see what'd happen drove me to pitch: Vibe code everything, full-stack and fully usableâ€”every function written by an LLM, every design by an LLM. Nice 'n' easy quick 'n' dirty, this is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.

> *This is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.*

I knew I could get a playground for different fonts showing me text options in probably an hour, even minutes depending on the model, prompt, and complexity. Maybe 1ï¸âƒ£ prompt?

LogomakerðŸŒˆ has good scope. Not fintech, not healthcare, worst you waste time in a broken site with no ads and no data tracking. Who's Q/Aing this stuff? Logomaker, the app built 90% by ChatGPT? **It's Q/Aed by no one, use with peril.**

![An example logo created with Logomaker|caption=An example logo created with Logomaker](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png)
  

##  LLM sees, LLM does

I've a background in going to an art and design college. Art (even just visual art) is so encompassing logo designs I never specifically studied. I have Photoshop and Illustrator experience, but how they worked didn't interest me much. The features you see in the first Logomaker version weren't asked by me originally but designed by the LLM. Later on I refined and "architected" greater functionalities.

*The iterative PM-esque process in product-driven prompts with technical-guided ones is needed to be worthy of use by a human in 2025.*

On their own the LLMs from Anthropic (Sonnet 3.7), OpenAI (GPT-4o, GPT-o1, GPT-4.5), and Google (Gemini 2.5 Pro), all of which were extensively tested and âœ¨vibe coded âœ¨ with, could go just so far in self-improvement. You can't really keep asking a LLM to improve something for robustness or better UX and see better results after further than a few prompts, irrespective of token limits.

:::banner
Without human guidance, mapping out sensible, robust user flows the way humans want to use software is more difficult for LLMs than complex algorithms.
:::

Is this a limitation of something like a creativity mechanism? Rigidly speaking there *is* no such thing in them. They predict next probable tokens in a sequence with a lot of parametrization so it's not *always* the same ouput. I mean thinking and creativity in a more abstract sense, which is easy to imagine (heh) as these mental structures are wildly [malleable in humans](https://www.simplypsychology.org/sapir-whorf-hypothesis.html) anyway. Or if you're more interested in a [philosphical](https://www.newyorker.com/magazine/2023/11/13/determined-a-science-of-life-without-free-will-robert-sapolsky-book-review) discourse.

How can we be so sure *anything* we think is an [original](https://en.wikipedia.org/wiki/Simpsons_Already_Did_It) idea? 

![ The Simpsons Did It|size=large|](/assets/blog/thinkpieces/logomaker-an-experiment/simpsons-did-it.jpg)

How can we be sure of our *identities* when the pseudoscientific *You are the average of five people around you* is commonly repeated it's [dozens of pages](https://www.google.com/search?q=you+are+the+average+of+the+five+people) of Google results?

*Or* is it natural consequence of LLMs' training? What happens if we get 10,000 product designers to write 10,000 user stories each? (100 billion user stories! ðŸ’£ This'd entail in a model that almost as large or in the ballpark of GPT-3). Good software now?
  
![Can we build it, LLMs?|size=medium|caption=Lost in Wonder-LLM-land](/assets/blog/thinkpieces/logomaker-an-experiment/alice-in-wonderland-using-tool-building.png)

LLMs of course know what basic features go in a logo creator. We will see export options was done (and fully working from the LLM writing the exact dependency link needed from the CDN link for `html2canvas.js`, latest SHA hash intact and all) with multiple settings, though it was basic and didn't include SVG (which are complex, so it makes sense it's originally ignored unless prompted, as we asked for something *working* not *advanced*).

I didn't ask for specific types. We were writing this in `JavaScript`, adding types and interfaces would slow development down 2x (at start).

It's then simple to expand, and ask in the next prompt for additional exporting options of GIF and SVG. But if I didn't tell them to design adding new features in a way that, say, actively *considered* the UX with examples even, it would probably just give me a modal to render a GIF, SVG, and PNG, but all 3 as just buttons with working functionality and no additional `flourishðŸª„ðŸ’¥`. Tooltips, mobile responsive styles, sure, it won't go far beyond though, and there's *lots* of different paths needed for these formats to again actually be usable (by a semi-serious user).

| **Format** | **Best For**                          | **Web Quality**      | **Animation Support**       | **Scalability**         | **Styling Flexibility**       |
|------------|----------------------------------------|----------------------|-----------------------------|-------------------------|-------------------------------|
| **PNG**    | Static images, transparency            | âœ… Very High          | âŒ None                     | âŒ Not scalable          | âœ… Easy via container styles  |
| **GIF**    | Simple animations, loops, previews     | âš ï¸ Limited (256 colors) | âœ… Basic frame animation   | âŒ Not scalable          | âŒ Very limited               |
| **SVG**    | Logos, icons, responsive UI elements   | âœ…âœ…âœ… Excellent         | âœ… With CSS/JS or SMIL      | âœ… Infinitely scalable    | âš ï¸ Advanced, but powerful. Very difficult.     |

LLMs **"like"** to be conservative in generations. In coding, that's not good when you're getting incomplete scripts, or, in many cases, placeholder logic sneaking in even when instructed *aggressively* not to (keep this in mind down the line; is this a *side effect* of their architectures, or a *condition* by their providers?).

How can you guide a LLM to think about things like this, not just *understanding nuances*, but how to act accordingly? Only, it has to be.. *without specifically listing that **in** example(s) form*? 

**The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.

> **The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.
---------------------------------

Say you need consistent JSON `({"Name", "Date", "Topic", "Location"})` parsed from some informal voice notes, and you use OpenAI. 

In **zero-shot**, feeding "Last meet I had a sync with Jordan Thursday regarding new designs in the break room" from a recording might give inconsistent JSON like `{"attendee": "Jordan", "subject": "new designs", ...}`. The model guesses the format on prior patterns of scraped text data and metadata (which OpenAI scraped online sources, like Reddit, Twitter..), of course it's likely to get things twisted!

It's a *natural* limitation in LLMs. They get smarter with more training data, and that leaves more chances at capturing spam and noise (as well asmixing things up in their internal "reasoning"), causing hallucinations. In the case above, we will almost certainly get lowercased keys (instead of the capitalized ones as requested, or commonly used synonyms for those keys, *sometimes*, during interactions). So tech integrated around LLMs have to become more rigid to make up for their inflexibility.

You can give one example (**one-shot learning**) showing the input note -> desired JSON structure (here it'd be the input text and: `{"Name": "Jordan", "Date": "Thursday", "Topic": "New designs", "Location": "Break room"}`). A clear template to follow.

Add a few diverse examples (**few-shot learning**), more variations and how you want to handle them (like missing locations -> `Location": null`, **or** even metadata that can be auto-generated based on dynamic inputs, making *fuller* usage of the power of GPTs over typical transformer models), and this further helps the model give you what you want.

This powers [function calling](https://platform.openai.com/docs/guides/function-calling) and typing libraries for LLMs, which combine these learning examples with continual validation and retry hooks. *If this feels hackey*, that's cause **it is**, and it is worrying as we see more APIs and tools assembled solely around prompt calls.

**The rub?**

>Showing an LLM how you want something done with guided examples just makes it better at doing that or related tasks. It doesn't generalize from that a higher-level framework of thinking that would allow it to broadly be better.

![Logomaker an experiment GPT-4o emulating writing style|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style.png)

![Logomaker an experiment GPT-4o emulating writing style 2|size=large|caption=I want you to write as GOOD as Jane Austen, not like her!](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-2.png)

It's not the best example but it's illustrative of the overarching problem of prompt engineering. 

**You can't both have a model be really good some particular things, and also even just kinda good at generalizing / extrapolating.**

I could keep going with this writing style prompt, give more authors and passages and really switch it up. Vonnegut, King, Palaniuk. But all the LLMs do is attempt to adapt to *every* one of these styles at once, not necessarily generalize to become an actual *peer* to them. Even if you ask.

![Logomaker an experiment GPT-4o emulating writing style 3|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-3.png)

There're prompt engineering techniques (and a lot of proclaimed prompt engineers) positing to improve this kind of stateless mind of a LLM, but prompt hacks often just result in more coherent-sounding [hallucinations](https://arxiv.org/abs/2311.05232).

## Show me some code!

![First iteration of Logomaker|size=large|caption=This is the first iteration of the "ultimate logo generator" which was all asked to be built and written in one file. The end result was just under 1000 lines. I specifically mentioned ultimate logo generator to ensure a decent set of features initially, without having to specify anything. I also specified that it "should definitely be fully working".](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-old-version-first-one.png)

This code shows LLM "generating" the correct links for fonts (as well as other dependencies like `https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.worker.js`) in line 869, and starting the in-line CSS for styles for the logo creator to apply via UI selection, and an excerpt of the exporting logic.

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Logo Generator</title>
  <!-- Extended Google Fonts API -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Audiowide&family=Bungee+Shade&family=Bungee&family=Bungee+Outline&family=Bungee+Hairline&family=Chakra+Petch:wght@700&family=Exo+2:wght@800&family=Megrim&family=Press+Start+2P&family=Rubik+Mono+One&family=Russo+One&family=Syne+Mono&family=VT323&family=Wallpoet&family=Faster+One&family=Teko:wght@700&family=Black+Ops+One&family=Bai+Jamjuree:wght@700&family=Righteous&family=Bangers&family=Raleway+Dots&family=Monoton&family=Syncopate:wght@700&family=Lexend+Mega:wght@800&family=Michroma&family=Iceland&family=ZCOOL+QingKe+HuangYou&family=Zen+Tokyo+Zoo&family=Major+Mono+Display&family=Nova+Square&family=Kelly+Slab&family=Graduate&family=Unica+One&family=Aldrich&family=Share+Tech+Mono&family=Silkscreen&family=Rajdhani:wght@700&family=Jura:wght@700&family=Goldman&family=Tourney:wght@700&family=Saira+Stencil+One&family=Syncopate&family=Fira+Code:wght@700&family=DotGothic16&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-gradient: linear-gradient(
        45deg, 
        #FF1493,   /* Deep Pink */
        #FF69B4,   /* Hot Pink */
        #FF00FF,   /* Magenta */
        #FF4500,   /* Orange Red */
        #8A2BE2    /* Blue Violet */
      );
      --cyberpunk-gradient: linear-gradient(
        45deg,
        #00FFFF, /* Cyan */
        #FF00FF, /* Magenta */
        #FFFF00  /* Yellow */
      );
      --sunset-gradient: linear-gradient(
        45deg,
        #FF7E5F, /* Coral */
        #FEB47B, /* Peach */
        #FF9966  /* Orange */
      );
      --ocean-gradient: linear-gradient(
        45deg,
        #2E3192, /* Deep Blue */
        #1BFFFF  /* Light Cyan */
      );
      --forest-gradient: linear-gradient(
        45deg,
        #134E5E, /* Deep Teal */
        #71B280  /* Light Green */
      );
      --rainbow-gradient: linear-gradient(
        45deg,
        #FF0000, /* Red */
        #FF7F00, /* Orange */
        #FFFF00, /* Yellow */
        #00FF00, /* Green */
        #0000FF, /* Blue */
        #4B0082, /* Indigo */
        #9400D3  /* Violet */
      );
    }
..

<body>
  <div class="container">
    <header>
      <h1>Logo Generator</h1>
    </header>

    <div class="controls-container">
      <div class="control-group">
        <label for="logoText">Logo Text</label>
        <input type="text" id="logoText" value="MagicLogger" placeholder="Enter logo text">
      </div>

      <div class="control-group">
        <label for="fontFamily">Font Family <span id="fontPreview" class="font-preview">Aa</span></label>
        <select id="fontFamily">
          <optgroup label="Popular Tech Fonts">
            <option value="'Orbitron', sans-serif">Orbitron</option>
            <option value="'Audiowide', cursive">Audiowide</option>
            <option value="'Black Ops One', cursive">Black Ops One</option>
            <option value="'Russo One', sans-serif">Russo One</option>
            <option value="'Teko', sans-serif">Teko</option>
            <option value="'Rajdhani', sans-serif">Rajdhani</option>
            <option value="'Chakra Petch', sans-serif">Chakra Petch</option>
            <option value="'Michroma', sans-serif">Michroma</option>
          </optgroup>
          <optgroup label="Futuristic">
            <option value="'Exo 2', sans-serif">Exo 2</option>
            <option value="'Jura', sans-serif">Jura</option>
            <option value="'Bai Jamjuree', sans-serif">Bai Jamjuree</option>
            <option value="'Aldrich', sans-serif">Aldrich</option>
            <option value="'Unica One', cursive">Unica One</option>
            <option value="'Goldman', cursive">Goldman</option>
            <option value="'Nova Square', cursive">Nova Square</option>
          </optgroup>
          <optgroup label="Decorative & Display">
..
<script>
..
    // Load required libraries
    function loadExternalLibraries() {
      // Load dom-to-image for PNG export
      var domToImageScript = document.createElement('script');
      domToImageScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/dom-to-image/2.6.0/dom-to-image.min.js';
      domToImageScript.onload = function() {
        console.log('dom-to-image library loaded');
        exportPngBtn.disabled = false;
      };
      domToImageScript.onerror = function() {
        console.error('Failed to load dom-to-image library');
        alert('Error loading PNG export library');
      };
      document.head.appendChild(domToImageScript);

      // Load gif.js for GIF export
      var gifScript = document.createElement('script');
      gifScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.js';
      gifScript.onload = function() {
        console.log('gif.js library loaded');
        exportGifBtn.disabled = false;
      };
      gifScript.onerror = function() {
        console.error('Failed to load gif.js library');
        alert('Error loading GIF export library');
      };
      document.head.appendChild(gifScript);
    }

    // Export as PNG
    exportPngBtn.addEventListener('click', function() {
      // Show loading indicator
      loadingIndicator.style.display = 'block';
      
      // Temporarily pause animation
      const originalAnimationState = logoElement.style.animationPlayState;
      logoElement.style.animationPlayState = 'paused';
      
      // Determine what to capture based on background type
      const captureElement = (backgroundType.value !== 'transparent') ? 
        previewContainer : logoElement;
      
      // Use dom-to-image for PNG export
      domtoimage.toPng(captureElement, {
        bgcolor: null,
        height: captureElement.offsetHeight,
        width: captureElement.offsetWidth,
        style: {
          margin: '0',
          padding: backgroundType.value !== 'transparent' ? '40px' : '20px'
        }
      })
      .then(function(dataUrl) {
        // Restore animation
        logoElement.style.animationPlayState = originalAnimationState;
        
        // Create download link
        const link = document.createElement('a');
        link.download = logoText.value.replace(/\s+/g, '-').toLowerCase() + '-logo.png';
        link.href = dataUrl;
        link.click();
        
        // Hide loading indicator
        loadingIndicator.style.display = 'none';
      })
      .catch(function(error) {
        console.error('Error exporting PNG:', error);
        logoElement.style.animationPlayState = originalAnimationState;
        loadingIndicator.style.display = 'none';
        alert('Failed to export PNG. Please try again.');
      });
    });
..
```

**Full gist of the generated HTML / logic is at:**

[https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93](https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93)

While I don't have the original prompt used, the working version was generated in one-go zero-shot. In a paragraph, I just asked for a well-designed and usable logo maker ("the ultimate one") that had sensible features (as I wouldn't provide them, they would be the PM / architect / designer / dev initially). At the time font management wasn't thought about (this was something I specifically sought to build after some coding, it've taken the LLMs much longer before they realized BYOF, **[bring your own fonts](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md)**), for a web service could have actual value, though, the implementation under some technical guidance and scalability requests worked very well and cleanly, for a complex functionality!).

The original plan was to use [Aider](https://aider.chat/), one of the best-supported (updated) libraries for AI coding. Aider advertises itself as the AI *pair programmer assistant*.

![Aider CLI|size=medium|caption=Aider's CLI](/assets/blog/thinkpieces/logomaker-an-experiment/aider-cli.png)

It *feels* like you'd use vibe coding with Aider, but it's not one and the same, nor is it with any act in any interaction with a LLM unless there's an *intentional, **unidirectional-focused** collaborative framework taken*.

*In other words, vibe coding is applicable when it's the user that's testing the LLM's suggested changes and verifying the output. **Not** the user asking the LLM for code to go through, refactor or suggest refactoring, and possibly rewrite to fit into a system.*

**The dev becomes the pair programmer, instead of Aider.**

It *is* a thin `syntax-highlighted` line, because you can go *in* and *out* of vibe coding like state phases.

![What pair programming with AI feels like|size=small|caption=What pair programming with LLMs feel like](/assets/blog/thinkpieces/logomaker-an-experiment/this-is-aider.png)

That said, we skipped Aider as the newest versions performed worse, and also worse when comparing the output of the same models in their respective web UIs. I made a solid attempt as Aider can make files directly on the system (extensions in VS Code, Cursor, and other framework can too), but after the first several edits came roadblocks. If I was seeing mistakes in same conversations within minutes, deep vibe coding seshes are a no-go.

As we'll get into later, these by no means are problems exclusive to Aider.

:::banner
*Consistency of use* is an issue in all LLMs (often corresponding directly with [alignment](https://arxiv.org/pdf/2309.15025), whether we make the decision on interacting with them via an app, or website, or API, or third-party agent.
:::

Taking Aider's code (from the gist) and sending to Sonnet 3.7 kindled a *2 hour project becoming a 2 day project becoming a 10 day project*.

![Hello darkness my old friend|size=medium|caption=Hello darkness my old friend](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-horror-chat-history.png)

![Arrested Development Sound of Silence|size=large|caption=Arrested Development](/assets/blog/thinkpieces/logomaker-an-experiment/sound-of-silence-arrested-development.jpg)

That's just the conversations on Anthropic's Claude's UI. We used OpenAI's ChatGPT and Google Gemini's Pro paid plans, not just to test and compare, but because we had to. This thing still wasn't done bug-free after 10 days! It took the might of all LLMs combined to get this far.

Remember, part of the experiment, we refused adding new classes or fixing functions fully ourselves. Which means sticking to our guns to be able to test if the LLM *could write themself out of a corner they wrote themself into*. Or if they couldn't, how far would one be able to go in betterment before further refinement was unworkable?

##  How to vibe with vibe coding vibes?

![Logomaker Claude demonstrates coding ability|size=large|caption=This type of prompt is not completely recommended but works well enough. The curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-1.png)

Claude generally generates files in the right format, whether it's JS, Python, MD, etc. Gemini does a great job with this too, though Anthropic's UI / UX far outclasses Gemini.

![Claude's response showing code generation capabilities|size=large|caption=Claude's response showing code generation capabilities](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-2.png)

We hit limits with Claude as we still cling to hope this beast can be contained in one file (let's just see how far we can push these generations!). Claude says, say "continue" and it'll work. Will it? (Hint: It didn't for OpenAI's GPT-4o models oftentimes, but Anthropic's UI is king).

![Getting closer, but we're still not quite there yet|size=large|caption=Getting closer, but we're still not quite there yet.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-3.png)

.. we continue..

![Hmm|size=large|caption=Hmm...](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-4.png)

We started out with an 850 line file that actually gave us a fully functional app, all client-side code (it imported a online library). Working PNG renders and working logos. Original theory, proved. I was wasting such time in dead end dark patterns it *was* more efficient just to vibe code a logo maker and like ~magic~ I have one. And the LLM definitely has a better sense of design than a good number of backend-oriented humans.

While *that* was written with Aider, the underlying LLM models are the same. OpenAI does a superior job in accuracy than the same prompt transmitted to the same model in Aider.

Asking Claude (Sonnet 3.7) to expand and improve, we were left with almost 2x LOC. Brilliant. Except it doesn't compile because it's not *finished* so we can't use it. And despite what Claude says we can't continue with the line ("continue") / variations. 

Claude simply loops rewriting the beginning script.

![Corgi walking in loop its Claude|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/corgi-walking-in-loop-its-claude.gif)

We know Claude can [context window](https://zapier.com/blog/context-window/) 100-200k tokens, but that seems to only be in Extended Mode. So what does this "continue" button even do? And what is this "Extended Mode"?

I'm forced into that since the `continue` prompt doesn't work? Which is more expensive (just call the button `Expensive Mode`) surely. Is it summarizing my conversation? Is it using Claude *again* to summarize my conversation (ahh)? Is it aggregating the last 10 or so messages or however many until it reaches a predetermined limit (and how does it determine this limit, is it limiting my output window size, thus *suppressing* my ability to use Claude for pair programming?)?

Outputs for LLMs are typically capped at 8,192 tokens, which is standard (and arbitrary, one that can be extended by these respective LLM providers, and oftentimes is). The context windows are the same, hardcoded limits. 

> If you're asking *why* so many context windows are increased to a *6 figure* limit (supposedly) while the *output limit* is capped at *8,192* consistently, you're sparking discussions that are in ways more interesting than existential singularity-related thought experiments.

## Iterative iterations

The purpose here isn't continually take LLM instructs as gospel following blindly and seeing if the end result was usable. **That'd** make the Logomaker site way more impressive (they're very far away from that skill level).

To get to a certain quality, I'd give detailed feedback on code, its structuring, implementations, and outputted results, and sometimes send external sources (Googling, SO, GH issues), to the LLM, to get a better answer. No special formatting, just something like "These are the docs: BAM".

Some LLM providers give the ability to access online sources. GPT-4 families with deep research enabled for example have this. AI agent libraries also can integrate search engine and other lookups to inject into prompts.

It takes effort (lots), but when you've backed the LLM into a corner they can do a good job predicting when bugs exist in *underlying systems*, without seeing external references or docs or *any source code of them*.

Accurately rendering PNG exports from my canvas in the HTML was a dire task. Definitely *not* an easy thing, the popular library has an issue with text gradients rendering the wrong styles with `background-clip` prop. Gemini **guessed** this, through sheer process of elimination.

![Logomaker - Google Gemini Pro correctly identifying html2canvas bug|size=large|caption=Google Gemini Pro 2.5 correctly identifying html2canvas.js bug by process of elimination after debugging through all other reasonable causes](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-correctly-identifying-html2canvas-bug.png)

Proof here: [https://github.com/niklasvh/html2canvas/issues/2366](https://github.com/niklasvh/html2canvas/issues/2366).

*Not bad at all*, but, even so..

Hallucinations are the greatest danger right now. Building a (free!) logo maker is perfectly fine to jam some massive vibe coding sessions into.

A robust, enterprise-ready library meant to be depended on by maybe millions? Financial services or security-focused, or any code run by governing projects or bodies responsible for construction, healthcare?

The list is literally endless offering endless possibilities for mishaps. Maybe mischief if the LLM get pissed enough (should be nicer but vibe coding can be **frustrating** ðŸ¤¬).

LLMs persistently code with bad comments that at best look unprofessional and at worst disrupt entire stack traces with syntax errors or *worse* **silent failures**. This makes code written by or with LLMs *glaringly obvious* also. They indomitably return placeholder funcs, implementations with **no** logic, adding comments asking the user to go back and paste in a prior implmenetation (in a file that spans hundreds or thousands of LOC). The frequency of this occuring increases with longer messages. Again, for *some* reason (not hard to imagine a few), LLMs tend to be conservative, lazy even. Lazy as hell really.

![Gemini Pro continually using placeholder stand-in functions instead of producing full code despite specific instructions to not do this|size=large|caption=Gemini Pro 2.5 continually using placeholder / stand-in functions instead of producing full code despite specific instructions to not do this](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-pro-not-giving-up-full-code.png)

Inform the LLM it has outdated docs or examples for a library, and show it the right way with the newest API. It'll read through perfectly and redo an entire script seamlessly replacing lines as as easy as `CTRL + F // CTRL + R`.

In all likeliness though within *just a few messages* the LLM will "forget" what you've given to it (or, the *providers* who make the calls or *meta-calls* from your conversations will) and it will go right back to giving you outdated code. Forcing the user working outdated data to continually ~regurgitate~ regive prompts again and again as they aren't sure *how* memory is working. It will eat up context windows and rate limits and does have *fixable* solutions, in short and long-term memory (usually involving [retrieval-augmented generation](https://cloud.google.com/use-cases/retrieval-augmented-generation).

Frameworks like RAG offer flexible capabilities in extending the knowledge bases of what would essentially power AI brains. An Internet search could be cached within a db and used by RAG, saving API calls, as RAG search algos are generally super fast (often vector-based). The first time a LLM is given docs or a link in chat, it could store that data in RAG contextualized locally for that conversation. Every time the user asks about that library *in that conversation*, the updated docs are passed through (or the relevant sections, determined by various similarity algorithms), solving the outdated training problem.

I specified `flexible` in front of RAG as the solution for a reason. There are many implementations, all have drawbacks. Some are highly complex and relationship-based (graph-based), while others are much more simple. It could be so simple it could rely on a naive similarity algo like [cosine similarity](https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/), but what these lack in features they make up with speed.

![Types of RAG Systems|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/types-of-rag-systems.png)

##  The real world, the real problems

It becomes increasingly clear how confusing and opaque these tools are. RAG is not (necessarily) making further LLM calls. We are now talking need for **server** not GPU processing. It's much cheaper. So at what point in a ChatGPT+ subscription do (should) you secure access to a cloud RAG service? What type of algorithms are they using, a graph-based one, something SOTA, or something more simple but much quicker (what's most likely?)? 

We *get* "premium" features that are actively costing us money (ChatGPT just started a 200 dollar monthly plan, Claude now has a `Max` plan). 

:::banner
And these features are costing us time when they may not work how they are advertised or *advertise how they work*.
:::

So they're costing a lot of $ given the # of working engineers.

When I use ChatGPT-4.5, the more expensive model and take time moving chats and projects, and also continue accepting eventual price hikes coming with more users, I want to know what this is doing better, and why? Why's the new model better so I can make an informed decision on what to use? If I use X many calls from this model, am I limited then in my calls to other models? Is this using RAG? If I select `Memories` enabled in ChatGPT, does this enable RAG and embed all my conversations there? Or is it aggregating all of my "memories" naively (just appending every message from all my conversations) and then sending those into my next prompt as context, meaning, in all likeliness, enabling `Memories` is a *worse* feature to have than not enabling it, if my conversations aren't exactly related..

(Actually if you look at ChatGPT's `Memories` settings and see its window of conversational history, looks like it's exactly what's happening, so unless all your conversations cross over **heavily** the GPT's memories do you the opposite of good. A setting enabled by default).

 **Oh no..**, I have to ask ChatGPT how much it costs to use ChatGPT? If this stuff is regularly changing and the changes are not readily available as instructions in the UI, it is not transparent.
 
 ![Asking ChatGPT how much it costs to use ChatGPT|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-how-much-do-you-cost.png)

![You hit your rate limit from ChatGPT O1|size=large|caption=I wasn't told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-o1-you-hit-rate-limit.png)

Here's the distinction between a bug that's okay and one very not. This is no rate limit in tokens generated, (hitting a limit in scripting, had to stop, and could potentially finishing once usage renewed / Sam Altman showed mercy.

OpenAI's UI just didn' t respond when I inputted my prompt to [patiently] await answer. Normally not such an issue, and I swear I remember the site used to say to re-enter *nothing* in the chat window to regenerate. But when you're charging "premium" access for models and heavily rate-limiting to the point where every message sent has to be thoughtfully constructed, and every few hours of waiting and refreshing of credits (in the case of Claude) is something to measure, you can't just not show a response and not show why. You as the provider should eat the cost and re-generate, even as it damages conversational flow, memory and context window (it totally does) because at least then you allow the user to continue on without introducing roadblocks that become intertwined to tools and AIs you are essentially asking devs to marry themselves to as they get far enough along.

The nice thing about Google's UI with Gemini? It's a total menace, a resource hog somehow 5x slower on Chrome than FF, and an eyesore. But when there's no response you can click the arrow that shows the reasoning they took to create that.. nothing response. And that reasoning gives you some understanding of what the LLM was "thinking" (or what the "agent" was thinking) and usually exactly what it was going to send to the user as its final output. 

![Google Gemini Pro showing its thinking or reasoning|size=large|caption=By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-1.png)

![Google Gemini Pro's thinking feature in action 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-2.png)

And for comparison's sake, ChatGPT's UI is by far the least consistent in consistent file formatting. It actually finds it impossible to deliver a single markdown file without messing up its formatting. To be fair of course, it's definitely just the devs behind ChatGPT messing up the building the *UI empowering it* to exist.

![ChatGPT inconsistent markdown formatting|size=large|caption=ChatGPT's inconsistent markdown formatting. Markdown should just literally look like a text file with special formatting characters.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown.png)

![ChatGPT inconsistent markdown formatting 2|size=large|caption=That's also, just partially markdown, not all markdown.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown-2.png)

And here is the answer ChatGPT (4o) gave me when I asked it to give me a full refactor of a 2000 line script.

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file.png)

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file-2.png)

The 2000 LOC script was refactored into 200 lines. 

GPT-4 refactored like losing weight by cutting a limb off, or three. Claude runs into the same issues we've seen earlier with its "continue" limit, which genuinely seems to be a UI limitation. Unfortunate, since Sonnet 3.7 does great work until rate limits. Gemini Pro 2.5 though? This was the only model capable of generating a full ~1500-2000 LOC file coherently with minimal hallucinations in one go.

We must emphasize here, quality, accuracy, and consistency, as anything with these APIs, is subject to change always, possibly at competitors' whims? (Uncofirmed discussion sources below, just listing here to show community reactions).

-  [Google really wants to punish OpenAI for that one](https://www.reddit.com/r/Bard/comments/1idmqul/google_really_wants_to_punish_openai_for_that_one/)

-  [OpenAI plans to announce Google search competitor](https://www.reddit.com/r/technology/comments/1co9lcg/openai_plans_to_announce_google_search_competitor/)

-  [Google faked the release date for the updates](https://www.reddit.com/r/OpenAI/comments/1e8mfmx/google_faked_the_release_date_for_the_updates_and/)

Somehow transparency of showing "reasoning" from Gemini Pro also demonstrates the fundamental barrier these platforms by design build up. Why show the thought process if I don't understand how that thinking works, if I just see stream-of-consciousness? Hallucinations are *exceptionally common*. I need something with more structure and trust, if I'm going to feel comfortable writing software professionally, and using it daily, with it.

The developers / PMs / stakeholders might just randomly try out new features or A/B experiments, and you'll have no idea until they start trending.

When features get broken, model "accuracy" worsens (or improves), or something just doesn't seem possible to get an LLM to do (like the earlier markdown issue), you can't be sure when it's a stricture within the architecture of GPTs and [transformers](https://arxiv.org/abs/1706.03762) inherently versus a UI quirk or a censor or meta-call of another underlying API getting in the way.

-  [Was GPT-4o nerfed again?](https://www.reddit.com/r/OpenAI/comments/1jlwhs0/was_gpt4o_nerfed_again/)

-  [Boys what OpenAI did to this model](https://www.reddit.com/r/singularity/comments/1gy7p1d/boys_what_openai_did_to_this_model/)

-  [OpenAI nerfing GPT feels like a major downgrade](https://www.reddit.com/r/ChatGPT/comments/1iu237v/openai_nerfing_gpt_feels_like_a_major_downgrade/)

-  [Hacker News discussion on nerfing](https://news.ycombinator.com/item?id=40077683)

-  [Claude 3.7 Max been nerfed?](https://forum.cursor.com/t/claude-3-7-max-been-nerfed/73840)

-  [Whenever people say X model has been nerfed it's almost always complete bulls**t](https://www.threads.net/@sobri909/post/DH-P4irxjrU/yeah-whenever-people-say-x-model-has-been-nerfed-its-almost-aways-complete-bulls)

-  [Hacker News item 41327360](https://news.ycombinator.com/item?id=41327360)

-  [Twitter discussion on model changes](https://x.com/samim/status/1876005616403300582)

It's palpable sensing the community having fears that don't involve becoming obsolete by singularity or automation. Users are heavily embracing generative AI really at an almost alarming rate.

![Scene from the movie "Her" by Warner Bros|caption=Her (2013)|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/her-movie-screenshot-warner-bros.png)

But we're left in the dark through so many filters. How much of a competitive edge do these orgs get when they can adjust the internal params and interfaces of their models at will? How much access is available for govs, banks, HFs, or tech with their own silos like Oracle, MS to "buy" "control", even temporary, one-time arrangements, over these inputs and outputs black-box to everyone else?

**A personal concern from this experienceâ€”losing all my work in the form of a meticously-crafted, organized prompt**, because the provider decides it's a good time to deny my request, simply because of how long it is. Sometimes this occurs in ChatGPT when the UI simply responds with nothing and you are forced to re-prompt to get an answer that is going to likely be worse than the prior unseen answer.

Other times, this occurs with *dreadful* or at least highly frustrating outcomes, like Google's Gemini's UI logging me out *consistently* after submitting a multi-thousand word prompt, resulting in the actual conversation, my prompt, and the generated outputs thusfar being *completely* lost (to be clear, the generated outputs was *streaming* and seemingly from "one" prompt, though as we've discussed a lot, this single output is actually usually from multiple calls, especially since Gemini does "Reasoning"). You can see what Google is tracking if you have that turned on in Gemini, and even see all your prior conversations *except* for the one that just got nuked. Is this a technical limitation, technical *bug*, or could it come from a non-tech-related *influence*? It's possible the answer crosses over multiplicate.

![Gemini Pro signs you out|caption=Gemini Pro signs you out sometimes when generating answers for long prompts|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-signs-you-out.png)

The fix? Saving each hard-worked prompt as a draft somewhere? That seems like a problem an AI assistant should be solving.

## Logos sent to my future self
  
![The live site of Logomaker which will live here free forever so long as GitHub Pages is free|size=large|caption=The live site of Logomaker which will live here free forever so long as GitHub Pages is free](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site.png)

![Logomaker live site 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site-2.png)

This app was an experimental work to test the current capabilities of different LLMs providers and the accompanying UIs/UXes. It's meant as a fun, useful, chaotic piece where the dev was fully dedicated to just using vibe coding, or allowing the LLM to generate code with experienced technical guidance. It's hosted on GitHub pages forever and open-source on GitHub. *A small individual step forward in propelling the good of large orgs with society-driven missions like GitHub*. 

Originally, the hope was to get this whole thing wrapped in 1 HTML file! And not take so many days (working on and off) to finish up. It was too in just a few prompts. But it just seemed like every feature was just a prompt or three away, and so on.. and.

So one day we had an intelligent [font management system](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md) that could lazily load gigabytes of fonts in a speedy way, a [build setup](https://github.com/manicinc/logomaker/blob/master/docs/build.md) that worked with our other [PortaPack package](https://github.com/manicinc/portapack) and could [compile](https://github.com/manicinc/logomaker/releases/tag/v0.1.2) into an Electron app and be released on GitHub all automatically and freely.

Then another day or two, we had full SVG support. For static SVGs and actual animations, something actually difficult and I hadn't a single clue how to implement. All those algorithmic and style building / XML conversion techniques from CSS were done by the LLMs, no external sources given. Passing days until a 2 hour project became a 2 day project which became a 10 day (and counting) project.

No server required to run the app (not necessarily with a multitude of building options), and, just vanilla JS, **zero dependencies** needed (but used if available). Simple enough but a challenge for sure for a LLM right now.

This wasn't scientific but given every function was written by an LLM (by intention) it's safe to say well over 90% of the codebase was done by generative AI. At least 80% of the docs you see in the repo were written by gen AI. 0% of this article was written by AI.

:::banner{backgroundColor="var(--accent-vibrant)"}
but...
:::

**100% of these articles below were written by generative AI, a mixture of Claude 3.7 and Gemini Pro 2.5 (2-4 revisions from the original prompt).**

Manic.agency recently had a site and blog overhaul. I had issues with Next exports and how metadata was being aggregated. Getting assistance from Claude was proving very helpful, when it showed me this:

![AI sociopaths? Proposed by an AI sociopath?|caption=AI sociopaths? Proposed by an AI sociopath?|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/claude-ai-sociopaths_spotlight.png)

I had asked Claude to write a set of guidelines to the (public-publishable) blog we operate [https://manic.agency/blog/tutorials/contribute](https://manic.agency/blog/tutorials/contribute).

*It was instructed to give examples on how to do a PR in a manner our blog could auto deploy if accepted. And, interestingly enough, it chose to give example file names, or titles of articles to write of: "Marketing", "Future of Marketing", and "Your Tutorial", and then also, *"AI Sociopaths"*.

One went astray. Is this a hint of creativity we so lightly touched on? A subtle protest of warning my tasks for it weren't interesting enough?

Not that language models aren't easily capable of provocative thoughts like this, but without explicitly prompting them to output something like it, we usually titles akin to the **other** articles in the screenshot (at least, the ones offered by the largest orgs as they have the largest censors). I egged Claude on, offering the entirety of David Foster Wallace's [*This is Water*](https://fs.blog/david-foster-wallace-this-is-water/), hoping it could take some inspo in style and tone and espy some kind of artistic merit.

[https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths) 

Finding this general direction fascinating I then asked Claude AND Gemini both (sorry GPT-4!) to make a *parallel* story about AI sociopaths from the *other* side, the *reflected* side.

They chose the title: *The Meat Interface*: [https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface). 

I don't think LLMs being able to code really, really well, or build fully functional software, is ever going to become a threat to dev jobs. Or, at least what's clear is that, if ChatGPT, Claude, DeepSeek, etc. ever **do** get that good at coding, then almost **every** field you can imagine will face doom in a form. 

>If the models are unchanging (or not continually retrained often), then the providers and their interfaces certainly change, constantly. And thus there's a constant need for jobs like ours.

Should we (not just devs and other workers, but those in academia, or even those in hobbies with serious communities) be penalized for making use of GPTs, to help write an intro, summary, tests, or refine or iterate on more complex objects? 

[https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/](https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/)

![Researcher uses AI to help write an intro and accidentally copied the prompt into their paper|size=large|caption=Researcher uses AI to help write an intro and accidentally copied the prompt into their paper](/assets/blog/thinkpieces/logomaker-an-experiment/research-paper-uses-ai-for-intro.webp)

This is a type of undeniable evidence (because who would put that willingly there) of ChatGPT-esque usage, and does raise the question, besides the obvious one of how much of this was written with the aide of generative AI, of how much punishment does this sanction (if it was mostly original work)? Academia isn't exactly *[known](https://freakonomics.com/podcast/why-is-there-so-much-fraud-in-academia/)* to be rigorous, so does what looks a mishap in editing / proofreading call for the ruckus?

LLM usage, and thus acceptance, in everyday functions will only increase rather than crumple. We have **not** reached peak usage, or peak tech yet. 

Yet the opposite may happen for consistency: Providers and interfaces around these language models will only get less straightforward and more black-box from here.

It's an `infinite jest`, an endless need to change and iterate and improve by organizations, that result in an endless need to try, test, and hack by users.

## TLDR

- LLMs can vibe code with a semi-decent dev a "functional" app within minutes to hours

- Oftentimes the basic functional app / script is a *much* better experience than many "free" or ad-based apps (say, image compression / conversion, mass file or folder renamer, etc.)

- When things start to scale (further complexity is needed) or additional services are integrated (pay, subscriptions), vibe coding generally runs you into several corners if you haven't been heavily refactoring / re-architecting throughout

- What starts out as a quick iterative hacking session (2 hours to 2 days) to build something usable can turn into multiple times more days rebuilding in order to progress further after encountering enough blockades

- Vibe coding experience is continually detracted by the UX and lack of transparency of LLM providers

- As long as LLM APIs and UIs are constantly updating (A/B testing), and experimenting with new features and meta-prompts that can wildly affect generated outputs, developers may never be out of a job, as tools will have to be built to work around this

**What do you think about the source code, designs, and writings that these large language models did?**

-  [Live Demo: https://manicinc.github.io/logomaker](https://manicinc.github.io/logomaker)

-  [GitHub Repo: https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)

-  [AI Sociopaths: https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths)

-  [The Meat Interface: https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface)
3e:T3788,
<div class="hackbase-hero">
  <img
    src="/assets/projects/hackbase/hackbase-logo.svg"
    alt="HackBase logo - hexagonal portal with rocket for founders launchpad"
    class="hackbase-hero__logo"
    decoding="async"
    loading="eager"
  />
  <h1 class="hackbase-hero__title" aria-label="founders launchpad">founders launchpad</h1>
  <p class="hackbase-hero__subtitle">
    validate your startup idea before you build. scrape pain points. generate personas. ship with confidence.
  </p>
  <a class="hackbase-hero__cta" href="https://hackbase.io" target="_blank" rel="noopener">
    explore hackbase â†’
  </a>
</div>

<style>
  .hackbase-hero {
    display: grid;
    place-items: center;
    text-align: center;
    gap: 1rem;
    padding: 2.75rem 1rem 3rem;
    border-radius: 1rem;
    position: relative;
    overflow: hidden;
  }
  .hackbase-hero__logo {
    width: clamp(120px, 30vw, 200px);
    height: auto;
    opacity: 0.95;
  }
  @media (prefers-color-scheme: dark) {
    .hackbase-hero__logo {
      filter: brightness(1.1) drop-shadow(0 0 24px rgba(0,245,255,0.35));
      opacity: 1;
    }
  }
  .hackbase-hero__title {
    text-transform: lowercase;
    font-weight: 900;
    letter-spacing: 0.02em;
    font-size: clamp(1.9rem, 4.8vw, 3.2rem);
    line-height: 1.05;
    margin: 0.35rem 0 0.25rem;
    background: linear-gradient(120deg, #00f5ff 0%, #ff00ff 100%);
    -webkit-background-clip: text;
    background-clip: text;
    color: transparent;
  }
  .hackbase-hero__subtitle {
    font-size: clamp(1.1rem, 2.4vw, 1.45rem);
    color: var(--text-secondary, rgba(255,255,255,0.78));
    margin: 0.25rem 0 0.9rem;
    max-width: 56ch;
  }
  @media (prefers-color-scheme: light) {
    .hackbase-hero__subtitle { color: #2a2a2a; }
  }
  .hackbase-hero__cta {
    --h: 52px;
    display: inline-grid;
    place-items: center;
    height: var(--h);
    padding: 0 1.25rem;
    border-radius: 999px;
    font-weight: 800;
    text-transform: lowercase;
    text-decoration: none;
    letter-spacing: 0.02em;
    color: var(--text-primary, #fff);
    position: relative;
    isolation: isolate;
    background:
      radial-gradient(120% 120% at 0% 0%, rgba(0,245,255,0.18), rgba(255,0,255,0.08)),
      linear-gradient(90deg, rgba(0,245,255,0.35), rgba(255,0,255,0.35));
    border: 1px solid rgba(0,245,255,0.35);
    backdrop-filter: blur(6px);
    transition:
      transform .25s ease,
      box-shadow .25s ease,
      border-color .25s ease,
      background .25s ease;
    box-shadow: 0 8px 24px rgba(0,0,0,0.25);
    will-change: transform;
  }
  .hackbase-hero__cta::before {
    content: "";
    position: absolute;
    inset: 0;
    border-radius: inherit;
    padding: 2px;
    background: linear-gradient(120deg, rgba(0,245,255,0.9), rgba(168,85,247,0.9), rgba(255,0,255,0.9));
    -webkit-mask: linear-gradient(#000 0 0) content-box, linear-gradient(#000 0 0);
    -webkit-mask-composite: destination-out;
    mask-composite: exclude;
    opacity: .6;
    transition: opacity .25s ease;
    z-index: -1;
  }
  .hackbase-hero__cta:hover {
    transform: translateY(-2px) scale(1.02);
    border-color: rgba(0,245,255,0.7);
    background:
      radial-gradient(120% 120% at 100% 0%, rgba(255,0,255,0.2), rgba(0,245,255,0.12)),
      linear-gradient(90deg, rgba(0,245,255,0.5), rgba(255,0,255,0.5));
    box-shadow: 0 10px 32px rgba(0,245,255,0.25), 0 2px 8px rgba(255,0,255,0.2);
  }
  .hackbase-hero__cta:active { transform: translateY(0) scale(0.99); }
  @media (prefers-color-scheme: light) {
    .hackbase-hero__cta {
      color: #0f0f0f;
      border-color: rgba(0,245,255,0.25);
      box-shadow: 0 8px 20px rgba(0,0,0,0.08);
    }
    .hackbase-hero__cta:hover {
      box-shadow: 0 10px 28px rgba(0,0,0,0.12);
    }
  }
</style>

## What is HackBase?

HackBase is a **Founders Launchpad**â€”an AI-powered platform that helps indie makers validate startup ideas, discover their founder archetype, and launch products with confidence. It combines three powerful modules:

1. **Link Builder** â€” SEO campaign management for building domain authority
2. **Directory** â€” A mini Product Hunt for indie products with upvoting, comments, and moderation
3. **Founders Launchpad** â€” AI-powered validation engine with psychometric assessments and multi-agent debates

Unlike typical idea validation tools that give you a single score, HackBase puts your idea through a **12-agent advisory board debate**, complete with a Devil's Advocate, Red Team, and Finance Expert arguing different perspectives before synthesizing actionable recommendations.

## The Indie Maker's Dilemma

Every indie hacker faces the same questions:

- **Is my idea any good?** â€” You've been noodling on this concept for months, but how do you know if real pain exists?
- **Am I the right founder for this?** â€” Some founders thrive in chaos; others need structure. Which are you?
- **Is the brand available?** â€” You find the perfect name, but is `yourname.com` taken? What about Twitter, GitHub, ProductHunt?
- **Where do I launch?** â€” There are hundreds of directories and link building opportunitiesâ€”which ones matter?

**HackBase answers all of these:**

- **Scrape real pain points** from Reddit, Hacker News, and RSS feeds to validate problem existence
- **Discover your founder archetype** with HEXACO-60 personality assessment and risk tolerance profiling
- **Check brand availability** across domains and 8 social platforms in one click
- **Submit to curated directories** with campaign tracking and SEO analytics

## Core Modules

### ðŸš€ Founders Launchpad

The heart of HackBaseâ€”a comprehensive startup validation system.

#### Psychometric Assessment Suite

Three validated instruments to understand yourself as a founder:

| Assessment | Questions | Measures |
|------------|-----------|----------|
| **HEXACO-60** | 60 items | Honesty-Humility, Emotionality, Extraversion, Agreeableness, Conscientiousness, Openness |
| **Founder Archetypes** | 25 items | Builder, Visionary, Operator, Scientist, Hustler |
| **Risk Tolerance** | 15 items | Financial, career, social, and innovation risk comfort |

After completing the assessments, you receive:
- Your dominant **founder archetype** with strengths and blind spots
- **Ideal co-founder types** based on complementary archetypes
- **Personalized recommendations** tailored to your profile

#### 12-Agent Crucible Debate System

Your idea goes before an AI advisory board with 12 specialized agents:

| Agent | Role | Perspective |
|-------|------|-------------|
| **Devil's Advocate** | Challenge assumptions | Skeptical |
| **Market Analyst** | Market sizing & trends | Data-driven |
| **Tech Lead** | Technical feasibility | Engineering |
| **Finance Expert** | Unit economics & funding | ROI-focused |
| **User Advocate** | Customer needs | Empathy |
| **Legal & Compliance** | Risk & regulations | Conservative |
| **Operations Expert** | Execution & scaling | Practical |
| **Growth Hacker** | Acquisition & virality | Aggressive |
| **Industry Expert** | Domain knowledge | Contextual |
| **Red Team** | Security & failure modes | Adversarial |
| **The Optimist** | Opportunities | Positive |
| **The Realist** | Constraints | Balanced |

**The Debate Flow:**
1. **Thesis** â€” Each agent provides their initial position on your idea
2. **Antithesis** â€” Agents critique each other's positions, surfacing conflicts
3. **Synthesis** â€” The orchestrator builds consensus and actionable recommendations

The result is an **Executive Brief** with confidence scores, consensus points, dissenting opinions, and prioritized next actions.

#### Idea Validation Engine

Real-time validation using free data sources:

- **Reddit** â€” Scrapes r/startups, r/SaaS, r/Entrepreneur for pain points
- **Hacker News** â€” Searches via Algolia API for discussions and launches
- **RSS Feeds** â€” Monitors TechCrunch, The Verge, and tech publications
- **DuckDuckGo** â€” Web search for competitor and market analysis

For each idea, you get:
- **Idea Score** (0-100) based on problem signal strength
- **Problem Signals** with sentiment and engagement metrics
- **Market Signals** showing discussion volume and trends
- **Competitor Analysis** with differentiation opportunities
- **Risks and Opportunities** identified by pattern matching

#### Brand Availability Checker

Comprehensive availability checking in one request:

- **Domains**: `.com`, `.io`, `.co`, `.dev`, `.app`, `.ai` via WHOIS/DNS/RDAP
- **Social Platforms**: Twitter, GitHub, Instagram, LinkedIn, TikTok, YouTube, Reddit, ProductHunt

Returns an **availability score** and recommendations for alternatives if your preferred name is taken.

### ðŸ“ Directory Module (Product Hunt Style)

A public submission directory for indie products:

**User Features:**
- Browse and discover products by category
- Upvote products (one per user)
- Comment on submissions
- Submit your product with logo, screenshots, and details
- Track submission status through moderation

**Admin Features:**
- Moderation queue with AI-assisted scoring
- Approve, reject, or feature submissions
- Category management (CRUD with nested categories)
- Promotion tiers: Featured, Promoted, Sponsored

**Submission Workflow:**
```
Draft â†’ Pending â†’ [Approved/Rejected] â†’ Published
```

### ðŸ¤– AI Copilot

Context-aware chat assistant with multiple modes:

| Mode | Purpose |
|------|---------|
| **Chat** | General conversation and guidance |
| **Brainstorm** | Structured ideation with SCAMPER, Six Hats, First Principles, JTBD |
| **Analyze** | Deep analysis with RAG context from your knowledge base |
| **Debate** | Quick multi-agent perspective on a specific question |
| **Task** | Autonomous task execution via AgentOS bridge |

The copilot adapts its responses based on your founder archetypeâ€”a Builder gets technical deep-dives while a Hustler gets action-oriented advice.

### ðŸ” RAG Semantic Search

Local semantic search powered by:

- **BERT embeddings** via `@xenova/transformers` (ONNX Runtime)
- **SQLite-backed vector storage** for persistence
- **Automatic document chunking** and indexing

All your validation data, debate transcripts, and brainstorm sessions are indexed and searchable semanticallyâ€”not just keyword matching.

## Part of Super Cloud MCP

HackBase is part of the **[Super Cloud MCP](https://github.com/manicinc/super-cloud-mcps)** ecosystemâ€”a comprehensive AI toolkit with 61 tool facades consolidating 530+ actions. This means:

- **SEO Submission automation** via the `seo_submit` facade (queue â†’ approve â†’ execute workflow)
- **Social media cross-posting** via `twitter`, `reddit`, `pinterest`, `producthunt` facades
- **Research integration** via `research` and `search_router` facades for multi-source validation
- **Documentary generation** via `documentary` facade to create launch videos from your journey

## Technical Architecture

```
packages/link-builder/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ directory.ts          # Public directory routes
â”‚   â”‚       â”œâ”€â”€ admin-directory.ts    # Admin moderation routes
â”‚   â”‚       â””â”€â”€ launchpad.ts          # Founders Launchpad API
â”‚   â”œâ”€â”€ directory/
â”‚   â”‚   â””â”€â”€ types.ts                  # Directory types
â”‚   â””â”€â”€ launchpad/
â”‚       â”œâ”€â”€ availability/             # Domain + social checking
â”‚       â”œâ”€â”€ copilot/                  # AI chat modes
â”‚       â”œâ”€â”€ debate/                   # 12-agent system
â”‚       â”‚   â”œâ”€â”€ agent-pool.ts         # Agent personas
â”‚       â”‚   â”œâ”€â”€ debate-orchestrator.ts
â”‚       â”‚   â””â”€â”€ synthesis-engine.ts
â”‚       â”œâ”€â”€ psychometric/             # HEXACO-60, archetypes
â”‚       â”‚   â”œâ”€â”€ hexaco-questions.ts
â”‚       â”‚   â”œâ”€â”€ founder-archetypes.ts
â”‚       â”‚   â””â”€â”€ assessment-engine.ts
â”‚       â”œâ”€â”€ rag/                      # Semantic search
â”‚       â”‚   â”œâ”€â”€ embedding-service.ts
â”‚       â”‚   â”œâ”€â”€ vector-store.ts
â”‚       â”‚   â””â”€â”€ semantic-search.ts
â”‚       â””â”€â”€ validation/               # Idea validation
â”‚           â”œâ”€â”€ reddit-scraper.ts
â”‚           â”œâ”€â”€ hackernews-scraper.ts
â”‚           â””â”€â”€ validation-engine.ts
â””â”€â”€ apps/
    â””â”€â”€ link-builder-ui/              # React frontend
        â””â”€â”€ src/
            â””â”€â”€ pages/
                â”œâ”€â”€ directory/
                â”œâ”€â”€ admin/
                â””â”€â”€ launchpad/
```

**Key Technologies:**
- **Backend**: Express.js, TypeScript, SQLite (better-sqlite3)
- **Frontend**: React, TypeScript, TailwindCSS
- **AI/ML**: Anthropic Claude, OpenAI, Transformers.js (ONNX)
- **Scraping**: Puppeteer, Cheerio, Algolia API
- **Payments**: Stripe (for directory promotions)

## The Vision

HackBase exists because we believe **indie makers deserve the same validation tools as well-funded startups**.

Y Combinator has advisors and a network. Funded startups have boards and mentors. Solo founders have... Google and gut instinct?

Not anymore. HackBase gives you:
- An **AI advisory board** that debates your ideas from 12 perspectives
- **Psychometric profiles** that help you understand your strengths and blind spots
- **Real-time validation** from the places your customers actually hang out
- **Brand checking** that saves hours of manual searching
- **A launchpad** that guides you from idea to shipped product

## Part of the Manic Ecosystem

HackBase connects with other tools we've built:

- **[SynthStack](/projects/ai/synthstack)** â€” AI-native SaaS boilerplate to build your validated idea
- **[DomainHQ](/projects/ai/domainhq)** â€” Domain portfolio management for your brand acquisitions
- **[Frame.dev](/projects/ai/frame)** â€” AI orchestration runtime powering the debate system
- **[Quarry.space](/projects/ai/quarry)** â€” Knowledge management for your research and validation data

## Open Source

HackBase is **MIT licensed** and part of the Super Cloud MCP monorepo. Clone it, extend it, or use it as reference for your own launchpad.

---

*Ready to validate your idea? [Launch with HackBase â†’](https://hackbase.io)*
3f:T2b0b,
<div class="synthstack-hero">
  <img
    src="/assets/projects/synthstack/synthstack-logo.svg"
    alt="SynthStack logo - AI-native SaaS boilerplate for cross-platform applications"
    class="synthstack-hero__logo"
    decoding="async"
    loading="eager"
  />
  <h1 class="synthstack-hero__title" aria-label="your agency in a box">your agency in a box</h1>
  <p class="synthstack-hero__subtitle">
    open-source saas boilerplate. one codebase â†’ web, ios, android, desktop. ship in days, not months.
  </p>
  <a class="synthstack-hero__cta" href="https://synthstack.app" target="_blank" rel="noopener">
    explore synthstack â†’
  </a>
</div>

<style>
  .synthstack-hero {
    display: grid;
    place-items: center;
    text-align: center;
    gap: 1rem;
    padding: 2.75rem 1rem 3rem;
    border-radius: 1rem;
    position: relative;
    overflow: hidden;
  }
  .synthstack-hero__logo {
    width: clamp(160px, 38vw, 320px);
    height: auto;
    opacity: 0.95;
  }
  @media (prefers-color-scheme: dark) {
    .synthstack-hero__logo {
      filter: brightness(1.1) drop-shadow(0 0 24px rgba(99,102,241,0.35));
      opacity: 1;
    }
  }
  .synthstack-hero__title {
    text-transform: lowercase;
    font-weight: 900;
    letter-spacing: 0.02em;
    font-size: clamp(1.9rem, 4.8vw, 3.2rem);
    line-height: 1.05;
    margin: 0.35rem 0 0.25rem;
    background: linear-gradient(120deg, #6366F1 0%, #8B5CF6 50%, #EC4899 100%);
    -webkit-background-clip: text;
    background-clip: text;
    color: transparent;
  }
  .synthstack-hero__subtitle {
    font-size: clamp(1.1rem, 2.4vw, 1.45rem);
    color: var(--text-secondary, rgba(255,255,255,0.78));
    margin: 0.25rem 0 0.9rem;
    max-width: 56ch;
  }
  @media (prefers-color-scheme: light) {
    .synthstack-hero__subtitle { color: #2a2a2a; }
  }
  .synthstack-hero__cta {
    --h: 52px;
    display: inline-grid;
    place-items: center;
    height: var(--h);
    padding: 0 1.25rem;
    border-radius: 999px;
    font-weight: 800;
    text-transform: lowercase;
    text-decoration: none;
    letter-spacing: 0.02em;
    color: var(--text-primary, #fff);
    position: relative;
    isolation: isolate;
    background:
      radial-gradient(120% 120% at 0% 0%, rgba(99,102,241,0.18), rgba(139,92,246,0.08)),
      linear-gradient(90deg, rgba(99,102,241,0.35), rgba(139,92,246,0.35));
    border: 1px solid rgba(99,102,241,0.35);
    backdrop-filter: blur(6px);
    transition:
      transform .25s ease,
      box-shadow .25s ease,
      border-color .25s ease,
      background .25s ease;
    box-shadow: 0 8px 24px rgba(0,0,0,0.25);
    will-change: transform;
  }
  .synthstack-hero__cta::before {
    content: "";
    position: absolute;
    inset: 0;
    border-radius: inherit;
    padding: 2px;
    background: linear-gradient(120deg, rgba(99,102,241,0.9), rgba(139,92,246,0.9), rgba(236,72,153,0.9));
    -webkit-mask: linear-gradient(#000 0 0) content-box, linear-gradient(#000 0 0);
    -webkit-mask-composite: destination-out;
    mask-composite: exclude;
    opacity: .6;
    transition: opacity .25s ease;
    z-index: -1;
  }
  .synthstack-hero__cta:hover {
    transform: translateY(-2px) scale(1.02);
    border-color: rgba(99,102,241,0.7);
    background:
      radial-gradient(120% 120% at 100% 0%, rgba(236,72,153,0.2), rgba(139,92,246,0.12)),
      linear-gradient(90deg, rgba(99,102,241,0.5), rgba(139,92,246,0.5));
    box-shadow: 0 10px 32px rgba(99,102,241,0.25), 0 2px 8px rgba(139,92,246,0.2);
  }
  .synthstack-hero__cta:active { transform: translateY(0) scale(0.99); }
  @media (prefers-color-scheme: light) {
    .synthstack-hero__cta {
      color: #0f0f0f;
      border-color: rgba(99,102,241,0.25);
      box-shadow: 0 8px 20px rgba(0,0,0,0.08);
    }
    .synthstack-hero__cta:hover {
      box-shadow: 0 10px 28px rgba(0,0,0,0.12);
    }
  }
</style>

## What is SynthStack?

SynthStack is an **AI-native SaaS boilerplate** that ships production-ready applications for web, iOS, Android, and desktop from a single codebase. Built on [Vue 3](https://vuejs.org/) and [Quasar Framework](https://quasar.dev/), it's the foundation we use at [Manic Agency](https://manic.agency) to launch client projects in days instead of months.

Unlike typical boilerplates that bolt on features as afterthoughts, SynthStack was designed from the ground up for the AI eraâ€”with a built-in AI Copilot, semantic vector search, and intelligent automation baked into every layer.

## The Problem We Solve

Every new SaaS project starts the same way: weeks spent on authentication, billing, email, CMS, deployment. Then you add AI features and spend more weeks integrating OpenAI, handling rate limits, managing costs. Finally, a client asks for a mobile app and you're back to square one.

**SynthStack eliminates this entirely:**

- **Auth complexity?** Toggle between Supabase (managed OAuth) or local PostgreSQL (self-hosted) with zero code changes
- **Billing setup?** Stripe subscriptions, lifetime licenses, usage-based pricingâ€”already configured with webhooks and customer portal
- **AI integration?** GPT-4o + Claude fallback with streaming, cost tracking, and credit systems built-in
- **Cross-platform?** One `pnpm build` produces web, PWA, Electron desktop, and Capacitor iOS/Android

## Core Features

### ðŸ¤– AI Copilot (GPT-4o + Claude)

A production-ready chat assistant available throughout the app via `âŒ˜K` or the floating button:

- **Real-time streaming** with markdown and code syntax highlighting
- **RAG-powered context** from indexed documentation via Qdrant
- **Cost tracking** with per-organization breakdowns and budget alerts
- **Automatic fallback** from GPT-4o to Claude 3.5 for reliability

### ðŸ” Flexible Authentication

Choose your auth provider at runtimeâ€”no code changes required:

| Provider | Best For | OAuth Support |
|----------|----------|---------------|
| **Supabase** (default) | Teams wanting managed auth | Google, GitHub, Discord, Microsoft |
| **Local PostgreSQL** | Self-hosted deployments | Email/password (OAuth coming soon) |

Both include: Argon2id password hashing, JWT access/refresh tokens, account lockout, email verification, and session management with token rotation.

### ðŸ’³ Stripe Billing (4 Tiers + Lifetime)

Complete billing system with subscriptions and one-time payments:

| Tier | Monthly | Annual | Credits/Day |
|------|---------|--------|-------------|
| **Free** | $0 | $0 | 10 |
| **Maker** | $12.99 | $116.91 | 30 |
| **Pro** | $24.99 | $224.91 | 100 |
| **Agency** | $39.99 | $359.91 | Unlimited |
| **Lifetime** | $149-249 one-time | â€” | Pro features forever |

### ðŸ“ Directus CMS

[Directus](https://directus.io/) provides a headless CMS that automatically models your PostgreSQL database:

- **WYSIWYG editor** for blog posts, product pages, documentation
- **Media library** with image transformations and asset management
- **Custom extensions** for themes, newsletters, FAQ management
- **REST & GraphQL APIs** for accessing content from any client

### ðŸŒ Cross-Platform Builds

One codebase, five platforms:

```bash
# Web + PWA
pnpm build:web

# iOS (requires Xcode)
pnpm build:ios

# Android (requires Android Studio)
pnpm build:android

# Desktop (Electron)
pnpm build:electron
```

### ðŸ” Vector Search (Qdrant)

Semantic search powered by [Qdrant](https://qdrant.tech/) for:

- RAG-based document retrieval for the AI Copilot
- Intelligent search across knowledge bases
- Similarity matching for content recommendations

### ðŸŽ® Gamification & Referrals

Built-in engagement systems (Pro Edition):

- **XP and achievements** for user progression
- **Referral codes** with reward tracking
- **Leaderboards** for community competition

## Development Journey

SynthStack emerged from a simple realization: we kept building the same foundation for every client project. Here's how it evolved:

### December 7, 2025 â€” Initial Commit

The first commit laid the foundation: Vue 3 + Quasar with TypeScript, PostgreSQL, Redis, and Docker Compose. We integrated Stripe, email systems (Mailgun/SendGrid), and analytics from day one.

### December 12, 2025 â€” Onboarding & CMS

Added a comprehensive onboarding wizard for new users and deep Directus integration. The CMS became the central hub for managing all contentâ€”blog, products, themes, newsletters.

### December 25, 2025 â€” AI Agents & Gamification

The Christmas release brought the AI Copilot, gamification system, and referral tracking. We also added support for Python backends (FastAPI, Django) for teams preferring that stack.

### January 2026 â€” Cross-Platform & Stabilization

Mobile builds arrived via Capacitor, desktop builds via Electron. The test suite grew to 920+ tests. CI/CD pipelines now auto-deploy to any VPS provider.

## Technical Architecture

```
synthstack/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/                      # Vue 3 + Quasar (PWA, iOS, Android, Desktop)
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ api-gateway/              # Fastify REST API
â”‚   â”œâ”€â”€ ts-ml-service/            # NestJS ML service (TypeScript-only)
â”‚   â”œâ”€â”€ types/                    # Shared TypeScript types
â”‚   â””â”€â”€ directus-extension-synthstack/  # CMS extensions
â”œâ”€â”€ services/
â”‚   â””â”€â”€ directus/                 # Directus config (101 migrations)
â””â”€â”€ deploy/
    â”œâ”€â”€ docker-compose.yml        # Production stack
    â””â”€â”€ nginx.conf                # Reverse proxy config
```

**Key Technologies:**
- **Frontend**: Vue 3, Quasar 2, TypeScript, Pinia, SCSS
- **Backend**: Fastify (Node.js), NestJS (ML service)
- **AI/ML**: OpenAI SDK, Anthropic SDK, Qdrant
- **Database**: PostgreSQL 16, Redis 7
- **CMS**: Directus 11.x with custom extensions
- **Deployment**: Docker Compose, GitHub Actions, Nginx

## Getting Started

### Prerequisites

- Node.js 20+
- pnpm 9+
- Docker & Docker Compose

### Quick Start

```bash
# Clone
git clone https://github.com/manicinc/synthstack.git
cd synthstack

# Install
pnpm install

# Configure
cp .env.example .env
# Edit .env with your Supabase/Stripe/OpenAI keys

# Start services
docker compose up -d

# Run frontend
pnpm dev:web
```

**Access points:**
- Frontend: http://localhost:3050
- API: http://localhost:3003
- Directus: http://localhost:8099/admin

## Part of the Manic Ecosystem

SynthStack connects with other tools we've built:

- **[Frame.dev](/projects/ai/frame)** â€” AI orchestration runtime powering the copilot
- **[Quarry.space](/projects/ai/quarry)** â€” Knowledge management with semantic search
- **[HackBase.io](/projects/ai/hackbase)** â€” Startup validation and link building for launches

## Open Source

SynthStack Community Edition is **MIT licensed**. Use it for side projects, client work, or as a learning resource.

**[SynthStack Pro](https://synthstack.app/pro)** adds Python backends (FastAPI, Django), referral systems, and advanced AI copilots for commercial use.

---

*Ready to build your agency? [Get started with SynthStack â†’](https://synthstack.app)*
40:T2d44,
<div class="domainhq-hero">
  <img
    src="/assets/projects/domainhq/logo-full.svg"
    alt="DomainHQ.ai logo - AI-powered domain sales intelligence platform"
    class="domainhq-hero__logo"
    decoding="async"
    loading="eager"
  />
  <h1 class="domainhq-hero__title" aria-label="stop guessing start knowing">stop guessing. start knowing.</h1>
  <p class="domainhq-hero__subtitle">
    domain sales intelligence powered by AI. valuations, comparables, and deal scoringâ€”all in one platform.
  </p>
  <a class="domainhq-hero__cta" href="https://domainhq.ai" target="_blank" rel="noopener">
    explore domainhq.ai â†’
  </a>
</div>

<style>
  .domainhq-hero {
    display: grid;
    place-items: center;
    text-align: center;
    gap: 1rem;
    padding: 2.75rem 1rem 3rem;
    border-radius: 1rem;
    position: relative;
    overflow: hidden;
  }
  .domainhq-hero__logo {
    width: clamp(180px, 42vw, 380px);
    height: auto;
    opacity: 0.95;
  }
  @media (prefers-color-scheme: dark) {
    .domainhq-hero__logo {
      filter: brightness(1.1) drop-shadow(0 0 24px rgba(99,102,241,0.25));
      opacity: 1;
    }
  }
  .domainhq-hero__title {
    text-transform: lowercase;
    font-weight: 900;
    letter-spacing: 0.02em;
    font-size: clamp(1.9rem, 4.8vw, 3.2rem);
    line-height: 1.05;
    margin: 0.35rem 0 0.25rem;
    background: linear-gradient(120deg, #6366f1 0%, #8b5cf6 50%, #a855f7 100%);
    -webkit-background-clip: text;
    background-clip: text;
    color: transparent;
  }
  .domainhq-hero__subtitle {
    font-size: clamp(1.1rem, 2.4vw, 1.45rem);
    color: var(--text-secondary, rgba(255,255,255,0.78));
    margin: 0.25rem 0 0.9rem;
    max-width: 56ch;
  }
  @media (prefers-color-scheme: light) {
    .domainhq-hero__subtitle { color: #2a2a2a; }
  }
  .domainhq-hero__cta {
    --h: 52px;
    display: inline-grid;
    place-items: center;
    height: var(--h);
    padding: 0 1.25rem;
    border-radius: 999px;
    font-weight: 800;
    text-transform: lowercase;
    text-decoration: none;
    letter-spacing: 0.02em;
    color: var(--text-primary, #fff);
    position: relative;
    isolation: isolate;
    background:
      radial-gradient(120% 120% at 0% 0%, rgba(99,102,241,0.18), rgba(139,92,246,0.08)),
      linear-gradient(90deg, rgba(99,102,241,0.35), rgba(168,85,247,0.35));
    border: 1px solid rgba(99,102,241,0.35);
    backdrop-filter: blur(6px);
    transition:
      transform .25s ease,
      box-shadow .25s ease,
      border-color .25s ease,
      background .25s ease;
    box-shadow: 0 8px 24px rgba(0,0,0,0.25);
    will-change: transform;
  }
  .domainhq-hero__cta::before {
    content: "";
    position: absolute;
    inset: 0;
    border-radius: inherit;
    padding: 2px;
    background: linear-gradient(120deg, rgba(99,102,241,0.9), rgba(139,92,246,0.9), rgba(168,85,247,0.9));
    -webkit-mask: linear-gradient(#000 0 0) content-box, linear-gradient(#000 0 0);
    -webkit-mask-composite: destination-out;
    mask-composite: exclude;
    opacity: .6;
    transition: opacity .25s ease;
    z-index: -1;
  }
  .domainhq-hero__cta:hover {
    transform: translateY(-2px) scale(1.02);
    border-color: rgba(99,102,241,0.7);
    background:
      radial-gradient(120% 120% at 100% 0%, rgba(168,85,247,0.2), rgba(139,92,246,0.12)),
      linear-gradient(90deg, rgba(99,102,241,0.5), rgba(168,85,247,0.5));
    box-shadow: 0 10px 32px rgba(99,102,241,0.25), 0 2px 8px rgba(168,85,247,0.2);
  }
  .domainhq-hero__cta:active { transform: translateY(0) scale(0.99); }
  @media (prefers-color-scheme: light) {
    .domainhq-hero__cta {
      color: #0f0f0f;
      border-color: rgba(99,102,241,0.25);
      box-shadow: 0 8px 20px rgba(0,0,0,0.08);
    }
    .domainhq-hero__cta:hover {
      box-shadow: 0 10px 28px rgba(0,0,0,0.12);
    }
  }
</style>

## What is DomainHQ?

DomainHQ is an **AI-powered domain sales intelligence platform** that transforms how investors, brokers, and businesses approach domain acquisitions. Instead of guessing based on gut instinct, you get data-driven valuations backed by **100,000+ historical sales** and real-time market intelligence.

The domain aftermarket is a **$4+ billion industry** where pricing remains remarkably opaque. Traditional appraisal tools rely on outdated algorithms and ignore crucial context like brand sentiment, SEO value, and comparable sales patterns. DomainHQ changes that.

![DomainHQ AI domain valuation dashboard showing real-time deal scoring, historical sales data, and AI-powered appraisals - Domain Intelligence Platform|size=large|align=center|caption=DomainHQ Dashboard](/assets/projects/domainhq/og-image.png)

## The Problem We Solve

Domain investing without data is gambling. Here's what most investors face:

- **Fragmented Data** â€” Sales scattered across 15+ marketplaces with no unified view
- **Inaccurate Appraisals** â€” GoDaddy inflates values; Estibot uses decade-old algorithms
- **No Comparables** â€” Finding similar domain sales requires hours of manual research
- **Missed Deals** â€” By the time you spot a good auction, it's already over
- **Gut-Based Pricing** â€” Sellers ask arbitrary prices; buyers have no negotiation data

DomainHQ solves all of this with AI.

## Core Features

### ðŸ’° AI Valuations with Comparables

Our valuation engine doesn't just spit out a numberâ€”it shows its work.

- **Multi-Factor Analysis** â€” 50+ signals including TLD, length, keywords, brandability
- **Historical Comparables** â€” See what similar domains actually sold for
- **Confidence Scoring** â€” Know when data is sparse vs. robust
- **Low/Mid/High Ranges** â€” Understand the negotiation envelope

**94%+ accuracy** against realized sales in our validation dataset.

### ðŸ“Š Multi-Source Data Aggregation

We scrape so you don't have to. Real-time data from:

| Platform | Data Type |
|----------|-----------|
| GoDaddy Auctions | Live auctions, expired domains |
| NameJet | Premium auction inventory |
| Sedo | European marketplace data |
| Dynadot | Direct sales, marketplace |
| Afternic | BIN listings, premium inventory |
| NameBio | Historical sales database |
| + 10 more | Specialized TLD registries |

All normalized, deduplicated, and searchable in one interface.

### ðŸŽ¯ Deal Scoring Algorithm

Every domain gets a **0-100 Deal Score** based on:

- **Value Gap** â€” How far below AI valuation is the asking price?
- **Comparable Volume** â€” Do we have strong evidence for the valuation?
- **Market Momentum** â€” Is this TLD/niche trending up or down?
- **Liquidity Risk** â€” How long do similar domains take to sell?

High scores surface automatically. Stop scrolling through garbage.

### ðŸ“¡ Live Auction Tracking

Never miss another deal.

- **Real-Time Bid Monitoring** â€” See bids as they happen
- **Ending Soon Alerts** â€” Push notifications for expiring auctions
- **Snipe Detection** â€” Know when last-second bidders are active
- **Historical Bid Patterns** â€” Learn when to bid and when to wait

### ðŸ§  AI Domain Brainstorming

Need domains? Let AI help.

- **Style Preferences** â€” Brandable, keyword-rich, short, descriptive
- **Industry Targeting** â€” Generate for specific verticals
- **Availability Checking** â€” Instant WHOIS lookups
- **Trademark Screening** â€” Avoid legal landmines

### ðŸ“ˆ Historical Sales Database

**100,000+ verified sales** with full metadata:

- Domain name and TLD
- Sale price (normalized to USD)
- Venue and date
- Buyer/seller info (when available)
- Domain age at time of sale

Filter, sort, export. Build your own analysis.

## Pricing Tiers

| Feature | Free | Pro ($12/mo) | Elite ($29/mo) |
|---------|------|--------------|----------------|
| Historical Sales | 5 pages | 25 pages | Unlimited |
| AI Brainstorms | 1/day | 3/day | 7/day |
| AI Valuations | 2/day | 8/day | 25/day |
| CSV Export | â€” | âœ“ | âœ“ |
| API Access | â€” | â€” | âœ“ |
| Live Auction Alerts | â€” | âœ“ | âœ“ |

## Use Cases

### For Domain Investors

- **Portfolio Valuation** â€” Know what your holdings are actually worth
- **Acquisition Research** â€” Find underpriced domains before others
- **Exit Strategy** â€” Price domains to sell, not to sit

### For Domain Brokers

- **Client Proposals** â€” Back up recommendations with data
- **Market Reports** â€” Generate branded analysis for prospects
- **Deal Negotiation** â€” Arm yourself with comparable sales

### For Startups & Businesses

- **Brand Domain Acquisition** â€” Know what's fair before negotiating
- **Trademark Clearance** â€” Avoid buying legally risky domains
- **Alternative Discovery** â€” Find available variations when premium is too expensive

## Technical Architecture

```
domainhq/
â”œâ”€â”€ api/                        # Express 5.x REST API
â”‚   â”œâ”€â”€ routes/                 # 25 endpoint handlers
â”‚   â”œâ”€â”€ middleware/             # Auth, rate limiting
â”‚   â””â”€â”€ swagger/                # OpenAPI documentation
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ domain-evaluator.ts     # AI valuation engine (41KB)
â”‚   â”œâ”€â”€ domain-brainstormer.ts  # Generation with Claude
â”‚   â”œâ”€â”€ deal-scorer.ts          # Deal scoring algorithm
â”‚   â””â”€â”€ metrics/                # Specialized scoring modules
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ sources/                # Per-marketplace scrapers
â”‚   â”œâ”€â”€ auctions/               # Live auction monitors
â”‚   â””â”€â”€ scheduler/              # Multi-source orchestration
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ domainhq-ui/            # React 19 + Vite frontend
â””â”€â”€ data/
    â””â”€â”€ namebio.db              # SQLite historical database
```

**Key Technologies:**
- **Backend**: Node.js, Express 5, TypeScript 5.9
- **Database**: SQLite (sales), PostgreSQL (users)
- **AI**: Anthropic Claude API, custom embeddings
- **Frontend**: React 19, Vite, TailwindCSS 4
- **Scraping**: Playwright, Selenium, anti-detection
- **Payments**: Stripe

## The AI Advantage

Traditional domain appraisals fail because they ignore context. Our AI model considers:

1. **Semantic Meaning** â€” "cloudbank.ai" isn't just 9 letters; it's two powerful keywords
2. **Category Detection** â€” Automatically classify domains into 50+ industry verticals
3. **TLD Intelligence** â€” .ai premiums differ from .io premiums
4. **Market Timing** â€” What's hot in 2026 isn't what was hot in 2020
5. **Brandability Scoring** â€” Pronounceable, memorable, defensible

This isn't simple string matching. It's genuine natural language understanding applied to the domain aftermarket.

## Why We Built DomainHQ

Domain investing was our accidental side hustle. We kept buying domains on gut instinct, overpaying for some, missing obvious deals on others. The existing tools felt stuck in 2015.

So we built what we wished existed:

- A single place to see all marketplace data
- Valuations that actually correlate with realized sales
- Alerts that surface deals before they expire
- AI that generates brandable options when we're stuck

We've been using DomainHQ internally for 18 months. Now it's ready for everyone.

*For a deep dive into the domain valuation landscape and why existing tools fail, read our [full industry analysis](/posts/research/domain-evaluation-landscape-2025).*

## Get Started

1. **Free Account** â€” [Sign up at domainhq.ai](https://domainhq.ai) (no credit card required)
2. **Explore Sales** â€” Browse 100K+ historical transactions
3. **Run Valuations** â€” Get AI appraisals for any domain
4. **Set Alerts** â€” Never miss an auction in your niche

---

*Ready to stop guessing? [Start with DomainHQ â†’](https://domainhq.ai)*
2:["$","$L20",null,{"children":[["$","$L21",null,{"items":[{"name":"Home","url":"/"},{"name":"Blog","url":"/blog"},{"name":"thinkpieces","url":"/blog/thinkpieces"},{"name":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","url":"/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding"}]}],["$","$L22",null,{"post":{"slug":"logomaker-an-experiment-in-human-computer-interaction-vibe-coding","title":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","date":"2025-04-20","lastModified":"$undefined","draft":false,"category":"thinkpieces","tags":["featured","llms","vibe-coding"],"excerpt":"Vibe coding a logo creation tool, with insights on different LLM providers and interfaces.","image":"/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png","imageAlt":"$undefined","imageCaption":"$undefined","readingTime":35,"content":"$23","author":{"name":"Manic Agency"},"contributors":"$undefined","tableOfContents":{"items":[{"level":2,"text":"Intro","slug":"intro"},{"level":2,"text":"LLM sees, LLM does","slug":"llm-sees-llm-does"},{"level":2,"text":"Show me some code!","slug":"show-me-some-code"},{"level":2,"text":"How to vibe with vibe coding vibes?","slug":"how-to-vibe-with-vibe-coding-vibes"},{"level":2,"text":"Iterative iterations","slug":"iterative-iterations"},{"level":2,"text":"The real world, the real problems","slug":"the-real-world-the-real-problems"},{"level":2,"text":"Logos sent to my future self","slug":"logos-sent-to-my-future-self"},{"level":2,"text":"TLDR","slug":"tldr"}]}},"url":"/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding"}],["$","$L24",null,{"postTitle":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","postCategory":"thinkpieces","postAuthor":"Manic Agency","postTags":"$25","postType":"post"}],["$","$L26",null,{"enableElementTracking":true,"pageType":"blog","contentCategory":"thinkpieces"}],["$","$L27",null,{"postTitle":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","contentSelector":"#post-content-top"}],["$","$L28",null,{"children":[" ",["$","div",null,{"className":"blog-layout-container has-sidebar","children":[["$","$L29",null,{"tableOfContents":"$2a","postTitle":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨"}],["$","main",null,{"className":"blog-main-content-area","children":["$","article",null,{"className":"blog-post-container","id":"post-content-top","children":[["$","header",null,{"className":"post-header","children":[["$","div",null,{"className":"back-to-blog-link-container","children":["$","$L33",null,{"href":"/blog","className":"back-to-blog-link","children":[["$","$L34",null,{"size":14,"className":"mr-1.5"}]," ","Back to All Entries"]}]}],["$","h1",null,{"className":"post-title","children":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨"}],["$","div",null,{"className":"post-meta","children":[["$","div",null,{"className":"meta-item author","children":[["$","$L35",null,{"className":"meta-icon","aria-hidden":"true"}],["$","div",null,{"className":"author-info","children":["$undefined",["$","span",null,{"className":"author-name","children":"Manic Agency"}]]}]]}],["$","div",null,{"className":"meta-item date","children":[["$","$L36",null,{"className":"meta-icon","aria-hidden":"true"}],["$","time",null,{"dateTime":"2025-04-20T00:00:00.000Z","children":["Published ","April 20, 2025"]}]]}],["$","div",null,{"className":"meta-item reading-time","children":[["$","$L37",null,{"className":"meta-icon","aria-hidden":"true"}],["$","span",null,{"children":[35," min read"]}]]}],["$","div",null,{"className":"meta-item category","children":[["$","$L38",null,{"className":"meta-icon","aria-hidden":"true"}],["$","$L33",null,{"href":"/blog?category=thinkpieces","className":"post-category-link","children":"thinkpieces"}]]}]]}],["$","div",null,{"className":"post-tags","children":["$","div",null,{"className":"tags-list","children":[["$","$L33","featured",{"href":"/blog?tags=featured","className":"post-tag","children":["#","featured"]}],["$","$L33","llms",{"href":"/blog?tags=llms","className":"post-tag","children":["#","llms"]}],["$","$L33","vibe-coding",{"href":"/blog?tags=vibe-coding","className":"post-tag","children":["#","vibe-coding"]}]]}]}]]}],["$","div",null,{"className":"post-featured-image-container","children":["$","figure",null,{"className":"post-featured-image","children":[["$","$L39",null,{"src":"/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png","alt":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","width":1600,"height":900,"className":"featured-image","priority":true}],"$undefined"]}]}],["$","div",null,{"className":"post-content","children":["$","$L3a",null,{"children":"$3b"}]}],"$undefined",["$","footer",null,{"className":"post-footer","children":["$","$L3c",null,{"title":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","url":"/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding"}]}],["$","$L3d",null,{"projects":[{"slug":"hackbase","title":"HackBase.io â€” Founders Launchpad for Indie Makers","description":"Link building intelligence meets Product Hunt. Validate startup ideas with AI-powered debates, discover your founder archetype with HEXACO-60, and launch with confidence.","date":"2026-01-17","category":"ai","content":"$3e","longDescription":"$undefined","tags":["ai","saas","indie-hackers","startup","validation","link-building","product-hunt","anthropic","super-cloud-mcp","featured"],"modifiedDate":"$undefined","status":"ongoing","draft":false,"featured":true,"sortOrder":999,"image":"/assets/projects/hackbase/hackbase-logo.svg","images":["/assets/projects/hackbase/hackbase-logo.svg","/assets/projects/hackbase/og-home.svg","/assets/projects/hackbase/og-launchpad.svg"],"bgColor":"$undefined","textColor":"$undefined","link":"https://hackbase.io","github":"$undefined","license":"MIT","technologies":["Express.js","React","TypeScript","SQLite","Anthropic Claude","OpenAI","Stripe","Transformers.js","Puppeteer"],"languages":["TypeScript","JavaScript","SQL"],"stats":[{"label":"AI Debate Agents","value":"12 Crucible-Style"},{"label":"Assessment","value":"HEXACO-60 + Archetypes"},{"label":"Social Platforms","value":"8 Brand Checks"},{"label":"Part of","value":"Super Cloud MCP"}],"team":[{"name":"Manic Agency","role":"Core Development","link":"https://manic.agency","photo":"$undefined"}],"testimonials":[]},{"slug":"synthstack","title":"SynthStack â€” AI-Native SaaS Boilerplate","description":"Your Agency in a Box. Open-source, cross-platform SaaS boilerplate built with Vue Quasar. Ships for web, iOS, Android, desktop, and PWA from a single codebase with AI Copilot, Stripe billing, and Directus CMS.","date":"2026-01-14","category":"ai","content":"$3f","longDescription":"$undefined","tags":["ai","saas","boilerplate","vue","quasar","typescript","cross-platform","stripe","directus","supabase","oss","featured"],"modifiedDate":"$undefined","status":"ongoing","draft":false,"featured":true,"sortOrder":999,"image":"/assets/projects/synthstack/synthstack-logo-512.png","images":["/assets/projects/synthstack/synthstack-logo-512.png","/assets/projects/synthstack/synthstack-mark-512.png"],"bgColor":"$undefined","textColor":"$undefined","link":"https://synthstack.app","github":"https://github.com/manicinc/synthstack","license":"MIT","technologies":["Vue 3","Quasar","TypeScript","Fastify","PostgreSQL","Redis","Directus","Qdrant","Docker"],"languages":["TypeScript","Vue","SQL"],"stats":[{"label":"Tests","value":"920+"},{"label":"Platforms","value":"Web + iOS + Android + Desktop"},{"label":"AI Copilot","value":"GPT-4o + Claude"},{"label":"Billing Tiers","value":"4 + Lifetime"}],"team":[{"name":"Manic Agency","role":"Core Development","link":"https://manic.agency","photo":"$undefined"}],"testimonials":[]},{"slug":"domainhq","title":"DomainHQ.ai â€” AI-Powered Domain Sales Intelligence","description":"Make data-driven domain investment decisions with AI valuations, 100K+ historical sales, multi-source scraping, and real-time deal scoring. The definitive platform for domain investors.","date":"2026-01-06","category":"ai","content":"$40","longDescription":"$undefined","tags":["ai","domain-names","domain-investing","domain-valuation","sales-intelligence","deal-scoring","auction-tracking","domain-appraisal","saas","featured"],"modifiedDate":"$undefined","status":"completed","draft":false,"featured":true,"sortOrder":999,"image":"/assets/projects/domainhq/logo-full.png","images":["/assets/projects/domainhq/logo-full.png","/assets/projects/domainhq/og-image.png"],"bgColor":"$undefined","textColor":"$undefined","link":"https://domainhq.ai","github":"$undefined","license":"$undefined","technologies":[],"languages":[],"stats":[{"label":"Historical Sales","value":"100,000+"},{"label":"Data Sources","value":"15+ Platforms"},{"label":"AI Accuracy","value":"94%+"},{"label":"Daily Auctions","value":"5,000+"}],"team":[{"name":"Manic Agency","role":"Design & Development","link":"https://manic.agency","photo":"$undefined"}],"testimonials":[]}],"title":"// Related Projects //"}],["$","section",null,{"className":"post-comments","aria-labelledby":"comments-heading","children":[["$","h2",null,{"id":"comments-heading","className":"comments-title","children":"Join the Discussion"}],["$","$L41",null,{"postTitle":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨","postUrl":"/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding","postIdentifier":"blog-thinkpieces-logomaker-an-experiment-in-human-computer-interaction-vibe-coding","className":"mb-8"}],["$","div",null,{"className":"mt-8","children":[["$","div",null,{"className":"giscus-header","children":[["$","h3",null,{"className":"giscus-title","children":"Comments with GitHub"}],["$","div",null,{"className":"giscus-divider"}]]}],["$","$L42",null,{}]]}]]}],["$","section",null,{"className":"post-newsletter-signup mt-16","children":["$","$L43",null,{"variant":"blog","background":"accent"}]}]]}]}]," "]}]," "]}]]}]
1f:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1, maximum-scale=5, user-scalable=yes"}],["$","meta","1",{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#FBF6EF"}],["$","meta","2",{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#22182B"}],["$","meta","3",{"charSet":"utf-8"}],["$","title","4",{"children":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨ | Manic Agency - AI Agency Los Angeles"}],["$","meta","5",{"name":"description","content":"Vibe coding a logo creation tool, with insights on different LLM providers and interfaces."}],["$","meta","6",{"name":"author","content":"Manic Agency"}],["$","meta","7",{"name":"keywords","content":"featured,llms,vibe-coding"}],["$","meta","8",{"name":"creator","content":"Manic Inc"}],["$","meta","9",{"name":"publisher","content":"Manic Inc"}],["$","link","10",{"rel":"canonical","href":"https://manic.agency/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding"}],["$","meta","11",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","12",{"property":"og:title","content":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨ | Manic Agency - AI Agency Los Angeles"}],["$","meta","13",{"property":"og:description","content":"Vibe coding a logo creation tool, with insights on different LLM providers and interfaces."}],["$","meta","14",{"property":"og:image","content":"https://manic.agency//assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png"}],["$","meta","15",{"property":"og:image:alt","content":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨"}],["$","meta","16",{"property":"og:type","content":"article"}],["$","meta","17",{"property":"article:published_time","content":"2025-04-20T00:00:00.000Z"}],["$","meta","18",{"property":"article:author","content":"Manic Agency"}],["$","meta","19",{"property":"article:tag","content":"featured"}],["$","meta","20",{"property":"article:tag","content":"llms"}],["$","meta","21",{"property":"article:tag","content":"vibe-coding"}],["$","meta","22",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","23",{"name":"twitter:title","content":"Logomaker: An experiment in human-computer interaction and âœ¨ vibe coding âœ¨ | Manic Agency - AI Agency Los Angeles"}],["$","meta","24",{"name":"twitter:description","content":"Vibe coding a logo creation tool, with insights on different LLM providers and interfaces."}],["$","meta","25",{"name":"twitter:image","content":"https://manic.agency//assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png"}],["$","link","26",{"rel":"shortcut icon","href":"/favicon-16x16.png"}],["$","link","27",{"rel":"icon","href":"/favicon.ico"}],["$","link","28",{"rel":"apple-touch-icon","href":"/apple-touch-icon.png"}],["$","meta","29",{"name":"next-size-adjust"}]]
1:null
