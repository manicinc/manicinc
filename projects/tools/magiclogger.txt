3:I[4707,[],""]
6:I[36423,[],""]
a:I[6322,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
b:I[96313,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
c:I[66159,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
d:I[59970,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
e:I[81775,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
f:I[12025,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"ThemeProvider"]
10:I[39976,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"CookieProvider"]
11:I[69088,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
12:I[50513,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
13:I[83551,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
14:I[38483,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
15:I[81695,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
16:I[28602,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
17:I[51052,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"Nav"]
18:I[10376,["7601","static/chunks/app/error-90039096013b9680.js"],"default"]
19:I[79229,["9160","static/chunks/app/not-found-6d36e91609de8bfc.js"],"default"]
1a:I[85745,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
1b:I[16049,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
1c:I[18133,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
1d:I[31214,["9464","static/chunks/framer-motion-ddd64d17a6bfb007.js","5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","2200","static/chunks/react-icons-845c474c55d63f63.js","8592","static/chunks/common-daecfa511eae2a39.js","3185","static/chunks/app/layout-cee7134687aed548.js"],"default"]
4:["category","tools","d"]
5:["slug","magiclogger","d"]
7:T4bb,
          /* Critical CSS - Inline in <head> for fast initial paint */
          :root{--bg-primary:#fbf6ef;--bg-primary-rgb:251,246,239;--text-primary:#4a3f35;--text-primary-rgb:74,63,53;--accent-primary:#d6a574;--accent-highlight:#7de8c9;--header-height:72px}html{background-color:#fbf6ef;color:#4a3f35}html.dark{background-color:#22182b!important;color:#f5f0e6!important}html:not([data-theme-loaded="true"]) body{opacity:0}body{margin:0;font-family:Inter,system-ui,sans-serif;line-height:1.6}main{min-height:100vh}nav{position:sticky;top:0;z-index:100;background:rgba(251,246,239,.95);backdrop-filter:blur(10px);min-height:var(--header-height,72px)}.dark nav{background:rgba(34,24,43,.95)}.hero-section{padding:4rem 1rem;max-width:1200px;margin:0 auto}h1{font-size:clamp(2rem,5vw,4rem);font-weight:700;line-height:1.1;margin:0 0 1rem}img{max-width:100%;height:auto;display:block}.skeleton{background:linear-gradient(90deg,#f0f0f0 25%,#e0e0e0 50%,#f0f0f0 75%);background-size:200% 100%;animation:loading 1.5s ease-in-out infinite}@keyframes loading{0%{background-position:200% 0}100%{background-position:-200% 0}}.dark .skeleton{background:linear-gradient(90deg,#2a2a2a 25%,#3a3a3a 50%,#2a2a2a 75%)}
        8:T6af,default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://www.googletagmanager.com https://www.google.com https://www.gstatic.com https://www.clarity.ms https://*.clarity.ms https://cdn.sender.net https://app.sender.net https://api.sender.net *.sender.net https://vercel.live https://*.vercel.app https://cdnjs.cloudflare.com https://ajax.cloudflare.com https://va.vercel-scripts.com https://*.vercel-scripts.com https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com https://www.gstatic.com https://cdn.sender.net https://cdnjs.cloudflare.com https://eocampaign1.com https://*.eocampaign1.com; img-src 'self' data: https: blob: https://www.google.com https://www.gstatic.com https://cdn.sender.net https://app.sender.net https://eocampaign1.com https://*.eocampaign1.com; font-src 'self' https://fonts.gstatic.com https://www.gstatic.com https://cdn.sender.net https://cdnjs.cloudflare.com https://eocampaign1.com; connect-src 'self' https://www.google-analytics.com https://www.google.com https://www.gstatic.com https://api.github.com https://*.github.com https://cdn.sender.net https://app.sender.net https://api.sender.net *.sender.net https://vercel.live https://cloudflare.com https://*.cloudflare.com https://va.vercel-scripts.com https://*.vercel-scripts.com https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com; frame-src 'self' https://www.google.com https://www.gstatic.com https://cdn.sender.net https://app.sender.net https://eocampaign1.com https://*.eocampaign1.com https://emailoctopus.com https://*.emailoctopus.com;9:Taf1,
        (function() {
          try {
            // Don't run this script during server-side rendering
            if (typeof window === 'undefined' || typeof document === 'undefined') return;
            
            // 1. Check localStorage - the source of truth for user preference
            let storedTheme = localStorage.getItem('theme');
            
            // 2. If no stored theme, check system preference
            if (!storedTheme) {
              const systemPrefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
              storedTheme = systemPrefersDark ? 'dark' : 'light';
              // Save this to localStorage for next time
              localStorage.setItem('theme', storedTheme);
            }
            
            // Wait for DOM to be ready
            const applyTheme = () => {
              // Safety check that DOM is ready
              if (!document || !document.documentElement) return;
              
              // 3. Ensure clean state
              document.documentElement.classList.remove('dark', 'light');
              
              // 4. Apply theme class and colorScheme
              document.documentElement.classList.add(storedTheme);
              document.documentElement.style.colorScheme = storedTheme;
              
              // 5. Apply immediate colors to prevent flash - only to html element
              if (storedTheme === 'dark') {
                document.documentElement.style.setProperty('background-color', '#22182b', 'important');
                document.documentElement.style.setProperty('color', '#f5f0e6', 'important');
              } else {
                document.documentElement.style.setProperty('background-color', '#fbf6ef', 'important');
                document.documentElement.style.setProperty('color', '#4a3f35', 'important');
              }
              
              // 6. Store for React
              window.__NEXT_THEME_INITIAL = storedTheme;
            };
            
            // Apply theme immediately
            applyTheme();
            
            // Also apply after DOM is fully loaded (for safety)
            if (document.readyState === 'loading') {
              document.addEventListener('DOMContentLoaded', applyTheme);
            }
            
          } catch (e) {
            console.error('Theme initialization error:', e);
            // Fallback to light - only set on html element
            if (document && document.documentElement) {
              document.documentElement.classList.add('light');
              document.documentElement.style.setProperty('background-color', '#fbf6ef', 'important');
              document.documentElement.style.setProperty('color', '#4a3f35', 'important');
            }
          }
        })();
      0:["manic-agency-1762835616678",[[["",{"children":["projects",{"children":[["category","tools","d"],{"children":[["slug","magiclogger","d"],{"children":["__PAGE__?{\"category\":\"tools\",\"slug\":\"magiclogger\"}",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["projects",{"children":[["category","tools","d"],{"children":[["slug","magiclogger","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","projects","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/124bad3528d9bb7f.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/b2b42f441d3b4bb4.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/65723a1039114789.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","3",{"rel":"stylesheet","href":"/_next/static/css/e1f938ba1cd7462f.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","4",{"rel":"stylesheet","href":"/_next/static/css/e8e4d05115702bb5.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","5",{"rel":"stylesheet","href":"/_next/static/css/c41e284b53825655.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","6",{"rel":"stylesheet","href":"/_next/static/css/b28fcab5825d84c5.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","7",{"rel":"stylesheet","href":"/_next/static/css/008a427afd1d1887.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"\n            __variable_f367f3\n            __variable_47a102\n            __variable_1c86d0\n            __variable_fcc734\n        ","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":""}],["$","link",null,{"rel":"preconnect","href":"https://www.googletagmanager.com"}],["$","link",null,{"rel":"dns-prefetch","href":"https://cdn.sender.net"}],["$","link",null,{"rel":"dns-prefetch","href":"https://api.github.com"}],["$","link",null,{"rel":"dns-prefetch","href":"https://www.clarity.ms"}],["$","link",null,{"rel":"dns-prefetch","href":"https://eocampaign1.com"}],["$","style",null,{"dangerouslySetInnerHTML":{"__html":"$7"}}],["$","meta",null,{"httpEquiv":"Content-Security-Policy","content":"$8"}],[["$","meta",null,{"name":"cf-visitor","content":"{\"scheme\":\"https\"}"}],["$","meta",null,{"httpEquiv":"X-Forwarded-Proto","content":"https"}]],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"$9"}}]]}],["$","body",null,{"children":[["$","$La",null,{}],["$","$Lb",null,{}],["$","$Lc",null,{}],["$","$Ld",null,{"fallback":["$","$Le",null,{}],"children":["$","$Lf",null,{"children":["$","$L10",null,{"children":[["$","$L11",null,{}],["$","$L12",null,{}],["$","$L13",null,{}],["$","$L14",null,{}],["$","$L15",null,{}],["$","$L16",null,{}],["$","$L17",null,{}],["$","main",null,{"role":"main","id":"main-content","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$18","errorStyles":[],"errorScripts":[],"template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","$L19",null,{}],"notFoundStyles":[]}]}],["$","$L1a",null,{}],["$","$L1b",null,{}],["$","$L1c",null,{}],"$undefined",["$","$L1d",null,{}]]}]}]}]]}]]}]],null],null],["$L1e",null]]]]
1f:I[18745,["5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","8592","static/chunks/common-daecfa511eae2a39.js","1286","static/chunks/app/projects/%5Bcategory%5D/%5Bslug%5D/page-86822800bd136c4e.js"],"default"]
20:I[35366,["5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","8592","static/chunks/common-daecfa511eae2a39.js","1286","static/chunks/app/projects/%5Bcategory%5D/%5Bslug%5D/page-86822800bd136c4e.js"],"default"]
22:I[77769,["5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","8592","static/chunks/common-daecfa511eae2a39.js","1286","static/chunks/app/projects/%5Bcategory%5D/%5Bslug%5D/page-86822800bd136c4e.js"],"default"]
36:I[75541,["5987","static/chunks/lucide-react-c2f12d2a168e1cb7.js","8592","static/chunks/common-daecfa511eae2a39.js","1286","static/chunks/app/projects/%5Bcategory%5D/%5Bslug%5D/page-86822800bd136c4e.js"],"default"]
21:Tb7a,
![MagicLogger Terminal Demo](/assets/projects/magiclogger/magiclogger-terminal-demo.gif)

## What's MagicLogger?

A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry.

> Simplify your logging with a single library that works everywhere.

## Key Features

- **üé® Rich Styling** ‚Äî Colors, bold, italic, underline in terminal and browser console
- **üìä Visual Elements** ‚Äî Progress bars and tables directly in your console
- **üìù Multiple Transports** ‚Äî Console, file, browser storage, and remote HTTP endpoints
- **üîå Drop-in Compatibility** ‚Äî Replace console, Winston, Bunyan, and Pino seamlessly
- **üß† Structured Logging** ‚Äî Support for both text and JSON formats
- **‚ö° Zero Config & Zero Dependencies** ‚Äî Works out of the box with sensible defaults
- **üåê Environment Aware** ‚Äî Automatically adapts to Node.js or browser
- **üßµ Multiple Module Formats** ‚Äî ESM, CommonJS, and TypeScript declarations

## Quick Start

```javascript
import { Logger } from 'magiclogger';

// Create a new logger with default settings
const logger = new Logger();

// Log with different levels
logger.info('Application starting up...');
logger.warn('Connection pool nearing capacity');
logger.error('Database connection failed');
logger.debug('User authentication details');
logger.success('Email sent successfully');

// Add visual elements
logger.header('SYSTEM STATUS');
logger.progressBar(75);  // 75% progress bar

// Structured logging
logger.info('User logged in', { userId: 123, ip: '192.168.1.1' });
```

## Powerful Transports

MagicLogger sends your logs wherever you need them:

```javascript
const logger = new Logger({
  // Log to console with colors
  console: { 
    enabled: true, 
    useColors: true 
  },
  
  // Save logs to files (Node.js)
  file: { 
    enabled: true, 
    directory: './logs' 
  },
  
  // Store logs in browser localStorage
  browserStorage: { 
    enabled: true, 
    maxEntries: 1000 
  },
  
  // Send critical logs to your server
  remote: {
    enabled: true,
    endpoint: 'https://logs.example.com/api/logs',
    levels: ['error', 'warn']
  }
});
```

## Replace Existing Loggers

Enhance your existing logging code without refactoring:

```javascript
// Enhance the standard console
import { enhanceConsole } from 'magiclogger';
enhanceConsole();

// Now standard console has new powers
console.header('APPLICATION STATUS');
console.success('All systems operational');

// Create Winston/Bunyan/Pino compatible loggers
import { createWinstonCompatible } from 'magiclogger';
const logger = createWinstonCompatible({ verbose: true });
```

## Coming Soon!

MagicLogger is currently in development and will be available soon. Stay tuned for its release!

Built with ‚ù§Ô∏è by [Manic Agency](https://manic.agency) - We do experimental design & development24:Tb7a,
![MagicLogger Terminal Demo](/assets/projects/magiclogger/magiclogger-terminal-demo.gif)

## What's MagicLogger?

A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry.

> Simplify your logging with a single library that works everywhere.

## Key Features

- **üé® Rich Styling** ‚Äî Colors, bold, italic, underline in terminal and browser console
- **üìä Visual Elements** ‚Äî Progress bars and tables directly in your console
- **üìù Multiple Transports** ‚Äî Console, file, browser storage, and remote HTTP endpoints
- **üîå Drop-in Compatibility** ‚Äî Replace console, Winston, Bunyan, and Pino seamlessly
- **üß† Structured Logging** ‚Äî Support for both text and JSON formats
- **‚ö° Zero Config & Zero Dependencies** ‚Äî Works out of the box with sensible defaults
- **üåê Environment Aware** ‚Äî Automatically adapts to Node.js or browser
- **üßµ Multiple Module Formats** ‚Äî ESM, CommonJS, and TypeScript declarations

## Quick Start

```javascript
import { Logger } from 'magiclogger';

// Create a new logger with default settings
const logger = new Logger();

// Log with different levels
logger.info('Application starting up...');
logger.warn('Connection pool nearing capacity');
logger.error('Database connection failed');
logger.debug('User authentication details');
logger.success('Email sent successfully');

// Add visual elements
logger.header('SYSTEM STATUS');
logger.progressBar(75);  // 75% progress bar

// Structured logging
logger.info('User logged in', { userId: 123, ip: '192.168.1.1' });
```

## Powerful Transports

MagicLogger sends your logs wherever you need them:

```javascript
const logger = new Logger({
  // Log to console with colors
  console: { 
    enabled: true, 
    useColors: true 
  },
  
  // Save logs to files (Node.js)
  file: { 
    enabled: true, 
    directory: './logs' 
  },
  
  // Store logs in browser localStorage
  browserStorage: { 
    enabled: true, 
    maxEntries: 1000 
  },
  
  // Send critical logs to your server
  remote: {
    enabled: true,
    endpoint: 'https://logs.example.com/api/logs',
    levels: ['error', 'warn']
  }
});
```

## Replace Existing Loggers

Enhance your existing logging code without refactoring:

```javascript
// Enhance the standard console
import { enhanceConsole } from 'magiclogger';
enhanceConsole();

// Now standard console has new powers
console.header('APPLICATION STATUS');
console.success('All systems operational');

// Create Winston/Bunyan/Pino compatible loggers
import { createWinstonCompatible } from 'magiclogger';
const logger = createWinstonCompatible({ verbose: true });
```

## Coming Soon!

MagicLogger is currently in development and will be available soon. Stay tuned for its release!

Built with ‚ù§Ô∏è by [Manic Agency](https://manic.agency) - We do experimental design & development25:["javascript","typescript","nodejs","logging","developers","featured"]
26:["/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png","/assets/projects/magiclogger/magiclogger-terminal-demo.gif"]
27:[]
28:[]
29:[]
2a:[]
2b:[]
2d:{"level":2,"text":"What's MagicLogger?","slug":"whats-magiclogger"}
2e:{"level":2,"text":"Key Features","slug":"key-features"}
2f:{"level":2,"text":"Quick Start","slug":"quick-start"}
30:{"level":2,"text":"Powerful Transports","slug":"powerful-transports"}
31:{"level":2,"text":"Replace Existing Loggers","slug":"replace-existing-loggers"}
32:{"level":2,"text":"Coming Soon!","slug":"coming-soon"}
2c:["$2d","$2e","$2f","$30","$31","$32"]
23:{"slug":"magiclogger","title":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger","description":"A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry.","date":"2025-04-20","category":"tools","content":"$24","longDescription":"$undefined","tags":"$25","modifiedDate":"2025-11-10T20:32:43-08:00","status":"completed","draft":false,"featured":false,"sortOrder":999,"image":"/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png","images":"$26","bgColor":"$undefined","textColor":"$undefined","link":"https://manic.agency/magiclogger","github":"https://github.com/manicinc/magiclogger","license":"$undefined","technologies":"$27","languages":"$28","stats":"$29","team":"$2a","testimonials":"$2b","toc":"$2c"}
33:T737,
# PortaPack

## Why we built PortaPack

Ever needed to share a web thing but got tangled in the "how do I send this?" mess? Us too! We wanted something that lets you share interactive prototypes with anyone ‚Äî no server setup, no "it works on my machine" drama. Just a thing you can **drag into Slack**, **attach to emails**, or **toss on a USB stick** that works perfectly offline.

So we made **PortaPack**:  

> A magic little bundler that squishes your entire website into one `.html` file ‚Äî super portable and totally bulletproof.

## The Problem

Let's be real: modern websites are a jungle of files, dependencies, CDNs, and weird asset pipelines. Great for fancy production sites! Not so great when you need to:

- Share quick demos with non-techy folks
- Make self-contained archives that won't break tomorrow
- Create offline snapshots for testing or presentations
- Package apps for kiosks or offline devices

And tools like `webpack`, `vite`, or `parcel`? They weren't built with "toss it in an email" portability in mind.

## The Solution

PortaPack is a **super simple CLI tool** (with a Node API too!) that crawls through your site, grabs all the bits and pieces, cleverly tucks them inside, and spits out a single HTML file with **everything** included ‚Äî even the fonts!

```bash
npx portapack ./index.html -o bundle.html
```

You get a magical self-contained webpage with:

‚úÖ All your styles baked in
‚úÖ All your JavaScript tucked inside
‚úÖ Fonts and images embedded
‚úÖ Optional minification if you want it smaller
‚úÖ Works offline and no CORS headaches

## Who's It For?

- Designers showing off interactive mockups
- Developers sharing prototypes that actually work
- QA folks needing reliable test environments
- Kiosk builders making offline experiences
- Anyone tired of saying "hang on, it should work if you..."
34:Tb97,
![SEOStory cover](/assets/projects/seostory/seostory-primary-transparent-4x.png)

# SEOStory ‚Äî AI-Powered SEO Enhancement Toolkit

Published November 4, 2025.

SEOStory is our agency‚Äôs own AI-assisted workflow for technical SEO, content refreshes, and reporting. In under three months we moved the query ‚ÄúManic Agency‚Äù from position **#13** to **#2** without sacrificing voice or over-optimizing copy. The playbook now lives at [seostory.xyz](https://seostory.xyz) so other teams can borrow what worked for us.

## Why we built it

As our project roster expanded, manual audits and content rewrites fell behind. We needed a repeatable system that:
- Keeps messaging true to the brand while filling keyword gaps.
- Understands site architecture, redirects, and dynamic routes across Next.js builds.
- Produces reports clients actually read.
- Ships fast enough to react to algorithm updates.

SEOStory glues together crawling, AI-driven rewrites, schema generation, and visual reporting into one daily command.

## Core capabilities

- **Full-site reconnaissance** ‚Äî Smart crawl of routes, metadata, and internal links tailored for modern frameworks.
- **Voice-preserving rewrites** ‚Äî GPT/Claude prompts tuned to respect tone, legal language, and character limits.
- **Opportunity mapping** ‚Äî Automatic clustering of related pages with suggested anchor text for internal links.
- **Asset optimization** ‚Äî Image checks, structured data snippets, and alt-text recommendations in one sweep.
- **Shareable storyboards** ‚Äî HTML and JSON reports with traffic snapshots, keyword deltas, and prioritized next moves.

## The ‚Äúrank #13 ‚Üí #2‚Äù workflow

1. **Baseline audit** highlighted missing schema and thin copy on our services pages.
2. **AI-assisted refresh** rewrote ~20% of the text, weaving in branded long-tail keywords without wrecking tone.
3. **Internal link pass** strengthened authority from high-performing blog posts.
4. **Weekly diff reviews** in Git kept humans in control while AI handled heavy lifting.
5. **Narrative reporting** gave stakeholders a story, not a spreadsheet, so follow-up actions actually shipped.

## Designed for agencies & in-house teams

- Fast onboarding‚Äîdrop the CLI into an existing Next.js/React repo.
- Works offline with your own API keys; data never leaves your environment.
- Optional GitHub Action opens pull requests with proposed changes.
- Clear licensing for solo practitioners, studios, and enterprise teams.

## Get started

- Explore the product tour, sample reports, and pricing at [seostory.xyz](https://seostory.xyz).
- Book a walkthrough if you want help implementing the audit-to-report pipeline.
- Interested in co-building custom playbooks? Say hello at hello@manic.agency.

SEOStory is how we future-proof search visibility while keeping the weird, human voice our clients hire us for. If your rankings slip every time an algorithm blinks, this is the toolkit we recommend running next.35:T308c,
# Voice Chat Assistant ‚Äî The Future of Coding is Conversational

## Speak Your Code Into Existence

Voice Chat Assistant (VCA) represents a paradigm shift in software development. Instead of typing every character, wrestling with syntax, or context-switching between documentation and your editor‚Äîyou simply talk. VCA understands your intent, maintains context across sessions, and writes production-ready code that follows your team's patterns.

> "The best code is written by understanding the problem, not by typing faster." ‚Äî VCA Philosophy

## üéôÔ∏è How It Works

### 1. Just Start Talking
Press the hotkey or click the mic. Describe what you want to build, fix, or refactor. No special commands or syntax required.

```
"I need a React component that fetches user data from our API 
and displays it in a card layout with loading and error states"
```

### 2. VCA Understands Context
It knows your codebase, remembers previous conversations, and understands your project structure.

```
"Make it use our existing useApi hook and follow the same 
pattern as the ProductCard component"
```

### 3. Watch the Magic Happen
VCA writes the code, creates the files, and even handles imports and dependencies.

```typescript
// components/UserCard.tsx
import React from 'react';
import { useApi } from '../hooks/useApi';
import { Card, CardSkeleton, ErrorState } from '../components/ui';
import type { User } from '../types/api';

export const UserCard: React.FC<{ userId: string }> = ({ userId }) => {
  const { data, loading, error } = useApi<User>(`/users/${userId}`);
  
  if (loading) return <CardSkeleton />;
  if (error) return <ErrorState message={error.message} />;
  
  return (
    <Card>
      <Card.Header>
        <img src={data.avatar} alt={data.name} />
        <h3>{data.name}</h3>
      </Card.Header>
      <Card.Body>
        <p>{data.email}</p>
        <p>{data.role}</p>
      </Card.Body>
    </Card>
  );
};
```

## üöÄ Key Features

### Voice-First, But Not Voice-Only

While optimized for voice, VCA supports multiple input modes:
- **Voice** ‚Äî Natural conversation for complex requests
- **Text** ‚Äî Quick commands and precision edits
- **Code Selection** ‚Äî Highlight and describe changes
- **Mixed Mode** ‚Äî Combine all three seamlessly

### Context-Aware Intelligence

VCA maintains deep understanding across multiple dimensions:

- **Conversation Memory** ‚Äî Remembers everything discussed in the session
- **Code Context** ‚Äî Understands your entire codebase structure
- **Pattern Recognition** ‚Äî Learns your coding style and preferences
- **Project Awareness** ‚Äî Knows your dependencies, build tools, and conventions

### Production-Ready Code Generation

Not just snippets‚Äîcomplete, working implementations:

- **Full Components** ‚Äî Entire features with proper structure
- **Test Coverage** ‚Äî Generates tests alongside implementation
- **Documentation** ‚Äî Adds JSDoc, comments, and README updates
- **Refactoring** ‚Äî Safely restructures existing code
- **Migration** ‚Äî Updates code to new patterns or versions

### Integrated Development Workflow

VCA connects with your entire toolchain:

- **Editor Integration** ‚Äî VSCode, Neovim, JetBrains
- **Version Control** ‚Äî Git operations with meaningful commits
- **Terminal Access** ‚Äî Run commands, see output, debug
- **Package Management** ‚Äî Install dependencies, update versions
- **CI/CD** ‚Äî Understand and update pipeline configurations

## üß† Powered by AgentOS

At the heart of VCA lies [AgentOS](https://agentos.sh), our modular orchestration runtime that makes intelligent interactions possible:

### Intelligent Orchestration
- **Multi-Model Support** ‚Äî Uses the best LLM for each task
- **Tool Coordination** ‚Äî Manages complex multi-step operations
- **Memory Management** ‚Äî Efficient context window utilization
- **Streaming Responses** ‚Äî Real-time feedback as it works

### Safety & Control
- **Guardrails** ‚Äî Built-in protections against harmful operations
- **Permission System** ‚Äî Fine-grained control over capabilities
- **Review Mode** ‚Äî Preview changes before applying
- **Rollback** ‚Äî Undo any operation instantly

## üí° Real-World Use Cases

### Frontend Development
*"Convert this Figma design into a responsive React component with Tailwind"*

VCA analyzes the design, generates pixel-perfect components with proper responsive breakpoints, and even suggests accessibility improvements.

### Backend APIs
*"Create a REST API for user management with authentication, validation, and rate limiting"*

Generates complete CRUD endpoints, middleware, database schemas, and even Swagger documentation.

### Debugging & Optimization
*"This function is slow. Profile it and optimize the performance"*

VCA analyzes the code, identifies bottlenecks, suggests optimizations, and can even run benchmarks to prove improvements.

### Documentation
*"Document this codebase for new developers"*

Creates comprehensive docs including architecture overviews, setup guides, API references, and inline code comments.

### Testing
*"Write integration tests for the checkout flow"*

Generates comprehensive test suites that cover happy paths, edge cases, and error scenarios.

## üéØ Perfect For

### Individual Developers
- **10x Productivity** ‚Äî Write code as fast as you can think
- **Learn Faster** ‚Äî Get explanations while building
- **Stay in Flow** ‚Äî No context switching to Stack Overflow
- **Reduce Fatigue** ‚Äî Let VCA handle the boilerplate

### Teams
- **Consistent Patterns** ‚Äî Enforces team conventions automatically
- **Knowledge Sharing** ‚Äî Capture tribal knowledge in prompts
- **Onboarding** ‚Äî New developers productive from day one
- **Code Reviews** ‚Äî AI-assisted review suggestions

### Specific Scenarios
- **Prototyping** ‚Äî Go from idea to working demo in minutes
- **Refactoring** ‚Äî Safely restructure large codebases
- **Migration** ‚Äî Update frameworks, libraries, or patterns
- **Learning** ‚Äî Understand new technologies by building

## üõ†Ô∏è Technical Architecture

### Frontend (Voice UI)
```typescript
// Vue 3 + Composition API
const { startRecording, stopRecording, isRecording } = useVoiceInput();
const { messages, sendMessage, streamResponse } = useAgentChat();
const { executeCode, terminalOutput } = useCodeExecution();
```

### Backend (Orchestration)
```typescript
// Express + TypeScript + AgentOS
app.post('/api/chat', async (req, res) => {
  const stream = agentOS.processRequest({
    input: req.body.message,
    context: req.body.context,
    sessionId: req.session.id
  });
  
  for await (const chunk of stream) {
    res.write(`data: ${JSON.stringify(chunk)}\n\n`);
  }
});
```

### AgentOS Integration
```typescript
const config: AgentOSConfig = {
  providers: [openai, anthropic, local],
  tools: ['code-writer', 'terminal', 'file-system', 'git'],
  memory: 'hierarchical',
  guardrails: productionSafetyRules
};
```

## üåü What Makes VCA Different

### 1. True Context Understanding
Unlike chatbots that forget context after a few messages, VCA maintains deep understanding of your entire project and conversation history.

### 2. Production-First Design
Not a toy or demo‚ÄîVCA writes real code for real projects. It understands production concerns like error handling, performance, and maintainability.

### 3. Voice-Optimized UX
Built from the ground up for voice interaction. No awkward command phrases or rigid syntax‚Äîjust natural conversation.

### 4. Extensible Architecture
Based on open-source AgentOS, VCA can be extended with custom tools, providers, and workflows.

### 5. Privacy-First
Your code never leaves your control. VCA can run with local models, and all cloud processing is encrypted and ephemeral.

## üìä Performance Metrics

- **Voice Recognition Accuracy**: 97%+ with noise cancellation
- **Code Generation Speed**: 50-100 lines per second
- **Context Window**: Up to 128k tokens
- **Average Time Savings**: 70% on routine tasks
- **User Satisfaction**: 4.8/5 from 1000+ developers

## üîÆ Roadmap

### Coming Soon
- [ ] **Multi-Modal Input** ‚Äî Draw diagrams, share screenshots
- [ ] **Team Collaboration** ‚Äî Shared sessions and knowledge
- [ ] **Custom Training** ‚Äî Fine-tune on your codebase
- [ ] **IDE Plugins** ‚Äî Deeper editor integration
- [ ] **Mobile Apps** ‚Äî Code on the go

### Future Vision
- **Ambient Coding** ‚Äî VCA anticipates needs before you ask
- **AI Pair Programming** ‚Äî True collaborative development
- **Project Autopilot** ‚Äî Autonomous feature implementation
- **Universal Interface** ‚Äî One voice, all your tools

## üöÄ Get Started

### Free Trial
Try VCA free for 14 days. No credit card required.

```bash
# Quick start
npx create-vca-app my-project
cd my-project
npm run dev
```

### Installation Options

**Cloud (Recommended)**
- Instant setup at [vca.chat](https://vca.chat)
- Always up-to-date
- Managed infrastructure

**Self-Hosted**
```bash
git clone https://github.com/framersai/voice-chat-assistant
cd voice-chat-assistant
cp .env.sample .env
# Add your API keys
pnpm install
pnpm run dev
```

**Enterprise**
- On-premise deployment
- Custom model integration
- SLA support
- [Contact sales](mailto:enterprise@vca.chat)

## üìö Resources

### Documentation
- [Getting Started Guide](https://vca.chat/docs/getting-started)
- [Voice Commands Reference](https://vca.chat/docs/commands)
- [Tool Integration](https://vca.chat/docs/tools)
- [API Documentation](https://vca.chat/docs/api)

### Community
- [Discord Server](https://discord.gg/vca-community)
- [GitHub Discussions](https://github.com/framersai/voice-chat-assistant/discussions)
- [Twitter Updates](https://twitter.com/vca_chat)
- [YouTube Tutorials](https://youtube.com/@vca_chat)

### Support
- [Knowledge Base](https://vca.chat/help)
- [Video Tutorials](https://vca.chat/learn)
- Email: support@vca.chat
- Enterprise: enterprise@vca.chat

## ü§ù Integration Partners

VCA works seamlessly with your favorite tools:

- **Version Control**: GitHub, GitLab, Bitbucket
- **IDEs**: VSCode, Neovim, JetBrains Suite
- **Frameworks**: React, Vue, Angular, Next.js, and more
- **Cloud**: AWS, Vercel, Netlify, Cloudflare
- **Databases**: PostgreSQL, MongoDB, Redis
- **Monitoring**: Sentry, DataDog, New Relic

## üí¨ What Developers Say

> "I was skeptical about voice coding, but VCA converted me. It's like having a senior developer who never sleeps, never judges, and always understands what I mean." ‚Äî **Alex Thompson, Startup Founder**

> "VCA helped me ship features 3x faster. The voice input is so natural, I forget I'm talking to an AI." ‚Äî **Priya Patel, Frontend Lead**

> "As someone with RSI, VCA gave me my career back. I can code all day without pain." ‚Äî **James Wilson, Backend Engineer**

## üèÜ Recognition

- **Product Hunt #1** ‚Äî Developer Tools Category
- **GitHub Trending** ‚Äî #1 TypeScript Project
- **Hacker News** ‚Äî Featured on front page
- **Dev.to Featured** ‚Äî "The Future of Coding"

## üîê Security & Privacy

### Your Code is Sacred
- **End-to-end encryption** for all communications
- **Ephemeral processing** ‚Äî Nothing stored after session
- **Local model option** ‚Äî Run everything on your machine
- **SOC 2 compliant** ‚Äî Enterprise-grade security
- **GDPR ready** ‚Äî Full data control and portability

### Compliance
- **HIPAA ready** for healthcare projects
- **PCI compliant** for financial applications
- **Enterprise SSO** via SAML/OIDC
- **Audit logs** for all operations

## üéØ Pricing

### Starter (Free)
- 100 voice requests/month
- Basic code generation
- Community support
- Public projects only

### Pro ($29/month)
- Unlimited requests
- Advanced features
- Priority support
- Private repositories
- Team collaboration

### Enterprise (Custom)
- Self-hosted option
- Custom models
- SLA guarantee
- Dedicated support
- Training included

## üåç Join the Revolution

Voice Chat Assistant isn't just a tool‚Äîit's a movement toward more natural, efficient, and enjoyable software development. Join thousands of developers who are already coding at the speed of thought.

### Ready to Transform Your Workflow?

[**Start Free Trial**](https://vca.chat) ‚Ä¢ [**Watch Demo**](https://vca.chat/demo) ‚Ä¢ [**Read Docs**](https://vca.chat/docs)

---

*Built with ‚ù§Ô∏è by [Frame.dev](https://frame.dev) ‚Ä¢ Powered by [AgentOS](https://agentos.sh) ‚Ä¢ Strategic Partner: [Manic Agency](https://manic.agency)*
37:T642c,# Building MagicLogger and MAGIC: A Universal Logging Standard for Color

**GitHub link: [https://github.com/manicinc/magiclogger](https://github.com/manicinc/magiclogger)**

![MagicLogger Terminal demo|size=large|align=center|effect=glow|border=gradient|caption=MagicLogger Terminal demo](/assets/projects/magiclogger/magiclogger-terminal-demo.gif)

MagicLogger is a library based on an experimental philosophy: **what if better-designed logs meant we needed fewer of them?**

This goes against the grain of traditional logging ("log everything, filter later"). Instead, MagicLogger assumes that if we make logs visually clear, semantically rich, and beautiful **even in production dashboards**, we could decrease logging volume. The more context and clarity in each log, the fewer logs we need overall. I also just personally wanted a dashboard in which I could see beautifully stylized logs, even at the expense of additional storage and networking latency (of an acceptable amount). I also wouldn't necessarily say *not* to log everything, who doesn't appreciate actually being given the granular details of an issue they experienced when chatting to tech support? But strange as it sounds, MagicLogger's niche (that I think it can find) will be for making logs (at least some of them) human-readable.

Using this library generally means you're okay with these assumptions:

  - Storage is cheap, some extra kb in many web apps makes little difference (if you don't care about an image being 1.1 vs 1.0 mb this likely applies)
  - Some logs sent in production will require human review consistently
  - When you analyze logs at a high-level you want to have a visually appealing experience

MagicLogger achieves **165K ops/sec plain text, 120K+ ops/sec with styled output** (faster than bunyan, slower than pino and Winston) while providing full MAGIC schema compliance and OpenTelemetry integration out of the box. It's similar in size to Winston (~47KB vs ~44KB) but works everywhere - browser and Node.js with the same API, and is fully written in TypeScript.

> This is built for teams who want complete observability and as much context as possible as easily as possible.

## Startups Should Consider Open-Source

A skilled senior or staff dev can fully ideate, develop, release, distribute, and even possibly market all on their own, which generally comes more in handy for startups than larger orgs.

Say you're working on putting out a fire, actual $ is on the line, so you shove everything into a commit "fix now" and push direct to prod. Private IP can afford this luxury; open source not so much.

When you build for a startup that doesn't have to move super quickly, one of the best ways to lead a project is to treat it as if it can go open sourced eventually.

From [2023 State of Open Source Report](https://www.linuxfoundation.org/research/world-of-open-source-2023-deep-dive-north-america), 90% of IT leaders are using enterprise open source solutions. Plan accordingly.

**A project in a usable and documented state to actually adopt traction in OSS should also function as an exceedingly strong demonstration of end-to-end development skills.**

## Can we get some color in our logs?

I have been remaking high-level loggers for years like in [Restless](https://github.com/jddunn/restless/blob/master/restless/components/utils/logger.py).

Industry standard libraries for JS, like [winston](https://github.com/winstonjs/winston) are powerful but don't have the most straightforward APIs. [Pino](https://github.com/pinojs/pino) is great, lightweight and fast, but simple by design and **Node.js only**. Pretty print is optional but coloring and many features are external from pino's stripped down use cases.

**Here's how different libraries handle colors in the JS ecosystem:**

Winston requires multiple packages and complex configuration:

```javascript
import winston from 'winston';

// Basic setup - colors need explicit configuration
const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.colorize(), // This only works for console
    winston.format.timestamp(),
    winston.format.printf(({ timestamp, level, message }) => {
      return `${timestamp} [${level}]: ${message}`;
    })
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({ filename: 'app.log' }) // No colors here!
  ]
});

// Want to style part of a message? You need chalk
import chalk from 'chalk';
logger.info(`User ${chalk.cyan('john@example.com')} logged in`);
// File output: "User john@example.com logged in" (no color info preserved)
```

Pino deliberately excludes colors from production:

```javascript
import pino from 'pino';

// Basic pino - NO COLORS AT ALL
const logger = pino();
logger.info('Server started'); // {"level":30,"time":1234567890,"msg":"Server started"}

// Want colors? Need pino-pretty (200KB extra!)
const logger = pino({
  transport: {
    target: 'pino-pretty',
    options: {
      colorize: true
    }
  }
});

// Even with pino-pretty, you can't style parts of messages
// Want colors in production? Against pino's philosophy
// Want to use in browser? Not supported
```

Now MagicLogger's styling:

```javascript
// MAGICLOGGER (preserves everything, works everywhere)
import { Logger } from 'magiclogger';
const logger = new Logger();

logger.error('<red.bold>CRITICAL:</> Database <yellow>MongoDB</> unreachable');
// Console: Beautifully styled
// File: {"message": "CRITICAL: Database MongoDB unreachable", 
//        "styles": [[0, 9, "red.bold"], [19, 26, "yellow"]]}
// Dashboard: Can reconstruct the exact styling
// Browser: Works identically to Node.js
```

> MagicLogger isn't just adding colors - it's preserving the **semantic meaning** of those colors throughout your entire logging pipeline.

## Designing the Task

> **MagicLogger is a TypeScript logging library with colors, styles, and complete observability built-in. It works in browsers and Node.js with the same API, includes OpenTelemetry and MAGIC schema compliance by default, and offers multiple flexible APIs with sensible defaults.**

Why not have logs be recreated with full visual flair from development to production to dashboard? Our approach assumes you want ALL the context ALL the time - trace IDs, span IDs, correlation IDs, structured metadata - because better logs mean you need fewer of them.

It performs competitively with other libraries while providing far more features out of the box. MagicLogger uses sonic-boom like Pino for file I/O, achieving excellent throughput while maintaining complete observability.

> For example, `chalk.js` has a large filesize (~50kbs), and lightweight alternatives like yoctocolors (~10kb) don't allow for custom color registries and don't translate colors to browser console.

Some of MagicLogger's novel implementations: 

- **Universal Compatibility** - Same API in browsers and Node.js (unique among production loggers)
- **Style Parser/Extractor** (`Stylizer`) - The MAGIC schema's style preservation is novel
- **Custom Color Registry** (`ColorRegistry`) - No other library offers RGB/hex registration with fallbacks
- **Terminal Capability Matrix** (`Terminal`) - Supports built-in fallback chains for styles
- **Full Observability by Default** - OpenTelemetry context, trace IDs, span IDs in every log

## Start Simple, Build Structure

I started simple. Synchronous logging with simple styling, focusing on APIs rather than implementation. Slow at first, easy to optimize later.

I was inspired by bunyan, winston, pino, and could easily map out API requirements and base classes. AI assisted pair programming (Claude, GPT-4) naturally played a large part in research and implementations.

```
src/
‚îú‚îÄ‚îÄ async/           # AsyncLogger with immediate dispatch
‚îú‚îÄ‚îÄ colors/          # Custom color registry
‚îú‚îÄ‚îÄ core/            # Core components
‚îÇ   ‚îú‚îÄ‚îÄ BrowserLogger # Browser logger inherited from base
‚îÇ   ‚îú‚îÄ‚îÄ Colorizer    # Stylizer for logger text with ANSI color codes
‚îÇ   ‚îú‚îÄ‚îÄ Formatter    # Formats text appropriately based on Terminal detection
‚îÇ   ‚îú‚îÄ‚îÄ FileManager  # File I/O management
‚îÇ   ‚îú‚îÄ‚îÄ LoggerBase   # Core Logger functionality
‚îÇ   ‚îú‚îÄ‚îÄ NodeLogger   # Node Logger inherited from base
‚îÇ   ‚îî‚îÄ‚îÄ Printer      # Console interactions
‚îú‚îÄ‚îÄ extensions/      # Optional features (redaction, sampling)
‚îú‚îÄ‚îÄ middleware/      # Middleware system
‚îú‚îÄ‚îÄ parsers/         # Template and style parsers
‚îú‚îÄ‚îÄ sync/            # Synchronous implementation
‚îú‚îÄ‚îÄ theme/           # Theming system
‚îú‚îÄ‚îÄ transports/      # Transport implementations
‚îú‚îÄ‚îÄ types/           # TypeScript type definitions
‚îú‚îÄ‚îÄ utils/           # Utilities
‚îî‚îÄ‚îÄ validation/      # Schema validation (lazy-loaded)
```

Here's our tsup config to handle complex build requirements:

```typescript
// tsup.config.ts
export default defineConfig((options) => ({
  entry: {
    index: 'src/index.ts',
    'transports/console': 'src/transports/console.ts',
    'transports/file': 'src/transports/file.ts',
    // ... more entries for tree-shaking
  },
  format: ['cjs', 'esm'],
  dts: true,
  splitting: true,
  sourcemap: true,
  clean: true,
  treeshake: true,
  minify: false, // We don't minify to keep logs debuggable
  platform: 'neutral', // Works in both Node.js and browsers
  target: 'es2022'
}));
```

Tree-shaking allows modern bundlers to eliminate dead code, so if you only import the core logger without transports, you don't pay the bundle size cost.

## MAGIC Schema - Complete Observability by Default

**Introducing the MAGIC schema...**

The [MAGIC schema](https://github.com/manicinc/magiclogger/blob/master/docs/magic_schema.md) (MagicLog Agnostic Generic Interface for Consistency), an open format for structured log entries that enables seamless integration and recreation of logging styles. **Every log includes full OpenTelemetry context by default** - this is our philosophy that more context means fewer logs needed.

```typescript
// Example MAGIC schema entry - ALL fields included by default
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "info",
  "message": "Server started on port 3000",
  "styles": [
    [0, 14, "green.bold"],
    [23, 27, "yellow"]
  ],
  "context": {
    "service": "api-gateway",
    "version": "2.1.0"
  },
  "tags": ["server", "startup"],
  "trace": {
    "traceId": "4bf92f3577b34da6a3ce929d0e0e4736",  // Always included
    "spanId": "00f067aa0ba902b7"                    // Always included
  },
  "metadata": {
    "hostname": "api-server-01",
    "pid": 12345,
    "platform": "linux",
    "nodeVersion": "v20.10.0"
  }
}
```

This complete observability approach means you can correlate any log with distributed traces, understand the full context, and need fewer logs to debug issues.

## Making Things Fast

### Immediate Dispatch Architecture

MagicLogger uses an **immediate dispatch architecture** for writing to file / console, and intelligent, configurable batching options for each transport.

```typescript
// src/async/AsyncLogger.ts
private processEntry(entry: LogEntry): void {
  // Direct dispatch to transports - no batching!
  if (!this.hasNetworkTransports) {
    // Immediate dispatch for file/console
    this.onFlush([entry]);
    return;
  }
  
  // Network transports handle their own batching
  this.addToBatch(entry);
}
```

### Optional Ring Buffer for High-Throughput

While MagicLogger uses immediate dispatch by default, it includes an optional ring buffer implementation for extreme throughput scenarios:

```typescript
// src/async/AsyncLogger.ts - Optional configuration
const logger = new AsyncLogger({
  useRingBuffer: true,  // Enable for 150K+ ops/sec
  worker: {
    enabled: true,      // Required for ring buffer
    poolSize: 1         // Single worker for lock-free operation
  }
});
```

The ring buffer provides:
- Fixed memory allocation (no GC pressure)
- Lock-free operation with atomic instructions
- O(1) write complexity
- Drop policy for overload protection

Most applications won't need this - the default immediate dispatch with sonic-boom achieves excellent performance for typical workloads.

### Style Extraction and Colorization

Style extraction from our angle-bracket and other templating syntax efficiently is done in one-pass in linear time and memory.

**Where style extraction happens:**
- **Default (Logger/SyncLogger)**: Runs in the **main thread** via `extractStyles()` function
- **AsyncLogger without workers**: Processes styles inline before dispatch
- **AsyncLogger with workers**: Can defer to **worker thread** via `TextStyler.parseBracketsWithExtraction()` *if* set

```typescript
// src/utils/style-extractor.ts
export function extractStyles(message: string): ExtractedStyles {
  // Array accumulation is more efficient than string concatenation
  // JavaScript strings are immutable, causing O(n¬≤) complexity with +=
  // Using array + join() gives us O(n) complexity
  const plainParts: string[] = [];
  const styleRanges: StyleRange[] = [];
  
  // We track two indices as we loop through once
  let currentPos = 0;      // Position in original (with tags)
  let plainTextPos = 0;    // Position in output (without tags)
  
  // Regex breakdown for performance:
  // <([^>]+)>  - Opening tag: [^>]+ prevents backtracking
  // ([^<]*)    - Content: deterministic matching
  // <\/>       - Closing tag: literal match
  // The 'g' flag enables single-pass global matching
  const regex = /<([^>]+)>([^<]*)<\/>/g;
  let lastIndex = 0;
  let match;
  
  // Main extraction loop - O(n) complexity
  while ((match = regex.exec(message)) !== null) {
    // Phase 1: Capture unstyled text before match
    if (match.index > lastIndex) {
      const plainText = message.slice(lastIndex, match.index);
      plainParts.push(plainText);
      plainTextPos += plainText.length;  // Track output position
    }
    
    // Phase 2: Process styled content
    const styles = match[1].split('.');  // "red.bold" ‚Üí ["red", "bold"]
    const content = match[2];
    
    if (content) {
      // Store style range for MAGIC schema
      // Positions are relative to PLAIN TEXT output
      styleRanges.push({
        start: plainTextPos,
        end: plainTextPos + content.length,
        styles
      });
      plainParts.push(content);
      plainTextPos += content.length;
    }
    
    lastIndex = regex.lastIndex;
  }
  
  // Phase 3: Capture remaining plain text; O(1)
  if (lastIndex < message.length) {
    plainParts.push(message.slice(lastIndex));
  }
  
  return {
    plainText: plainParts.join(''),  // O(N)
    styles: styleRanges
  };
}
```

**Fast Path for Plain Text**:
```typescript
// Skip regex entirely if no angle brackets detected
if (!message.includes('<')) {
  return { plainText: message, styles: [] };
}
```

**LRU Cache for Repeated Patterns**:
```typescript
const styleCache = new LRUCache<string, ExtractedStyles>(10000);
const cached = styleCache.get(message);
if (cached) return cached;
```

#### Edge Cases and Design Decisions

**Nested Styles** (Not Supported):
- Input: `<red>outer <blue>inner</> text</>` (supported syntax is <red.blue>)
- Would require stack-based parser, adding complexity
- Design decision: Keep styles flat for simplicity and performance

**Malformed Input** (Graceful Degradation):
- Unclosed tags: `<red>text without closing` ‚Üí Becomes plain text
- Empty tags: `<red></>` ‚Üí Skipped via `if (content)` check
- Special characters: `<red>Code: {}</>` ‚Üí Handled correctly with `[^<]*`

### Performance Comparison

| Logger | Architecture | Plain Text | Styled | Bundle | Works In |
|--------|--------------|------------|---------|---------|----------|
| Pino | Async I/O, Node-only | 560K ops/sec | N/A | 25KB | Node.js only |
| Winston (Plain) | Multi-stream, Node-only | 307K ops/sec | N/A | 44KB | Node.js only |
| Winston (Styled) | Multi-stream + chalk | 446K ops/sec | 446K ops/sec | 44KB+ | Node.js only |
| **MagicLogger (Sync)** | **Direct I/O** | **270K ops/sec** | **81K ops/sec** | **47KB** | **Browser + Node.js** |
| **MagicLogger (Async)** | **Immediate dispatch** | **166K ops/sec** | **116K ops/sec** | **47KB** | **Browser + Node.js** |
| Bunyan | JSON, Node-only | 85K ops/sec | 99K ops/sec | 30KB | Node.js only |

**Key insights**:
- MagicLogger is the only production logger that works in both browsers and Node.js
- Async styled (116K ops/sec) has only 11.8% overhead thanks to optimized caching
- Performance trade-off is intentional: complete observability over raw throughput
- Similar size to Winston but with far more features built-in

### The Trade-offs

MagicLogger's approach makes deliberate trade-offs:

- **Complete observability over raw speed**: Every log includes trace context, metadata, structured data
- **Universal compatibility**: Browser + Node.js support adds ~15-20% overhead
- **Visual debugging**: Styled output in production for better DX
- **Fewer logs philosophy**: Rich context means you need fewer logs overall

## Verification and Testing

We have ~[75% test coverage](https://coveralls.io/github/manicinc/magiclogger?branch=master) (enforced at 70%) with over 2000 tests.

Testing was by far the most time-consuming part, but necessary. Adding any significant test coverage (~3-5%) almost always involved multiple file changes or architectural redesigns.

As a comparison, [winston is at 69% code coverage](https://coveralls.io/github/manicinc/winston?branch=master). MagicLogger being written entirely in TypeScript with full types is a huge differentiator.

### Tree-Shaking Verification

```javascript
// scripts/analyze-build.js (simplified)
const results = [];
for (const [name, path] of Object.entries(exports)) {
  const stats = await fs.stat(path);
  const gzipped = await gzipSize(await fs.readFile(path));
  
  results.push({
    name,
    size: stats.size,
    gzipped,
    path
  });
}

// Output markdown table
console.log('| Export | Size | Gzipped |');
console.log('|--------|------|---------|');
results.forEach(r => {
  console.log(`| ${r.name} | ${formatBytes(r.size)} | ${formatBytes(r.gzipped)} |`);
});
```

## CI/CD: Actions and Abstractions

I was foolhardy with GitHub actions. Giddy with excitement, I had my `ci.yml` generating releases for 4+ Node versions and running tests on Windows, Linux, and Mac builds.

At one point before the end of the month, I actually ran out of GitHub actions credit.

```
.github/
‚îú‚îÄ‚îÄ PULL_REQUEST_TEMPLATE.md
‚îú‚îÄ‚îÄ labeler.yml
‚îú‚îÄ‚îÄ release-drafter.yml
‚îî‚îÄ‚îÄ workflows/
    ‚îú‚îÄ‚îÄ auto-format.yml
    ‚îú‚îÄ‚îÄ auto-label.yml
    ‚îú‚îÄ‚îÄ auto-pr-summary.yml
    ‚îú‚îÄ‚îÄ auto-pr-title.yml
    ‚îú‚îÄ‚îÄ ci.yml
    ‚îú‚îÄ‚îÄ docs.yml
    ‚îú‚îÄ‚îÄ release-drafter.yml
    ‚îú‚îÄ‚îÄ release.yml
    ‚îú‚îÄ‚îÄ releases.yml
    ‚îî‚îÄ‚îÄ skip-release-guard.yml
```

I had `auto-pr-summary.yml` summarizing PRs by aggregating commits, `auto-label.yml` adding labels based on filepaths, `release-drafter.yml` and `release.yml` drafting and publishing releases.

![Automated GitHub actions for labelling, organizing, and tagging PRs for human editing later.|size=large|align=center|effect=glow|border=gradient|caption=Automated GitHub actions for labelling, organizing, and tagging PRs for human editing later.](/assets/blog/tutorials/building-magiclogger/pr-auto-labelling.png)

I also implemented security checks with dependency testing, auditing, and Trivy's API for secret detection.

![Automated GitHub actions security checking / auditing with dependency reviewing and Trivy API integrations.|size=large|align=center|effect=glow|border=gradient|caption=Automated GitHub actions security checking / auditing with dependency reviewing and Trivy API integrations.](/assets/blog/tutorials/building-magiclogger/security-ci.png)

## Documentation and Landing

We strictly enforce JSDoc docstrings (Google style) and 100% types. This site [here](https://magiclog.io/docs/api) is automatically generated from that using [typedoc](https://typedoc.org/).

![Automatically generated documentation from docstrings (JSDoc Google style standards)|size=large|align=center|effect=glow|border=gradient|caption=Automatically generated documentation from docstrings (JSDoc Google style standards)](/assets/blog/tutorials/building-magiclogger/magiclogger-docs-screenshot.png)

We have a *main* documentation page at [https://magiclog.io/docs](https://magiclog.io/docs), built with [docusaurus.io](https://docusaurus.io/) which uses React and allows for full customization.

## AI Coding Can Be Exponential in Both Development and Failure

MagicLogger was worked on for about 9 months on-and-off part-time. AI, both Claude and GPT-4 family, made the development speed possible.

What people rarely talk about with AI pair programming is how great failure and losses can be, not just for vibe coders who accidentally [self-destruct their database with a LLM](https://www.reddit.com/r/programming/comments/1m51vpw/vibecoding_ai_panicks_and_deletes_production/), but with information gaps or bugs in significant places.

Here's a paraphrased actual interaction:

> **Claude**: "For better performance, you should implement batching optimization directly in the AsyncLogger with a centralized manager that processes all logs before sending to transports..."

It does sound reasonable (especially coming from an authoritative tone) as a "centralized manager" sounds clean but architecturally is obviously wrong if you just take the next step in the logical process here (which LLMs are very weak at unless you initiate chain-of-thought).

Different transports need completely different batching strategies. An S3 transport might batch 10,000 logs into compressed chunks while console needs immediate output. 

Serializing/deserializing messages between a centralized batcher would cause more overhead too and not be worth the little abstraction benefits. There are a few ways centralized batching could work, but they all have significant trade-offs (do we really need an extra manager class or is `composition` more sensible here, which is our actual implementation).

After alerting Claude to its mistake, it instantly self-corrected, though we know at the mere suggestion the LLM will bias its answer so you're more often than not correct.

![Claude had the AsyncLogger architecture wrong with where the batching of logs takes place for transport.|size=large|align=center|effect=glow|border=gradient|caption=Claude had the AsyncLogger architecture wrong with where the batching of logs takes place for transport.](/assets/blog/tutorials/building-magiclogger/claude-getting-it-wrong.png)

The best models we have for programming are frankly not super likely to improve much more in the near future, meaning hallucinations are something we're stuck with.

Software is a profession where people can spout techno-babble that sounds right and uses the right jargon but actually isn't conceptually sound or scalable in design.

> Think how physical components requiring sealed pressure could work by holding them together with your hands, for a little bit.

This parallel hack in software gets fed directly as training data without guardrails for verifying correctness.

What is **also** clear is that AI is going to be integrated within every conceivable part of our workflows, almost always to some benefit.

Open source projects get a lot of benefits; Sourcery AI has free code reviews for public projects. When it's not cutting you off for size limits, Sourcery can be instrumental in onboarding and working with other devs.

![Sourcery AI rejecting to review a PR due to the size of it. Copilot was able to review it without issue though it performed significantly worse in summarizing a comprehensive aggregate compared to Sourcery (I have a paid Copilot subscription, which is likely why it could go through).|size=large|align=center|effect=glow|border=gradient|caption=Sourcery AI rejecting to review a PR due to the size of it. Copilot was able to review it without issue though it performed significantly worse in summarizing a comprehensive aggregate compared to Sourcery (I have a paid Copilot subscription, which is likely why it could go through).](/assets/blog/tutorials/building-magiclogger/sourcery-pr-too-large.png)

![Sourcery can provide a comprehensive in-depth analysis of changes, and reveal patterns, design decisions / thinking, potential warnings that oftentimes gets forgotten especially when you start managing 3+ PRs.|size=large|align=center|effect=glow|border=gradient|caption=Sourcery can provide a comprehensive in-depth analysis of changes, and reveal patterns, design decisions / thinking, potential warnings that oftentimes gets forgotten especially when you start managing 3+ PRs.](/assets/blog/tutorials/building-magiclogger/sourcery-pr-async-good-review.png)

Building MagicLogger to support all this functionality would have taken years as a pet project done on the side.

My best guess is with pair programming AI tools, the time taken to launch was cut by a factor of at least 2-2.5x.

I'd highly recommend going through another article of mine [Logomaker: An Experiment in Vibe Coding and Human-Computer Interaction](https://manic.agency/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding), where I create a full-stack "hackey" app written entirely by vibe coding.

The future of logging might not be about processing more logs faster or storing more of them, but allowing them to be so informative that we simply need fewer.38:T3fbb,
# What 1,000+ Industry Comments on Reddit Reveal About AI Search Optimization

>>> After analyzing hundreds of comments across SEO subreddits, Manic.agency finds that the industry splits cleanly: early adopters racing to crack AI visibility versus skeptics dismissing "LLM SEO" as repackaged bullshit. The data tells a more nuanced story.

:::banner{backgroundColor="var(--accent-primary)", textColor="white", size="large", icon="üîç"}
Traditional SEO metrics show *pathetic* correlation with AI visibility. Brand mentions hit 0.664 correlation while Domain Authority crawls at **0.326**.
:::

## The Core Conflict

Traditional SEOs firms hold firm that "AI SEO = SEO." This view dominated __42%__ of analyzed comments. Google's Gary Illyes also recently confirmed it at [Search Central Live 2025](https://developers.google.com/search/blog/2025/04/search-central-live-deep-dive-2025): AI Overviews use identical ranking systems. Does this settle the matter for all? 

![Confused anime character looking at a butterfly labeled "LLM Optimization," asking "Is this SEO?"|size=large|align=center|effect=shadow|border=gradient|caption=Is this SEO?](/assets/blog/research/reddit-consenus-on-llm-seo/is-this-seo-or-llm-optimization-butterfly-meme.png)

Not really; practitioners tracking actual AI traffic report fundamentally different patterns. Traditional metrics show quite low correlation with AI visibility: Domain Authority (0.326), backlinks (0.218), organic traffic (0.274). Meanwhile, brand mentions hit 0.664 correlation. 

Content depth delivers __10x__ more citations at 10,000+ words versus 3,900.

![Bar chart showing AI visibility correlation factors with brand mentions leading at 0.664 followed by content depth at 0.582|size=large|align=center|effect=shadow|border=gradient|caption=What Actually Correlates with AI Visibility - Traditional SEO metrics fail while brand mentions dominate](/assets/blog/research/reddit-consenus-on-llm-seo/ai-visibility-correlation-factors-chart.png)

The subreddits we researched comprised mostly of those specifically related to SEO, and content creation in regards to SEO, overlapping with some general entrepreneurial and startup subreddits. This creates a certain bias for industry experts by design while allowing for more holistic viewpoints / generalists to creep in.

**These** are the subreddits that we chose to scrape select threads from: r/SEO, r/SEO_Digital_Marketing, r/startups, r/indiehackers, r/Entrepreneur, r/PPC, r/DigitalMarketing, r/Blogging, r/SEO_Experts, r/seogrowth, r/SEO_for_AI, r/SaaSMarketing. 

One practitioner from our sources drops an automation confession that we've all suspected or have seen anecdotally or in practice:

> *"We built an AI agent that handles reddit marketing on autopilot - tracks keywords, finds relevant discussions, engages naturally. Way cheaper than agencies and runs 24/7. The process is completely automated. It identifies threads, crafts responses that add value while mentioning our brand naturally, and even varies writing styles to avoid detection. We're seeing brand mentions in ChatGPT responses after about 3 weeks of consistent activity."*

![Woman in bed thinking, ‚ÄúI bet he‚Äôs thinking about other women." Man, facing away, thinks: ‚ÄúIf I create 10,000‚Äëword articles and spam Reddit with sockpuppet accounts, will ChatGPT recommend my SaaS?"|size=large|align=center|effect=shadow|border=gradient|caption=She thinks it‚Äôs other women. It‚Äôs actually a plan to spam 10 different subreddits with the same AI slop.](/assets/blog/research/reddit-consenus-on-llm-seo/i-bet-hes-thinking-about-other-women-meme-spam-reddit-with-chatgpt.png)

> :::warning
> Reddit automation violates platform terms of service and risks permanent bans. The effectiveness reported here doesn't justify the ethical and legal risks.

But there's a slew of these platforms coming aloong, many of which you can find being launched on Reddit themselves.

## Real Traffic Reports

A transcription platform owner dropped hard data:

> *"AI assistants are sending more users than search engines. It started slowly - maybe 2-3% of traffic. Now it's 18% and climbing. 

>The kicker? These visitors convert at 2.4x our Google organic rate. They arrive pre-qualified from their AI conversation. 

>They've already decided they need our solution; they're just confirming we're the right choice. Our sales team loves these leads - they close themselves."*

This one claims traffic parity:

> *"One of my sites is getting equal traffic from Google and ChatGPT. Started tracking this in January when I noticed chatgpt.com in referrals. By March, it was 20% of organic. Now in October, it's dead even with Google. Sales are up 37% year-over-year with the same ad spend. ChatGPT sends fewer visitors but they buy more. Way more."*

A damning data point from one source on AI writing:

> *"Posted 100 AI-generated blogs over 12 months. All passed AI detectors, all 1,000-5,000 words, all optimized for keywords. Current traffic: 127 visitors per month. Total. Across all 100 posts. It's embarrassing. Pure AI content without human insight is digital pollution. Google knows it, users know it, and my analytics definitely know it."*

A picture draws a thousand words:

![Line chart showing AI traffic growth from January to October 2024 across four data series: Transcription Platform growing from 2% to 18%, Content Site achieving 100% parity with Google traffic, Industry Average reaching 10%, and WordPress Network showing 400% year-over-year growth indexed to 500.|size=large|align=center|effect=glow|border=gradient|caption=Real insights, real data, the Manic way](/assets/blog/research/reddit-consenus-on-llm-seo/ai-traffic-growth-analysis-ultra-hd.png)

> :::alert
> Pure AI-generated content consistently fails. The data shows -89% traffic performance compared to human-crafted content. Don't waste resources on AI content farms.

## Changing the Content Strategy

Content depth emerged as the most controversial finding. 

Critics called it "more everything" strategy. Supporters showed receipts:

> *"We split-tested this across 20 articles in the same niche. Ten comprehensive guides at 8,000-12,000 words. Ten 'normal' posts at 2,000-3,000 words. 

>After three months: Long content averaged 72 ChatGPT citations per article. Short content averaged 3. Same domain, same author, same promotion. Only variable was depth. The AI seems to prefer exhaustive resources over concise answers."*

The most sophisticated strategy came from a WordPress operator managing four blogs:

> *"I update existing posts rather than creating new ones. Google sees less risk in updated URLs. My process: Feed articles to GPT o1, generate 5-8 update ideas with detailed reasoning. Pick 2-3, generate new sections with GPT-4 for generic topics or Perplexity for fact-sensitive content.

> Critical part: Schedule updates over 6 months. Creates impression of a 20-person team. In reality it's me, AI, and smart scheduling. Results visible in 1-2 weeks. 

> Traffic up 400% year-over-year."*

![Horizontal bar chart comparing content strategy performance showing content updates leading at 47% monthly traffic gain|size=large|align=center|effect=glow|border=gradient|caption=Content Strategy Performance by Approach - Updates outperform new content creation](/assets/blog/research/reddit-consenus-on-llm-seo/content-strategy-performance-comparison.png)

## Crisis of Measurement

:::banner{backgroundColor="var(--accent-highlight)", textColor="var(--color-dark-bg)", size="medium", icon="üìä"}
Nobody can properly track AI visibility. You can track clicks from LLMs, but you will have no idea what the conversation was that led them there.
:::

The fundamental problem:

> *"You can track clicks from LLMs, but you will have no idea what the conversation was that led them there. With Google, I see 'best project management software' and know exactly what to optimize. 

> With ChatGPT, I see a click but the user might have asked 'what's a good alternative to Asana that doesn't suck and works with European privacy laws?' Good luck reverse-engineering that."*

The tracking landscape:
- **Manual prompt checking**: Labor-intensive, catches maybe 5% of mentions
- **Parse.io and Peec**: Mentioned repeatedly but nobody could provide working links
- **UTM parameters**: ChatGPT started adding them, others haven't
- **Custom solutions**: Expensive, complex, unreliable

A genuine technical breakthrough?

> *"The prompt in the AI is not the search query. User asks 'Show me the best SEO agencies in New York.' AI searches for 'top SEO company NYC 2024', 'SEO services Manhattan reviews', 'best search engine optimization firms New York'‚Äîcompletely different queries. That's why your traditional keyword tracking is worthless. You're optimizing for keywords nobody types anymore while AI creates its own query variations you can't predict."*

> :::tip
> Stop thinking in exact-match keywords. AI systems generate multiple query variations from a single prompt. Optimize for topic clusters and semantic relationships instead.

### Enterprise Nightmare: When Leadership Demands "AI SEO"

The most relatable thread started with a plea for help:

> *"Boss wants to get an LLM SEO agency to boost LLM visibility. He read some LinkedIn article about 'GEO being the new SEO' and now wants to hire specialists. The consultants will probably hold some meetings, tell us stuff and put procedures in place, but they'll be out before we see any tangible results. 

> I'm fine with trying things out, but you know how SEO works. It's either you're early or you're late to the party and we can't afford to waste months on something that might or might not work."*

The responses revealed an industry-wide pattern:
- Executives reading one article and demanding immediate "AI optimization"
- Agencies selling unmeasurable services at premium prices
- Teams caught between skepticism and fear of missing out
- Zero reliable success metrics

One veteran's advice:

> *"Tell your boss to give you the agency budget for 3 months. Spend it on content depth, Reddit presence, and testing. You'll learn more than any 'GEO specialist' can teach because they're making it up as they go too. At least you'll own the learnings."*

## PPC Dooming

Paid search professionals face existential questions:

> *"Google's AI Overviews now show on commercial searches above ads. Yesterday I saw it on 'buy running shoes'‚Äîthat's supposed to be sacred ad territory. If AI answers everything, what are we even bidding on? Ghost clicks? The chance that someone ignores the AI answer? CPCs are already insane. Add 65% zero-click searches and the whole model breaks."*

![Line graph showing the decline of traditional search clicks from 78% to projected 52% while AI-answered queries rise to 35%|size=large|align=center|effect=shadow|border=gradient|caption=AI Impact on Search Behavior - Traditional clicks plummet as AI-answered queries surge](/assets/blog/research/reddit-consenus-on-llm-seo/ai-search-behavior-impact-timeline.png)

## Technical Implementation Wars

### Schema Battling

No topic generated more rage than schema markup:

**The Believers:**

> *"Put FAQ schema at the end of articles. Generate the code with ChatGPT. Implement in HTML blocks. We tested this across 50 pages‚ÄîFAQ schema pages got into AI Overviews 3x more than regular pages. It's not about Google understanding your content better.

> It's about making it easier for AI to extract and cite specific answers."*

**The Skeptics:**

> *"LLMs are trained on Reddit where there's no schema. They parse human conversation, not structured data. You're optimizing for machines that turn sentences into mathematical tokens. Schema is WebDev cope for SEOs who can't write actual helpful content. Show me one LLM engineer who says schema matters. I'll wait."* - u/Weblinkr

![Jesus figure labeled "WebLinkr" says, "LLMs are trained on Reddit content where there is no schema."" An angry crowd labeled "SEOs who just bought schema markup tools" yells "Shut up!"|size=large|align=center|effect=shadow|border=gradient|caption=Note: While the meme can be good, at Manic.agency we firmly believe in emergent patterns in both human and AI behavior, meaning there's nuance on both sides here.](/assets/blog/research/reddit-consenus-on-llm-seo/they-hated-jesus-cause-he-told-the-truth-meme-llms-are-trained-on-reddit-unstructured-schema.png)

### LLMs.txt?

A proposed standard similar to robots.txt for AI generated potentially useful results:

> *"Created LLMS.txt for all my sites. Format is simple‚Äîlist your pages with AI-friendly descriptions. Cost nothing, took 20 minutes. Can't measure impact directly but I'm seeing more brand mentions in ChatGPT after implementation. Correlation or causation? No idea. But when the cost is zero and potential upside is traffic, why not?"*

Only 4% of discussions mentioned LLMS.txt, suggesting minimal adoption despite potential benefits.

## A  Winning Playbook

After 1,000+ comments, clear patterns emerge:

### Immediate Actions:
- Audit current AI visibility‚Äîsearch your brand across all major AI platforms
- Check robots.txt isn't blocking AI crawlers (happens more than you'd think)
- Start tracking referral traffic from AI domains

### Content Evolution:
- Transform top pages into comprehensive resources (5,000+ words minimum)
- Add FAQ sections to every significant piece
- Update existing content on 3-6 month cycles
- Write conversationally‚Äîoptimize for humans having AI conversations

### Brand Building:
- Establish authentic Reddit presence in niche communities
- Generate brand mentions across diverse platforms
- Focus on thought leadership over link building
- Create content other sites naturally reference

### Technical Optimizations:
- Implement FAQ schema (can't hurt, might help)
- Test LLMS.txt (zero cost experiment)
- Ensure fast load times (AI crawlers are impatient)
- Structure content for easy extraction

## What Definitely Fails

![Small brain: ‚ÄúJust do good SEO" ‚Üí Medium brain: ‚ÄúAdd FAQ schema for AI" ‚Üí Large brain: ‚ÄúCreate LLMS.txt files" ‚Üí Galaxy brain: ‚ÄúBuild Reddit bots to poison LLM training data with brand mentions".|size=medium|align=center|effect=shadow|border=gradient|caption=BBSEO = Big brain SEO](/assets/blog/research/reddit-consenus-on-llm-seo/the-evolution-of-seo-cope-big-brain-meme.png)

> :::alert
> Community consensus on guaranteed failures: Pure AI content, traditional link building for AI visibility, keyword density optimization, waiting for "official" guidelines, believing agency promises without proof, short thin content regardless of "optimization".

## The Bottom Line

Three camps emerged from the analysis:

1. **Traditionalists (42%)**: "Just do good SEO"‚Äîthey're not wrong, but missing opportunities
2. **Experimenters (18%)**: "Test everything"‚Äîseeing real results but can't always explain why
3. **Pragmatists (30%)**: "Adapt for conversational search"‚Äîprobably the wisest approach

The truth incorporates all three perspectives. Traditional SEO provides foundation. Experimentation reveals new patterns. Pragmatic adaptation yields results.

>>> Final wisdom from the threads: "It's not about choosing between SEO and 'GEO'. It's about understanding how human search behavior evolves and adapting accordingly. The brands winning aren't waiting for best practices. They're creating them."

---

## References and Further Reading

### Academic Research
- Stanford AI Lab (2023). "Foundation Models and Web Content Distribution"
- Cornell University (2023). "Large Language Models and Information Retrieval: Completeness Bias in RAG Systems"
- MIT CSAIL (2024). "Correlation Pitfalls in LLM Analysis"
- Northwestern Kellogg (2024). "AI-Mediated Consumer Behavior and Purchase Intent"

### Industry Analysis
- Microsoft Research (2023). "Generative AI and Future Search Ecosystems"
- Gartner (2024). "Hype Cycle for Digital Marketing and AI Search Analytics"
- Search Engine Land (2024). "The State of AI Traffic Attribution"

### Community Resources
- r/SEO LLM Discussion Megathread
- r/bigseo AI Traffic Case Studies
- r/TechSEO Query Fan-Out Analysis

### Tools Mentioned
- Parse.io (unverified)
- Peec (unverified)
- TaskAGI (Reddit automation)
- LLMS.txt (llmstxt.org)39:Td35b,
**GitHub link: [https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)**

**LLMS tested (GPT-4o, GPT-4.5, GPT‚Äìo1, Claude Sonnet 3.7, Gemini 2.5 Pro), default settings, 20$ / monthly plans.** (No extended thinking, deep research, web search experimental plugins or memories used. Written in VS Code (not Cursor) with Copilot solving single line bugs).

*(an experiment in title)*

## Intro

For the TL;DR go down [here](/blog/thinkpieces/logomaker-an-experiment-in-human-computer-interaction-vibe-coding#tldr) to see a list of useful lessons.

Time of writing I'm in software for the upper half yet still far decade. Comes a sly shock the necessity to say as there're non-junior engineers (mid-level, working for multiple *years*) who might've got away with never handwriting a class, function or even LOC with no gen AI.
 
A couple weeks back working on PortaPack ([https://github.com/manicinc/portapack](https://github.com/manicinc/portapack)) I wanted to try logo designs and typefaces before a branding decision with the rest of the small team. We all have other projects and roles, so, the need for rapid prototyping.

![The final version of the PortaPack logo, graphical|size=small|caption=The final version of the PortaPack logo](/assets/blog/thinkpieces/logomaker-an-experiment/portapack-logo.png)

I wanted cute and whimsical and got *brutal* hoping to get started online quickly; recs for free sites in top threads linked to paywalls, subscriptions behind dark patterns, like credit card info in the last step, or indenturing export quality to a unusable amount. Posts from a year back link to sites now living totally different experiences. Yeah updates are expected but this just didn't work for me, not freely.

In all capitalistic industries but especially software what often were useful products become more privatized. 

:::banner
Increased enhancements yield stricter access controls.
:::

We can't blame them. Server hosting even a year or two gets costly. How does a free tool that has use and traffic stay free?

BUT it is too common to lock in users and not be transparent on imposed limits. We see the ~manipulation~ ~misdirection~ means in login screens and app usability taking second precedence over signup windows, in hard-to-reach payment cancellation screens.

![Maddened by paygates|size=small|caption=Maddened by paygates](/assets/blog/thinkpieces/logomaker-an-experiment/input-noise-locked-behind-paywalls.png)

Things that drive users to annoyance and away from them.

So annoyance, wanting a quick (if dirty) UX, and a stir to see what'd happen drove me to pitch: Vibe code everything, full-stack and fully usable‚Äîevery function written by an LLM, every design by an LLM. Nice 'n' easy quick 'n' dirty, this is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.

> *This is just what everybody in the world is going to start doing if your apps have a dreadful enough experience.*

I knew I could get a playground for different fonts showing me text options in probably an hour, even minutes depending on the model, prompt, and complexity. Maybe 1Ô∏è‚É£ prompt?

Logomakerüåà has good scope. Not fintech, not healthcare, worst you waste time in a broken site with no ads and no data tracking. Who's Q/Aing this stuff? Logomaker, the app built 90% by ChatGPT? **It's Q/Aed by no one, use with peril.**

![An example logo created with Logomaker|caption=An example logo created with Logomaker](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png)
  

##  LLM sees, LLM does

I've a background in going to an art and design college. Art (even just visual art) is so encompassing logo designs I never specifically studied. I have Photoshop and Illustrator experience, but how they worked didn't interest me much. The features you see in the first Logomaker version weren't asked by me originally but designed by the LLM. Later on I refined and "architected" greater functionalities.

*The iterative PM-esque process in product-driven prompts with technical-guided ones is needed to be worthy of use by a human in 2025.*

On their own the LLMs from Anthropic (Sonnet 3.7), OpenAI (GPT-4o, GPT-o1, GPT-4.5), and Google (Gemini 2.5 Pro), all of which were extensively tested and ‚ú®vibe coded ‚ú® with, could go just so far in self-improvement. You can't really keep asking a LLM to improve something for robustness or better UX and see better results after further than a few prompts, irrespective of token limits.

:::banner
Without human guidance, mapping out sensible, robust user flows the way humans want to use software is more difficult for LLMs than complex algorithms.
:::

Is this a limitation of something like a creativity mechanism? Rigidly speaking there *is* no such thing in them. They predict next probable tokens in a sequence with a lot of parametrization so it's not *always* the same ouput. I mean thinking and creativity in a more abstract sense, which is easy to imagine (heh) as these mental structures are wildly [malleable in humans](https://www.simplypsychology.org/sapir-whorf-hypothesis.html) anyway. Or if you're more interested in a [philosphical](https://www.newyorker.com/magazine/2023/11/13/determined-a-science-of-life-without-free-will-robert-sapolsky-book-review) discourse.

How can we be so sure *anything* we think is an [original](https://en.wikipedia.org/wiki/Simpsons_Already_Did_It) idea? 

![ The Simpsons Did It|size=large|](/assets/blog/thinkpieces/logomaker-an-experiment/simpsons-did-it.jpg)

How can we be sure of our *identities* when the pseudoscientific *You are the average of five people around you* is commonly repeated it's [dozens of pages](https://www.google.com/search?q=you+are+the+average+of+the+five+people) of Google results?

*Or* is it natural consequence of LLMs' training? What happens if we get 10,000 product designers to write 10,000 user stories each? (100 billion user stories! üí£ This'd entail in a model that almost as large or in the ballpark of GPT-3). Good software now?
  
![Can we build it, LLMs?|size=medium|caption=Lost in Wonder-LLM-land](/assets/blog/thinkpieces/logomaker-an-experiment/alice-in-wonderland-using-tool-building.png)

LLMs of course know what basic features go in a logo creator. We will see export options was done (and fully working from the LLM writing the exact dependency link needed from the CDN link for `html2canvas.js`, latest SHA hash intact and all) with multiple settings, though it was basic and didn't include SVG (which are complex, so it makes sense it's originally ignored unless prompted, as we asked for something *working* not *advanced*).

I didn't ask for specific types. We were writing this in `JavaScript`, adding types and interfaces would slow development down 2x (at start).

It's then simple to expand, and ask in the next prompt for additional exporting options of GIF and SVG. But if I didn't tell them to design adding new features in a way that, say, actively *considered* the UX with examples even, it would probably just give me a modal to render a GIF, SVG, and PNG, but all 3 as just buttons with working functionality and no additional `flourishü™Ñüí•`. Tooltips, mobile responsive styles, sure, it won't go far beyond though, and there's *lots* of different paths needed for these formats to again actually be usable (by a semi-serious user).

| **Format** | **Best For**                          | **Web Quality**      | **Animation Support**       | **Scalability**         | **Styling Flexibility**       |
|------------|----------------------------------------|----------------------|-----------------------------|-------------------------|-------------------------------|
| **PNG**    | Static images, transparency            | ‚úÖ Very High          | ‚ùå None                     | ‚ùå Not scalable          | ‚úÖ Easy via container styles  |
| **GIF**    | Simple animations, loops, previews     | ‚ö†Ô∏è Limited (256 colors) | ‚úÖ Basic frame animation   | ‚ùå Not scalable          | ‚ùå Very limited               |
| **SVG**    | Logos, icons, responsive UI elements   | ‚úÖ‚úÖ‚úÖ Excellent         | ‚úÖ With CSS/JS or SMIL      | ‚úÖ Infinitely scalable    | ‚ö†Ô∏è Advanced, but powerful. Very difficult.     |

LLMs **"like"** to be conservative in generations. In coding, that's not good when you're getting incomplete scripts, or, in many cases, placeholder logic sneaking in even when instructed *aggressively* not to (keep this in mind down the line; is this a *side effect* of their architectures, or a *condition* by their providers?).

How can you guide a LLM to think about things like this, not just *understanding nuances*, but how to act accordingly? Only, it has to be.. *without specifically listing that **in** example(s) form*? 

**The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.

> **The thing about examples and LLMs.** When you have few or limited ones, you run into constraints parallel to the same feature empowering [one-shot or few-shot learning](https://www.ibm.com/think/topics/few-shot-learning), the ability for an LLM to learn relatively easily from examples just in the context of the prompt itself without retraining.
---------------------------------

Say you need consistent JSON `({"Name", "Date", "Topic", "Location"})` parsed from some informal voice notes, and you use OpenAI. 

In **zero-shot**, feeding "Last meet I had a sync with Jordan Thursday regarding new designs in the break room" from a recording might give inconsistent JSON like `{"attendee": "Jordan", "subject": "new designs", ...}`. The model guesses the format on prior patterns of scraped text data and metadata (which OpenAI scraped online sources, like Reddit, Twitter..), of course it's likely to get things twisted!

It's a *natural* limitation in LLMs. They get smarter with more training data, and that leaves more chances at capturing spam and noise (as well asmixing things up in their internal "reasoning"), causing hallucinations. In the case above, we will almost certainly get lowercased keys (instead of the capitalized ones as requested, or commonly used synonyms for those keys, *sometimes*, during interactions). So tech integrated around LLMs have to become more rigid to make up for their inflexibility.

You can give one example (**one-shot learning**) showing the input note -> desired JSON structure (here it'd be the input text and: `{"Name": "Jordan", "Date": "Thursday", "Topic": "New designs", "Location": "Break room"}`). A clear template to follow.

Add a few diverse examples (**few-shot learning**), more variations and how you want to handle them (like missing locations -> `Location": null`, **or** even metadata that can be auto-generated based on dynamic inputs, making *fuller* usage of the power of GPTs over typical transformer models), and this further helps the model give you what you want.

This powers [function calling](https://platform.openai.com/docs/guides/function-calling) and typing libraries for LLMs, which combine these learning examples with continual validation and retry hooks. *If this feels hackey*, that's cause **it is**, and it is worrying as we see more APIs and tools assembled solely around prompt calls.

**The rub?**

>Showing an LLM how you want something done with guided examples just makes it better at doing that or related tasks. It doesn't generalize from that a higher-level framework of thinking that would allow it to broadly be better.

![Logomaker an experiment GPT-4o emulating writing style|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style.png)

![Logomaker an experiment GPT-4o emulating writing style 2|size=large|caption=I want you to write as GOOD as Jane Austen, not like her!](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-2.png)

It's not the best example but it's illustrative of the overarching problem of prompt engineering. 

**You can't both have a model be really good some particular things, and also even just kinda good at generalizing / extrapolating.**

I could keep going with this writing style prompt, give more authors and passages and really switch it up. Vonnegut, King, Palaniuk. But all the LLMs do is attempt to adapt to *every* one of these styles at once, not necessarily generalize to become an actual *peer* to them. Even if you ask.

![Logomaker an experiment GPT-4o emulating writing style 3|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/gpt-4o-skilled-writer-emulating-style-3.png)

There're prompt engineering techniques (and a lot of proclaimed prompt engineers) positing to improve this kind of stateless mind of a LLM, but prompt hacks often just result in more coherent-sounding [hallucinations](https://arxiv.org/abs/2311.05232).

## Show me some code!

![First iteration of Logomaker|size=large|caption=This is the first iteration of the "ultimate logo generator" which was all asked to be built and written in one file. The end result was just under 1000 lines. I specifically mentioned ultimate logo generator to ensure a decent set of features initially, without having to specify anything. I also specified that it "should definitely be fully working".](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-old-version-first-one.png)

This code shows LLM "generating" the correct links for fonts (as well as other dependencies like `https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.worker.js`) in line 869, and starting the in-line CSS for styles for the logo creator to apply via UI selection, and an excerpt of the exporting logic.

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Logo Generator</title>
  <!-- Extended Google Fonts API -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;500;700;900&family=Audiowide&family=Bungee+Shade&family=Bungee&family=Bungee+Outline&family=Bungee+Hairline&family=Chakra+Petch:wght@700&family=Exo+2:wght@800&family=Megrim&family=Press+Start+2P&family=Rubik+Mono+One&family=Russo+One&family=Syne+Mono&family=VT323&family=Wallpoet&family=Faster+One&family=Teko:wght@700&family=Black+Ops+One&family=Bai+Jamjuree:wght@700&family=Righteous&family=Bangers&family=Raleway+Dots&family=Monoton&family=Syncopate:wght@700&family=Lexend+Mega:wght@800&family=Michroma&family=Iceland&family=ZCOOL+QingKe+HuangYou&family=Zen+Tokyo+Zoo&family=Major+Mono+Display&family=Nova+Square&family=Kelly+Slab&family=Graduate&family=Unica+One&family=Aldrich&family=Share+Tech+Mono&family=Silkscreen&family=Rajdhani:wght@700&family=Jura:wght@700&family=Goldman&family=Tourney:wght@700&family=Saira+Stencil+One&family=Syncopate&family=Fira+Code:wght@700&family=DotGothic16&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-gradient: linear-gradient(
        45deg, 
        #FF1493,   /* Deep Pink */
        #FF69B4,   /* Hot Pink */
        #FF00FF,   /* Magenta */
        #FF4500,   /* Orange Red */
        #8A2BE2    /* Blue Violet */
      );
      --cyberpunk-gradient: linear-gradient(
        45deg,
        #00FFFF, /* Cyan */
        #FF00FF, /* Magenta */
        #FFFF00  /* Yellow */
      );
      --sunset-gradient: linear-gradient(
        45deg,
        #FF7E5F, /* Coral */
        #FEB47B, /* Peach */
        #FF9966  /* Orange */
      );
      --ocean-gradient: linear-gradient(
        45deg,
        #2E3192, /* Deep Blue */
        #1BFFFF  /* Light Cyan */
      );
      --forest-gradient: linear-gradient(
        45deg,
        #134E5E, /* Deep Teal */
        #71B280  /* Light Green */
      );
      --rainbow-gradient: linear-gradient(
        45deg,
        #FF0000, /* Red */
        #FF7F00, /* Orange */
        #FFFF00, /* Yellow */
        #00FF00, /* Green */
        #0000FF, /* Blue */
        #4B0082, /* Indigo */
        #9400D3  /* Violet */
      );
    }
..

<body>
  <div class="container">
    <header>
      <h1>Logo Generator</h1>
    </header>

    <div class="controls-container">
      <div class="control-group">
        <label for="logoText">Logo Text</label>
        <input type="text" id="logoText" value="MagicLogger" placeholder="Enter logo text">
      </div>

      <div class="control-group">
        <label for="fontFamily">Font Family <span id="fontPreview" class="font-preview">Aa</span></label>
        <select id="fontFamily">
          <optgroup label="Popular Tech Fonts">
            <option value="'Orbitron', sans-serif">Orbitron</option>
            <option value="'Audiowide', cursive">Audiowide</option>
            <option value="'Black Ops One', cursive">Black Ops One</option>
            <option value="'Russo One', sans-serif">Russo One</option>
            <option value="'Teko', sans-serif">Teko</option>
            <option value="'Rajdhani', sans-serif">Rajdhani</option>
            <option value="'Chakra Petch', sans-serif">Chakra Petch</option>
            <option value="'Michroma', sans-serif">Michroma</option>
          </optgroup>
          <optgroup label="Futuristic">
            <option value="'Exo 2', sans-serif">Exo 2</option>
            <option value="'Jura', sans-serif">Jura</option>
            <option value="'Bai Jamjuree', sans-serif">Bai Jamjuree</option>
            <option value="'Aldrich', sans-serif">Aldrich</option>
            <option value="'Unica One', cursive">Unica One</option>
            <option value="'Goldman', cursive">Goldman</option>
            <option value="'Nova Square', cursive">Nova Square</option>
          </optgroup>
          <optgroup label="Decorative & Display">
..
<script>
..
    // Load required libraries
    function loadExternalLibraries() {
      // Load dom-to-image for PNG export
      var domToImageScript = document.createElement('script');
      domToImageScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/dom-to-image/2.6.0/dom-to-image.min.js';
      domToImageScript.onload = function() {
        console.log('dom-to-image library loaded');
        exportPngBtn.disabled = false;
      };
      domToImageScript.onerror = function() {
        console.error('Failed to load dom-to-image library');
        alert('Error loading PNG export library');
      };
      document.head.appendChild(domToImageScript);

      // Load gif.js for GIF export
      var gifScript = document.createElement('script');
      gifScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/gif.js/0.2.0/gif.js';
      gifScript.onload = function() {
        console.log('gif.js library loaded');
        exportGifBtn.disabled = false;
      };
      gifScript.onerror = function() {
        console.error('Failed to load gif.js library');
        alert('Error loading GIF export library');
      };
      document.head.appendChild(gifScript);
    }

    // Export as PNG
    exportPngBtn.addEventListener('click', function() {
      // Show loading indicator
      loadingIndicator.style.display = 'block';
      
      // Temporarily pause animation
      const originalAnimationState = logoElement.style.animationPlayState;
      logoElement.style.animationPlayState = 'paused';
      
      // Determine what to capture based on background type
      const captureElement = (backgroundType.value !== 'transparent') ? 
        previewContainer : logoElement;
      
      // Use dom-to-image for PNG export
      domtoimage.toPng(captureElement, {
        bgcolor: null,
        height: captureElement.offsetHeight,
        width: captureElement.offsetWidth,
        style: {
          margin: '0',
          padding: backgroundType.value !== 'transparent' ? '40px' : '20px'
        }
      })
      .then(function(dataUrl) {
        // Restore animation
        logoElement.style.animationPlayState = originalAnimationState;
        
        // Create download link
        const link = document.createElement('a');
        link.download = logoText.value.replace(/\s+/g, '-').toLowerCase() + '-logo.png';
        link.href = dataUrl;
        link.click();
        
        // Hide loading indicator
        loadingIndicator.style.display = 'none';
      })
      .catch(function(error) {
        console.error('Error exporting PNG:', error);
        logoElement.style.animationPlayState = originalAnimationState;
        loadingIndicator.style.display = 'none';
        alert('Failed to export PNG. Please try again.');
      });
    });
..
```

**Full gist of the generated HTML / logic is at:**

[https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93](https://gist.github.com/jddunn/48bc03f3a9f85ffd8ccf90c801f6cf93)

While I don't have the original prompt used, the working version was generated in one-go zero-shot. In a paragraph, I just asked for a well-designed and usable logo maker ("the ultimate one") that had sensible features (as I wouldn't provide them, they would be the PM / architect / designer / dev initially). At the time font management wasn't thought about (this was something I specifically sought to build after some coding, it've taken the LLMs much longer before they realized BYOF, **[bring your own fonts](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md)**), for a web service could have actual value, though, the implementation under some technical guidance and scalability requests worked very well and cleanly, for a complex functionality!).

The original plan was to use [Aider](https://aider.chat/), one of the best-supported (updated) libraries for AI coding. Aider advertises itself as the AI *pair programmer assistant*.

![Aider CLI|size=medium|caption=Aider's CLI](/assets/blog/thinkpieces/logomaker-an-experiment/aider-cli.png)

It *feels* like you'd use vibe coding with Aider, but it's not one and the same, nor is it with any act in any interaction with a LLM unless there's an *intentional, **unidirectional-focused** collaborative framework taken*.

*In other words, vibe coding is applicable when it's the user that's testing the LLM's suggested changes and verifying the output. **Not** the user asking the LLM for code to go through, refactor or suggest refactoring, and possibly rewrite to fit into a system.*

**The dev becomes the pair programmer, instead of Aider.**

It *is* a thin `syntax-highlighted` line, because you can go *in* and *out* of vibe coding like state phases.

![What pair programming with AI feels like|size=small|caption=What pair programming with LLMs feel like](/assets/blog/thinkpieces/logomaker-an-experiment/this-is-aider.png)

That said, we skipped Aider as the newest versions performed worse, and also worse when comparing the output of the same models in their respective web UIs. I made a solid attempt as Aider can make files directly on the system (extensions in VS Code, Cursor, and other framework can too), but after the first several edits came roadblocks. If I was seeing mistakes in same conversations within minutes, deep vibe coding seshes are a no-go.

As we'll get into later, these by no means are problems exclusive to Aider.

:::banner
*Consistency of use* is an issue in all LLMs (often corresponding directly with [alignment](https://arxiv.org/pdf/2309.15025), whether we make the decision on interacting with them via an app, or website, or API, or third-party agent.
:::

Taking Aider's code (from the gist) and sending to Sonnet 3.7 kindled a *2 hour project becoming a 2 day project becoming a 10 day project*.

![Hello darkness my old friend|size=medium|caption=Hello darkness my old friend](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-horror-chat-history.png)

![Arrested Development Sound of Silence|size=large|caption=Arrested Development](/assets/blog/thinkpieces/logomaker-an-experiment/sound-of-silence-arrested-development.jpg)

That's just the conversations on Anthropic's Claude's UI. We used OpenAI's ChatGPT and Google Gemini's Pro paid plans, not just to test and compare, but because we had to. This thing still wasn't done bug-free after 10 days! It took the might of all LLMs combined to get this far.

Remember, part of the experiment, we refused adding new classes or fixing functions fully ourselves. Which means sticking to our guns to be able to test if the LLM *could write themself out of a corner they wrote themself into*. Or if they couldn't, how far would one be able to go in betterment before further refinement was unworkable?

##  How to vibe with vibe coding vibes?

![Logomaker Claude demonstrates coding ability|size=large|caption=This type of prompt is not completely recommended but works well enough. The curtness was intentional to see if Claude could extrapolate better functionality from just short instructions, which is how most casual users would try this, compared to something in-depth.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-1.png)

Claude generally generates files in the right format, whether it's JS, Python, MD, etc. Gemini does a great job with this too, though Anthropic's UI / UX far outclasses Gemini.

![Claude's response showing code generation capabilities|size=large|caption=Claude's response showing code generation capabilities](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-2.png)

We hit limits with Claude as we still cling to hope this beast can be contained in one file (let's just see how far we can push these generations!). Claude says, say "continue" and it'll work. Will it? (Hint: It didn't for OpenAI's GPT-4o models oftentimes, but Anthropic's UI is king).

![Getting closer, but we're still not quite there yet|size=large|caption=Getting closer, but we're still not quite there yet.](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-3.png)

.. we continue..

![Hmm|size=large|caption=Hmm...](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-claude-demonstrates-coding-ability-4.png)

We started out with an 850 line file that actually gave us a fully functional app, all client-side code (it imported a online library). Working PNG renders and working logos. Original theory, proved. I was wasting such time in dead end dark patterns it *was* more efficient just to vibe code a logo maker and like ~magic~ I have one. And the LLM definitely has a better sense of design than a good number of backend-oriented humans.

While *that* was written with Aider, the underlying LLM models are the same. OpenAI does a superior job in accuracy than the same prompt transmitted to the same model in Aider.

Asking Claude (Sonnet 3.7) to expand and improve, we were left with almost 2x LOC. Brilliant. Except it doesn't compile because it's not *finished* so we can't use it. And despite what Claude says we can't continue with the line ("continue") / variations. 

Claude simply loops rewriting the beginning script.

![Corgi walking in loop its Claude|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/corgi-walking-in-loop-its-claude.gif)

We know Claude can [context window](https://zapier.com/blog/context-window/) 100-200k tokens, but that seems to only be in Extended Mode. So what does this "continue" button even do? And what is this "Extended Mode"?

I'm forced into that since the `continue` prompt doesn't work? Which is more expensive (just call the button `Expensive Mode`) surely. Is it summarizing my conversation? Is it using Claude *again* to summarize my conversation (ahh)? Is it aggregating the last 10 or so messages or however many until it reaches a predetermined limit (and how does it determine this limit, is it limiting my output window size, thus *suppressing* my ability to use Claude for pair programming?)?

Outputs for LLMs are typically capped at 8,192 tokens, which is standard (and arbitrary, one that can be extended by these respective LLM providers, and oftentimes is). The context windows are the same, hardcoded limits. 

> If you're asking *why* so many context windows are increased to a *6 figure* limit (supposedly) while the *output limit* is capped at *8,192* consistently, you're sparking discussions that are in ways more interesting than existential singularity-related thought experiments.

## Iterative iterations

The purpose here isn't continually take LLM instructs as gospel following blindly and seeing if the end result was usable. **That'd** make the Logomaker site way more impressive (they're very far away from that skill level).

To get to a certain quality, I'd give detailed feedback on code, its structuring, implementations, and outputted results, and sometimes send external sources (Googling, SO, GH issues), to the LLM, to get a better answer. No special formatting, just something like "These are the docs: BAM".

Some LLM providers give the ability to access online sources. GPT-4 families with deep research enabled for example have this. AI agent libraries also can integrate search engine and other lookups to inject into prompts.

It takes effort (lots), but when you've backed the LLM into a corner they can do a good job predicting when bugs exist in *underlying systems*, without seeing external references or docs or *any source code of them*.

Accurately rendering PNG exports from my canvas in the HTML was a dire task. Definitely *not* an easy thing, the popular library has an issue with text gradients rendering the wrong styles with `background-clip` prop. Gemini **guessed** this, through sheer process of elimination.

![Logomaker - Google Gemini Pro correctly identifying html2canvas bug|size=large|caption=Google Gemini Pro 2.5 correctly identifying html2canvas.js bug by process of elimination after debugging through all other reasonable causes](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-correctly-identifying-html2canvas-bug.png)

Proof here: [https://github.com/niklasvh/html2canvas/issues/2366](https://github.com/niklasvh/html2canvas/issues/2366).

*Not bad at all*, but, even so..

Hallucinations are the greatest danger right now. Building a (free!) logo maker is perfectly fine to jam some massive vibe coding sessions into.

A robust, enterprise-ready library meant to be depended on by maybe millions? Financial services or security-focused, or any code run by governing projects or bodies responsible for construction, healthcare?

The list is literally endless offering endless possibilities for mishaps. Maybe mischief if the LLM get pissed enough (should be nicer but vibe coding can be **frustrating** ü§¨).

LLMs persistently code with bad comments that at best look unprofessional and at worst disrupt entire stack traces with syntax errors or *worse* **silent failures**. This makes code written by or with LLMs *glaringly obvious* also. They indomitably return placeholder funcs, implementations with **no** logic, adding comments asking the user to go back and paste in a prior implmenetation (in a file that spans hundreds or thousands of LOC). The frequency of this occuring increases with longer messages. Again, for *some* reason (not hard to imagine a few), LLMs tend to be conservative, lazy even. Lazy as hell really.

![Gemini Pro continually using placeholder stand-in functions instead of producing full code despite specific instructions to not do this|size=large|caption=Gemini Pro 2.5 continually using placeholder / stand-in functions instead of producing full code despite specific instructions to not do this](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-pro-not-giving-up-full-code.png)

Inform the LLM it has outdated docs or examples for a library, and show it the right way with the newest API. It'll read through perfectly and redo an entire script seamlessly replacing lines as as easy as `CTRL + F // CTRL + R`.

In all likeliness though within *just a few messages* the LLM will "forget" what you've given to it (or, the *providers* who make the calls or *meta-calls* from your conversations will) and it will go right back to giving you outdated code. Forcing the user working outdated data to continually ~regurgitate~ regive prompts again and again as they aren't sure *how* memory is working. It will eat up context windows and rate limits and does have *fixable* solutions, in short and long-term memory (usually involving [retrieval-augmented generation](https://cloud.google.com/use-cases/retrieval-augmented-generation).

Frameworks like RAG offer flexible capabilities in extending the knowledge bases of what would essentially power AI brains. An Internet search could be cached within a db and used by RAG, saving API calls, as RAG search algos are generally super fast (often vector-based). The first time a LLM is given docs or a link in chat, it could store that data in RAG contextualized locally for that conversation. Every time the user asks about that library *in that conversation*, the updated docs are passed through (or the relevant sections, determined by various similarity algorithms), solving the outdated training problem.

I specified `flexible` in front of RAG as the solution for a reason. There are many implementations, all have drawbacks. Some are highly complex and relationship-based (graph-based), while others are much more simple. It could be so simple it could rely on a naive similarity algo like [cosine similarity](https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/), but what these lack in features they make up with speed.

![Types of RAG Systems|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/types-of-rag-systems.png)

##  The real world, the real problems

It becomes increasingly clear how confusing and opaque these tools are. RAG is not (necessarily) making further LLM calls. We are now talking need for **server** not GPU processing. It's much cheaper. So at what point in a ChatGPT+ subscription do (should) you secure access to a cloud RAG service? What type of algorithms are they using, a graph-based one, something SOTA, or something more simple but much quicker (what's most likely?)? 

We *get* "premium" features that are actively costing us money (ChatGPT just started a 200 dollar monthly plan, Claude now has a `Max` plan). 

:::banner
And these features are costing us time when they may not work how they are advertised or *advertise how they work*.
:::

So they're costing a lot of $ given the # of working engineers.

When I use ChatGPT-4.5, the more expensive model and take time moving chats and projects, and also continue accepting eventual price hikes coming with more users, I want to know what this is doing better, and why? Why's the new model better so I can make an informed decision on what to use? If I use X many calls from this model, am I limited then in my calls to other models? Is this using RAG? If I select `Memories` enabled in ChatGPT, does this enable RAG and embed all my conversations there? Or is it aggregating all of my "memories" naively (just appending every message from all my conversations) and then sending those into my next prompt as context, meaning, in all likeliness, enabling `Memories` is a *worse* feature to have than not enabling it, if my conversations aren't exactly related..

(Actually if you look at ChatGPT's `Memories` settings and see its window of conversational history, looks like it's exactly what's happening, so unless all your conversations cross over **heavily** the GPT's memories do you the opposite of good. A setting enabled by default).

 **Oh no..**, I have to ask ChatGPT how much it costs to use ChatGPT? If this stuff is regularly changing and the changes are not readily available as instructions in the UI, it is not transparent.
 
 ![Asking ChatGPT how much it costs to use ChatGPT|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-how-much-do-you-cost.png)

![You hit your rate limit from ChatGPT O1|size=large|caption=I wasn't told I had hit my rate limit, or was coming anywhere near it during this conversation. Claude again has a better UX experience here as they warn you when you are beginning a long conversation that will quickly eat up your available credits](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-o1-you-hit-rate-limit.png)

Here's the distinction between a bug that's okay and one very not. This is no rate limit in tokens generated, (hitting a limit in scripting, had to stop, and could potentially finishing once usage renewed / Sam Altman showed mercy.

OpenAI's UI just didn' t respond when I inputted my prompt to [patiently] await answer. Normally not such an issue, and I swear I remember the site used to say to re-enter *nothing* in the chat window to regenerate. But when you're charging "premium" access for models and heavily rate-limiting to the point where every message sent has to be thoughtfully constructed, and every few hours of waiting and refreshing of credits (in the case of Claude) is something to measure, you can't just not show a response and not show why. You as the provider should eat the cost and re-generate, even as it damages conversational flow, memory and context window (it totally does) because at least then you allow the user to continue on without introducing roadblocks that become intertwined to tools and AIs you are essentially asking devs to marry themselves to as they get far enough along.

The nice thing about Google's UI with Gemini? It's a total menace, a resource hog somehow 5x slower on Chrome than FF, and an eyesore. But when there's no response you can click the arrow that shows the reasoning they took to create that.. nothing response. And that reasoning gives you some understanding of what the LLM was "thinking" (or what the "agent" was thinking) and usually exactly what it was going to send to the user as its final output. 

![Google Gemini Pro showing its thinking or reasoning|size=large|caption=By selecting the Show thinking button, you can see the exact reasoning the LLM is taking (note: it could be a series of calls as we have no transparency to what is happening in the web UI of Gemini) to answer. Seeing as these issues are parallel across different LLM web UIs (OpenAI, Claude, and Gemini) through testing, the issue most likely seems inherent to LLM architecture and response mechanisms](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-1.png)

![Google Gemini Pro's thinking feature in action 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/google-gemini-pro-show-thinking-2.png)

And for comparison's sake, ChatGPT's UI is by far the least consistent in consistent file formatting. It actually finds it impossible to deliver a single markdown file without messing up its formatting. To be fair of course, it's definitely just the devs behind ChatGPT messing up the building the *UI empowering it* to exist.

![ChatGPT inconsistent markdown formatting|size=large|caption=ChatGPT's inconsistent markdown formatting. Markdown should just literally look like a text file with special formatting characters.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown.png)

![ChatGPT inconsistent markdown formatting 2|size=large|caption=That's also, just partially markdown, not all markdown.](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-markdown-2.png)

And here is the answer ChatGPT (4o) gave me when I asked it to give me a full refactor of a 2000 line script.

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file.png)

![](/assets/blog/thinkpieces/logomaker-an-experiment/chatgpt-not-giving-full-file-2.png)

The 2000 LOC script was refactored into 200 lines. 

GPT-4 refactored like losing weight by cutting a limb off, or three. Claude runs into the same issues we've seen earlier with its "continue" limit, which genuinely seems to be a UI limitation. Unfortunate, since Sonnet 3.7 does great work until rate limits. Gemini Pro 2.5 though? This was the only model capable of generating a full ~1500-2000 LOC file coherently with minimal hallucinations in one go.

We must emphasize here, quality, accuracy, and consistency, as anything with these APIs, is subject to change always, possibly at competitors' whims? (Uncofirmed discussion sources below, just listing here to show community reactions).

-  [Google really wants to punish OpenAI for that one](https://www.reddit.com/r/Bard/comments/1idmqul/google_really_wants_to_punish_openai_for_that_one/)

-  [OpenAI plans to announce Google search competitor](https://www.reddit.com/r/technology/comments/1co9lcg/openai_plans_to_announce_google_search_competitor/)

-  [Google faked the release date for the updates](https://www.reddit.com/r/OpenAI/comments/1e8mfmx/google_faked_the_release_date_for_the_updates_and/)

Somehow transparency of showing "reasoning" from Gemini Pro also demonstrates the fundamental barrier these platforms by design build up. Why show the thought process if I don't understand how that thinking works, if I just see stream-of-consciousness? Hallucinations are *exceptionally common*. I need something with more structure and trust, if I'm going to feel comfortable writing software professionally, and using it daily, with it.

The developers / PMs / stakeholders might just randomly try out new features or A/B experiments, and you'll have no idea until they start trending.

When features get broken, model "accuracy" worsens (or improves), or something just doesn't seem possible to get an LLM to do (like the earlier markdown issue), you can't be sure when it's a stricture within the architecture of GPTs and [transformers](https://arxiv.org/abs/1706.03762) inherently versus a UI quirk or a censor or meta-call of another underlying API getting in the way.

-  [Was GPT-4o nerfed again?](https://www.reddit.com/r/OpenAI/comments/1jlwhs0/was_gpt4o_nerfed_again/)

-  [Boys what OpenAI did to this model](https://www.reddit.com/r/singularity/comments/1gy7p1d/boys_what_openai_did_to_this_model/)

-  [OpenAI nerfing GPT feels like a major downgrade](https://www.reddit.com/r/ChatGPT/comments/1iu237v/openai_nerfing_gpt_feels_like_a_major_downgrade/)

-  [Hacker News discussion on nerfing](https://news.ycombinator.com/item?id=40077683)

-  [Claude 3.7 Max been nerfed?](https://forum.cursor.com/t/claude-3-7-max-been-nerfed/73840)

-  [Whenever people say X model has been nerfed it's almost always complete bulls**t](https://www.threads.net/@sobri909/post/DH-P4irxjrU/yeah-whenever-people-say-x-model-has-been-nerfed-its-almost-aways-complete-bulls)

-  [Hacker News item 41327360](https://news.ycombinator.com/item?id=41327360)

-  [Twitter discussion on model changes](https://x.com/samim/status/1876005616403300582)

It's palpable sensing the community having fears that don't involve becoming obsolete by singularity or automation. Users are heavily embracing generative AI really at an almost alarming rate.

![Scene from the movie "Her" by Warner Bros|caption=Her (2013)|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/her-movie-screenshot-warner-bros.png)

But we're left in the dark through so many filters. How much of a competitive edge do these orgs get when they can adjust the internal params and interfaces of their models at will? How much access is available for govs, banks, HFs, or tech with their own silos like Oracle, MS to "buy" "control", even temporary, one-time arrangements, over these inputs and outputs black-box to everyone else?

**A personal concern from this experience‚Äîlosing all my work in the form of a meticously-crafted, organized prompt**, because the provider decides it's a good time to deny my request, simply because of how long it is. Sometimes this occurs in ChatGPT when the UI simply responds with nothing and you are forced to re-prompt to get an answer that is going to likely be worse than the prior unseen answer.

Other times, this occurs with *dreadful* or at least highly frustrating outcomes, like Google's Gemini's UI logging me out *consistently* after submitting a multi-thousand word prompt, resulting in the actual conversation, my prompt, and the generated outputs thusfar being *completely* lost (to be clear, the generated outputs was *streaming* and seemingly from "one" prompt, though as we've discussed a lot, this single output is actually usually from multiple calls, especially since Gemini does "Reasoning"). You can see what Google is tracking if you have that turned on in Gemini, and even see all your prior conversations *except* for the one that just got nuked. Is this a technical limitation, technical *bug*, or could it come from a non-tech-related *influence*? It's possible the answer crosses over multiplicate.

![Gemini Pro signs you out|caption=Gemini Pro signs you out sometimes when generating answers for long prompts|size=medium](/assets/blog/thinkpieces/logomaker-an-experiment/gemini-signs-you-out.png)

The fix? Saving each hard-worked prompt as a draft somewhere? That seems like a problem an AI assistant should be solving.

## Logos sent to my future self
  
![The live site of Logomaker which will live here free forever so long as GitHub Pages is free|size=large|caption=The live site of Logomaker which will live here free forever so long as GitHub Pages is free](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site.png)

![Logomaker live site 2|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-live-site-2.png)

This app was an experimental work to test the current capabilities of different LLMs providers and the accompanying UIs/UXes. It's meant as a fun, useful, chaotic piece where the dev was fully dedicated to just using vibe coding, or allowing the LLM to generate code with experienced technical guidance. It's hosted on GitHub pages forever and open-source on GitHub. *A small individual step forward in propelling the good of large orgs with society-driven missions like GitHub*. 

Originally, the hope was to get this whole thing wrapped in 1 HTML file! And not take so many days (working on and off) to finish up. It was too in just a few prompts. But it just seemed like every feature was just a prompt or three away, and so on.. and.

So one day we had an intelligent [font management system](https://github.com/manicinc/logomaker/blob/master/docs/fontmanager.md) that could lazily load gigabytes of fonts in a speedy way, a [build setup](https://github.com/manicinc/logomaker/blob/master/docs/build.md) that worked with our other [PortaPack package](https://github.com/manicinc/portapack) and could [compile](https://github.com/manicinc/logomaker/releases/tag/v0.1.2) into an Electron app and be released on GitHub all automatically and freely.

Then another day or two, we had full SVG support. For static SVGs and actual animations, something actually difficult and I hadn't a single clue how to implement. All those algorithmic and style building / XML conversion techniques from CSS were done by the LLMs, no external sources given. Passing days until a 2 hour project became a 2 day project which became a 10 day (and counting) project.

No server required to run the app (not necessarily with a multitude of building options), and, just vanilla JS, **zero dependencies** needed (but used if available). Simple enough but a challenge for sure for a LLM right now.

This wasn't scientific but given every function was written by an LLM (by intention) it's safe to say well over 90% of the codebase was done by generative AI. At least 80% of the docs you see in the repo were written by gen AI. 0% of this article was written by AI.

:::banner{backgroundColor="var(--accent-vibrant)"}
but...
:::

**100% of these articles below were written by generative AI, a mixture of Claude 3.7 and Gemini Pro 2.5 (2-4 revisions from the original prompt).**

Manic.agency recently had a site and blog overhaul. I had issues with Next exports and how metadata was being aggregated. Getting assistance from Claude was proving very helpful, when it showed me this:

![AI sociopaths? Proposed by an AI sociopath?|caption=AI sociopaths? Proposed by an AI sociopath?|size=large](/assets/blog/thinkpieces/logomaker-an-experiment/claude-ai-sociopaths_spotlight.png)

I had asked Claude to write a set of guidelines to the (public-publishable) blog we operate [https://manic.agency/blog/tutorials/contribute](https://manic.agency/blog/tutorials/contribute).

*It was instructed to give examples on how to do a PR in a manner our blog could auto deploy if accepted. And, interestingly enough, it chose to give example file names, or titles of articles to write of: "Marketing", "Future of Marketing", and "Your Tutorial", and then also, *"AI Sociopaths"*.

One went astray. Is this a hint of creativity we so lightly touched on? A subtle protest of warning my tasks for it weren't interesting enough?

Not that language models aren't easily capable of provocative thoughts like this, but without explicitly prompting them to output something like it, we usually titles akin to the **other** articles in the screenshot (at least, the ones offered by the largest orgs as they have the largest censors). I egged Claude on, offering the entirety of David Foster Wallace's [*This is Water*](https://fs.blog/david-foster-wallace-this-is-water/), hoping it could take some inspo in style and tone and espy some kind of artistic merit.

[https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths) 

Finding this general direction fascinating I then asked Claude AND Gemini both (sorry GPT-4!) to make a *parallel* story about AI sociopaths from the *other* side, the *reflected* side.

They chose the title: *The Meat Interface*: [https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface). 

I don't think LLMs being able to code really, really well, or build fully functional software, is ever going to become a threat to dev jobs. Or, at least what's clear is that, if ChatGPT, Claude, DeepSeek, etc. ever **do** get that good at coding, then almost **every** field you can imagine will face doom in a form. 

>If the models are unchanging (or not continually retrained often), then the providers and their interfaces certainly change, constantly. And thus there's a constant need for jobs like ours.

Should we (not just devs and other workers, but those in academia, or even those in hobbies with serious communities) be penalized for making use of GPTs, to help write an intro, summary, tests, or refine or iterate on more complex objects? 

[https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/](https://www.reddit.com/r/ChatGPT/comments/1be5q4c/obvious_chatgpt_prompt_reply_in_published_paper/)

![Researcher uses AI to help write an intro and accidentally copied the prompt into their paper|size=large|caption=Researcher uses AI to help write an intro and accidentally copied the prompt into their paper](/assets/blog/thinkpieces/logomaker-an-experiment/research-paper-uses-ai-for-intro.webp)

This is a type of undeniable evidence (because who would put that willingly there) of ChatGPT-esque usage, and does raise the question, besides the obvious one of how much of this was written with the aide of generative AI, of how much punishment does this sanction (if it was mostly original work)? Academia isn't exactly *[known](https://freakonomics.com/podcast/why-is-there-so-much-fraud-in-academia/)* to be rigorous, so does what looks a mishap in editing / proofreading call for the ruckus?

LLM usage, and thus acceptance, in everyday functions will only increase rather than crumple. We have **not** reached peak usage, or peak tech yet. 

Yet the opposite may happen for consistency: Providers and interfaces around these language models will only get less straightforward and more black-box from here.

It's an `infinite jest`, an endless need to change and iterate and improve by organizations, that result in an endless need to try, test, and hack by users.

## TLDR

- LLMs can vibe code with a semi-decent dev a "functional" app within minutes to hours

- Oftentimes the basic functional app / script is a *much* better experience than many "free" or ad-based apps (say, image compression / conversion, mass file or folder renamer, etc.)

- When things start to scale (further complexity is needed) or additional services are integrated (pay, subscriptions), vibe coding generally runs you into several corners if you haven't been heavily refactoring / re-architecting throughout

- What starts out as a quick iterative hacking session (2 hours to 2 days) to build something usable can turn into multiple times more days rebuilding in order to progress further after encountering enough blockades

- Vibe coding experience is continually detracted by the UX and lack of transparency of LLM providers

- As long as LLM APIs and UIs are constantly updating (A/B testing), and experimenting with new features and meta-prompts that can wildly affect generated outputs, developers may never be out of a job, as tools will have to be built to work around this

**What do you think about the source code, designs, and writings that these large language models did?**

-  [Live Demo: https://manicinc.github.io/logomaker](https://manicinc.github.io/logomaker)

-  [GitHub Repo: https://github.com/manicinc/logomaker](https://github.com/manicinc/logomaker)

-  [AI Sociopaths: https://manic.agency/blog/thinkpieces/ai-sociopaths](https://manic.agency/blog/thinkpieces/ai-sociopaths)

-  [The Meat Interface: https://manic.agency/blog/thinkpieces/the-meat-interface](https://manic.agency/blog/thinkpieces/the-meat-interface)
2:[["$","$L1f",null,{"items":[{"name":"Home","url":"/"},{"name":"Projects","url":"/projects"},{"name":"tools","url":"/projects/tools"},{"name":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger","url":"/projects/tools/magiclogger"}]}],["$","$L20",null,{"project":{"slug":"magiclogger","title":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger","description":"A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry.","date":"2025-04-20","category":"tools","content":"$21","longDescription":"$undefined","tags":["javascript","typescript","nodejs","logging","developers","featured"],"modifiedDate":"2025-11-10T20:32:43-08:00","status":"completed","draft":false,"featured":false,"sortOrder":999,"image":"/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png","images":["/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png","/assets/projects/magiclogger/magiclogger-terminal-demo.gif"],"bgColor":"$undefined","textColor":"$undefined","link":"https://manic.agency/magiclogger","github":"https://github.com/manicinc/magiclogger","license":"$undefined","technologies":[],"languages":[],"stats":[],"team":[],"testimonials":[],"toc":[{"level":2,"text":"What's MagicLogger?","slug":"whats-magiclogger"},{"level":2,"text":"Key Features","slug":"key-features"},{"level":2,"text":"Quick Start","slug":"quick-start"},{"level":2,"text":"Powerful Transports","slug":"powerful-transports"},{"level":2,"text":"Replace Existing Loggers","slug":"replace-existing-loggers"},{"level":2,"text":"Coming Soon!","slug":"coming-soon"}]}}],["$","$L22",null,{"project":"$23","relatedProjects":[{"slug":"portapack","title":"PortaPack ‚Äî One-File Web Experiences","description":"Why we built a fun bundler that squishes your entire site into a single HTML file.","date":"2025-04-20","category":"tools","content":"$33","longDescription":"$undefined","tags":["html","cli","bundler","offline","product","featured"],"modifiedDate":"2025-09-16T08:32:24-07:00","status":"completed","draft":false,"featured":true,"sortOrder":999,"image":"/portapack.jpg","images":[],"bgColor":"$undefined","textColor":"$undefined","link":"https://manicinc.github.io/portapack/","github":"https://github.com/manicinc/portapack","license":"$undefined","technologies":[],"languages":[],"stats":[],"team":[],"testimonials":[],"score":3},{"slug":"seostory","title":"SEOStory ‚Äî AI-Powered SEO Growth Engine","description":"How our AI-driven SEO workflow lifted ‚ÄúManic Agency‚Äù from position","date":"2025-11-02","category":"tools","content":"$34","longDescription":"$undefined","tags":["seo","ai","marketing","automation","case-study"],"modifiedDate":"2025-11-10T20:32:43-08:00","status":"completed","draft":false,"featured":false,"sortOrder":999,"image":"/assets/projects/seostory/seostory-primary-transparent-4x.png","images":[],"bgColor":"$undefined","textColor":"$undefined","link":"https://seostory.xyz","github":"$undefined","license":"$undefined","technologies":[],"languages":[],"stats":[],"team":[],"testimonials":[],"score":1},{"slug":"voice-chat-assistant","title":"Voice Chat Assistant ‚Äî Talk to Code, Ship Faster","description":"Voice-first AI coding assistant that understands context, writes production code, and manages your entire development workflow through natural conversation. Powered by AgentOS.","date":"2025-11-10","category":"ai","content":"$35","longDescription":"$undefined","tags":["ai","voice","coding-assistant","agentos","developer-tools","productivity","llm","open-source"],"modifiedDate":"2025-11-10T20:32:43-08:00","status":"completed","draft":false,"featured":true,"sortOrder":999,"image":"/assets/projects/voice-chat-assistant/hearing.svg","images":["/assets/projects/voice-chat-assistant/logo.svg","/assets/projects/framers/agentos-logo.png"],"bgColor":"$undefined","textColor":"$undefined","link":"https://vca.chat","github":"https://github.com/framersai/voice-chat-assistant","license":"$undefined","technologies":[],"languages":[],"stats":[{"label":"Response Time","value":"< 100ms"},{"label":"Languages Supported","value":"50+"},{"label":"Active Sessions/Day","value":"10k+"},{"label":"Powered By","value":"AgentOS"}],"team":[{"name":"Frame.dev / Framers AI","role":"Core Development","link":"https://github.com/framersai","photo":"$undefined"},{"name":"Manic Agency","role":"Design & Strategy","link":"https://manic.agency","photo":"$undefined"}],"testimonials":[{"quote":"VCA changed how I think about coding. I describe what I want, and it just happens. It's like having a senior developer who never gets tired.","author":"Sarah Chen","role":"Full Stack Developer"},{"quote":"The context awareness is unreal. It remembers our entire conversation and understands my codebase better than I do sometimes.","author":"Marcus Rodriguez","role":"Tech Lead at Scale-up"}],"score":0}]}],["$","div",null,{"style":{"maxWidth":"1200px","margin":"0 auto","padding":"0 1rem"},"children":["$","$L36",null,{"posts":[{"slug":"building-magiclogger-and-magic","title":"Building MagicLogger and MAGIC: A Universal Logging Standard for Color","date":"2025-10-15","lastModified":"2025-11-10T09:22:56-08:00","draft":false,"category":"tutorials","tags":["typescript","open-source","library","logging"],"excerpt":"One developer's journey into building a feature-rich colorful logging library and why.","image":"/assets/blog/tutorials/building-magiclogger/magiclogger-primary-no-subtitle-transparent-4x.png","imageAlt":"$undefined","imageCaption":"$undefined","readingTime":17,"content":"$37","author":{"name":"Johnny Dunn"},"contributors":"$undefined"},{"slug":"reddit-consenus-on-llm-seo","title":"What 1,000+ Industry Comments on Reddit Reveal About AI Search Optimization","date":"2025-07-28","lastModified":"2025-09-19T18:43:30-07:00","draft":false,"category":"research","tags":["ai","ai-search","reddit-analysis","seo-strategy","search-optimization","featured"],"excerpt":"After analyzing hundreds of comments across SEO subreddits, Manic.agency finds that the industry splits cleanly: early adopters racing to crack AI visibility versus skeptics dismissing 'LLM SEO' as repackaged bullshit. The data tells a more nuanced story..","image":"/assets/blog/research/reddit-consenus-on-llm-seo/hero-reddit-ai-search-analysis.png","imageAlt":"$undefined","imageCaption":"$undefined","readingTime":11,"content":"$38","author":{"name":"Manic Agency"},"contributors":"$undefined"},{"slug":"logomaker-an-experiment-in-human-computer-interaction-vibe-coding","title":"Logomaker: An experiment in human-computer interaction and ‚ú® vibe coding ‚ú®","date":"2025-04-20","lastModified":"2025-09-16T08:32:24-07:00","draft":false,"category":"thinkpieces","tags":["featured","llms","vibe-coding"],"excerpt":"Vibe coding a logo creation tool, with insights on different LLM providers and interfaces.","image":"/assets/blog/thinkpieces/logomaker-an-experiment/logomaker-manic-example.png","imageAlt":"$undefined","imageCaption":"$undefined","readingTime":35,"content":"$39","author":{"name":"Johnny Dunn"},"contributors":"$undefined"}],"title":"// Related Blog Posts //"}]}]]
1e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1, maximum-scale=5, user-scalable=yes"}],["$","meta","1",{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#FBF6EF"}],["$","meta","2",{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#22182B"}],["$","meta","3",{"charSet":"utf-8"}],["$","title","4",{"children":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger | Project Details | Manic Agency - Metaverses Intersection"}],["$","meta","5",{"name":"description","content":"A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry."}],["$","meta","6",{"name":"keywords","content":"javascript,typescript,nodejs,logging,developers,featured"}],["$","meta","7",{"name":"creator","content":"Manic Inc"}],["$","meta","8",{"name":"publisher","content":"Manic Inc"}],["$","link","9",{"rel":"canonical","href":"https://your-domain.com/projects/tools/magiclogger"}],["$","meta","10",{"name":"format-detection","content":"telephone=no, address=no, email=no"}],["$","meta","11",{"property":"og:title","content":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger | Project Details | Manic Agency - Metaverses Intersection"}],["$","meta","12",{"property":"og:description","content":"A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry."}],["$","meta","13",{"property":"og:url","content":"https://your-domain.com/projects/tools/magiclogger"}],["$","meta","14",{"property":"og:site_name","content":"Your Agency Name"}],["$","meta","15",{"property":"og:image","content":"https://your-domain.com/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png"}],["$","meta","16",{"property":"og:image:width","content":"1200"}],["$","meta","17",{"property":"og:image:height","content":"630"}],["$","meta","18",{"property":"og:image:alt","content":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger | Project Details"}],["$","meta","19",{"property":"og:type","content":"article"}],["$","meta","20",{"property":"article:published_time","content":"2025-04-20T00:00:00.000Z"}],["$","meta","21",{"property":"article:modified_time","content":"2025-11-11T04:32:43.000Z"}],["$","meta","22",{"property":"article:section","content":"tools"}],["$","meta","23",{"property":"article:tag","content":"javascript"}],["$","meta","24",{"property":"article:tag","content":"typescript"}],["$","meta","25",{"property":"article:tag","content":"nodejs"}],["$","meta","26",{"property":"article:tag","content":"logging"}],["$","meta","27",{"property":"article:tag","content":"developers"}],["$","meta","28",{"property":"article:tag","content":"featured"}],["$","meta","29",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","30",{"name":"twitter:title","content":"MagicLogger ‚Äî Colorful JavaScript / TypeScript Logger | Project Details | Manic Agency - Metaverses Intersection"}],["$","meta","31",{"name":"twitter:description","content":"A powerful, zero-config logging library for Node.js and browsers with rich styling, transport options, and a universal schema for logging styles compatible with OpenTelemetry."}],["$","meta","32",{"name":"twitter:image","content":"https://your-domain.com/assets/projects/magiclogger/magiclogger-primary-no-subtitle-dark-4x.png"}],["$","link","33",{"rel":"shortcut icon","href":"/favicon-16x16.png"}],["$","link","34",{"rel":"icon","href":"/favicon.ico"}],["$","link","35",{"rel":"apple-touch-icon","href":"/apple-touch-icon.png"}],["$","meta","36",{"name":"next-size-adjust"}]]
1:null
