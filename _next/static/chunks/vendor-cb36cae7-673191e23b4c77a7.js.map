{"version":3,"file":"static/chunks/vendor-cb36cae7-673191e23b4c77a7.js","mappings":"mIC4BA,IAAAA,EA1BA,CACAC,KAAAC,CAAA,GACQC,SDJDD,CAAA,EACP,QACAE,EAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAAAL,EAcSM,OAdTL,EAcSM,SAdTL,EAcS,UAdTC,EAcS,SAbTF,EAAAO,cAAA,qBAGAR,CAAA,CAAAE,EAAA,CAAAF,CAAA,CAAAE,EAAA,EACA,WACA,CAAAF,CAAA,CAAAE,EAAA,CAAAO,CAAA,CAAAT,CAAA,CAAAE,EAAA,CAAAO,CAAA,MAAAC,IAAA,CAAAC,UACA,EAEAP,CADAA,EAAAH,EAAAW,aAAA,CAAAT,EAAA,EACAU,KAAA,GACAT,EAAAU,GAAA,+BAIShB,EAJT,WACAM,EAAAW,EAAA,kBAEAV,CADAA,EAAAJ,EAAAe,oBAAA,CAAAb,EAAA,KACAc,UAAA,CAAAC,YAAA,CAAAd,EAAAC,GAGA,CAAG,MAAAc,EAAA,CACH,MACA,CACA,ECjBoBrB,EAAA,iBACpB,EAEAsB,OAAAC,CAAA,CAAAC,CAAA,EACAhB,OAAAV,OAAA,OAAAyB,EAAAC,EACA,EAEAC,SAAAC,CAAA,CAAAC,CAAA,CAAAC,CAAA,CAAAC,CAAA,EACArB,OAAAV,OAAA,YAAA4B,EAAAC,EAAAC,EAAAC,EACA,EAEAC,QAAAA,EAAA,IACAtB,OAAAV,OAAA,WAAAgC,EACA,EAEAC,QAAAC,CAAA,EACAxB,OAAAV,OAAA,WAAAkC,EACA,EAEAC,MAAAC,CAAA,EACA1B,OAAAV,OAAA,SAAAoC,EACA,CACA,2YCbO,IAAAC,EAAA,CACPC,SASA,SAAAC,CAAA,MAGAC,EAFA,IAAAC,EAAAF,EAAAG,OAAA,MAAAC,MAAA,CAAAC,UAAA,CAAAC,cAAA,CAMA,SAAAC,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAP,EAAAQ,OAAA,CAAAD,GACA,MACA,CAIA,OAHAP,EAAAS,KAAA,eACAT,EAAAQ,OAAA,CAAAD,GACAP,EAAAU,IAAA,eACW,GAAAC,EAAAC,CAAA,EAAYZ,EAAAE,EAAA,aACvB,EAGA,SAAAK,CAAA,EAEA,OADAP,EAAAS,KAAA,cACAI,SAIAA,EAAAN,CAAA,EACA,IAAAO,EAAAd,EAAAS,KAAA,cACAM,YAAA,OACAd,SAAAA,CACA,GAKA,OAJAA,GACAA,CAAAA,EAAAe,IAAA,CAAAF,CAAA,EAEAb,EAAAa,EACAG,SAIAA,EAAAV,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAP,EAAAU,IAAA,cACAV,EAAAU,IAAA,cACAV,EAAAQ,OAAA,CAAAD,GACA,MACA,OACA,CAAQ,EAAAW,EAAAC,EAAA,EAAkBZ,IAC1BP,EAAAQ,OAAA,CAAAD,GACAP,EAAAU,IAAA,cACAG,IAIAb,EAAAQ,OAAA,CAAAD,GACAU,EACA,EApBAV,EACA,EAdAA,EACA,GAlBA,OAAAL,CAmDA,CA/DA,iBCQO,IAAMkB,EAAQ,CACrBrB,SAcA,SAAAC,CAAA,MAMAqB,EAEAC,EAEAC,EATA,IAAAC,EAAA,KAEAC,EAAA,GACAC,EAAA,EAOA,OAAAC,EAGA,SAAAA,EAAApB,CAAA,EAWA,GAAAmB,EAAAD,EAAAG,MAAA,EACA,IAAAC,EAAAJ,CAAA,CAAAC,EAAA,CAEA,OADAF,EAAAM,cAAA,CAAAD,CAAA,IACA7B,EAAAG,OAAA,CAAA0B,CAAA,IAAAE,YAAA,CAAAC,EAAAC,GAAA1B,EACA,CAGA,OAAA0B,EAAA1B,EACA,CAGA,SAAAyB,EAAAzB,CAAA,EAMA,GALAmB,IAKAF,EAAAM,cAAA,CAAAI,UAAA,MAWAC,CAVAX,CAAAA,EAAAM,cAAA,CAAAI,UAAA,CAAAE,KAAAA,EACAf,GACAgB,IAKA,IAAAC,EAAAd,EAAAe,MAAA,CAAAX,MAAA,CACAY,EAAAF,EAKA,KAAAE,KACA,GAAAhB,SAAAA,EAAAe,MAAA,CAAAC,EAAA,KAAAhB,cAAAA,EAAAe,MAAA,CAAAC,EAAA,IAAAC,IAAA,EACAN,EAAAX,EAAAe,MAAA,CAAAC,EAAA,IAAAE,GAAA,CACA,KACA,CAEAC,EAAAjB,GAGA,IAAAkB,EAAAN,EACA,KAAAM,EAAApB,EAAAe,MAAA,CAAAX,MAAA,EACAJ,EAAAe,MAAA,CAAAK,EAAA,IAAAF,GAAA,EACA,GAAAP,CAAA,EAEAS,IAQA,MAJM,GAAAC,EAAAC,CAAA,EAAMtB,EAAAe,MAAA,CAAAC,EAAA,IAAAhB,EAAAe,MAAA,CAAAQ,KAAA,CAAAT,IAGZd,EAAAe,MAAA,CAAAX,MAAA,CAAAgB,EACAX,EAAA1B,EACA,CACA,OAAAoB,EAAApB,EACA,CAGA,SAAA0B,EAAA1B,CAAA,EAMA,GAAAmB,IAAAD,EAAAG,MAAA,EAIA,IAAAP,EACA,OAAA2B,EAAAzC,GAMA,GAAAc,EAAA4B,gBAAA,EAAA5B,EAAA4B,gBAAA,CAAAC,QAAA,CACA,OAAAC,EAAA5C,EAQAiB,CAAAA,EAAA4B,SAAA,CAAAC,CAAAA,CAAAhC,CAAAA,EAAA4B,gBAAA,GAAA5B,EAAAiC,6BAAA,CACA,CAIA,OADA9B,EAAAM,cAAA,IACA9B,EAAAuD,KAAA,CAAAC,EAAAC,EAAAC,GAAAnD,EACA,CAGA,SAAAkD,EAAAlD,CAAA,EAGA,OAFAc,GAAAgB,IACAM,EAAAjB,GACAsB,EAAAzC,EACA,CAGA,SAAAmD,EAAAnD,CAAA,EAGA,OAFAiB,EAAApB,MAAA,CAAAuD,IAAA,CAAAnC,EAAAoC,GAAA,GAAAC,IAAA,EAAAnC,IAAAD,EAAAG,MAAA,CACAL,EAAAC,EAAAoC,GAAA,GAAAE,MAAA,CACAX,EAAA5C,EACA,CAGA,SAAAyC,EAAAzC,CAAA,EAGA,OADAiB,EAAAM,cAAA,IACA9B,EAAAG,OAAA,CAAAqD,EAAAO,EAAAZ,GAAA5C,EACA,CAGA,SAAAwD,EAAAxD,CAAA,EAIA,OAHAmB,IACAD,EAAAlD,IAAA,EAAAiD,EAAAyB,gBAAA,CAAAzB,EAAAM,cAAA,GAEAkB,EAAAzC,EACA,CAGA,SAAA4C,EAAA5C,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAc,GAAAgB,IACAM,EAAA,GACA3C,EAAAQ,OAAA,CAAAD,GACA,MACA,CAOA,OANAc,EAAAA,GAAAG,EAAApB,MAAA,CAAA4D,IAAA,CAAAxC,EAAAoC,GAAA,IACA5D,EAAAS,KAAA,cACAwD,WAAA5C,EACAN,YAAA,OACAd,SAAAqB,CACA,GACA4C,SAIAA,EAAA3D,CAAA,EACA,GAAAA,OAAAA,EAAA,CACA4D,EAAAnE,EAAAU,IAAA,kBACAiC,EAAA,GACA3C,EAAAQ,OAAA,CAAAD,GACA,MACA,OACA,CAAQ,EAAAW,EAAAC,EAAA,EAAkBZ,IAC1BP,EAAAQ,OAAA,CAAAD,GACA4D,EAAAnE,EAAAU,IAAA,eAEAgB,EAAA,EACAF,EAAA4B,SAAA,CAAAhB,KAAAA,EACAT,IAEA3B,EAAAQ,OAAA,CAAAD,GACA2D,EACA,EArBA3D,EACA,CA8BA,SAAA4D,EAAArD,CAAA,CAAAsD,CAAA,EACA,IAAAC,EAAA7C,EAAA8C,WAAA,CAAAxD,GAyCA,GAxCAsD,GAAAC,EAAA9F,IAAA,OACAuC,EAAAb,QAAA,CAAAqB,EACAA,GAAAA,CAAAA,EAAAN,IAAA,CAAAF,CAAA,EACAQ,EAAAR,EACAO,EAAAkD,UAAA,CAAAzD,EAAAa,KAAA,EACAN,EAAAmD,KAAA,CAAAH,GAmCA7C,EAAApB,MAAA,CAAAuD,IAAA,CAAA7C,EAAAa,KAAA,CAAAkC,IAAA,GACA,IAoBAY,EAEAtC,EAtBAS,EAAAvB,EAAAkB,MAAA,CAAAX,MAAA,CACA,KAAAgB,KACA,GAEAvB,EAAAkB,MAAA,CAAAK,EAAA,IAAAjB,KAAA,CAAAmC,MAAA,CAAAvC,GAEA,EAAAF,EAAAkB,MAAA,CAAAK,EAAA,IAAAF,GAAA,EAEArB,EAAAkB,MAAA,CAAAK,EAAA,IAAAF,GAAA,CAAAoB,MAAA,CAAAvC,CAAA,EAGA,OAMA,IAAAe,EAAAd,EAAAe,MAAA,CAAAX,MAAA,CACAY,EAAAF,EAOA,KAAAE,KACA,GAAAhB,SAAAA,EAAAe,MAAA,CAAAC,EAAA,KAAAhB,cAAAA,EAAAe,MAAA,CAAAC,EAAA,IAAAC,IAAA,EACA,GAAAgC,EAAA,CACAtC,EAAAX,EAAAe,MAAA,CAAAC,EAAA,IAAAE,GAAA,CACA,KACA,CACA+B,EAAA,EACA,CAMA,IAJA9B,EAAAjB,GAGAkB,EAAAN,EACAM,EAAApB,EAAAe,MAAA,CAAAX,MAAA,EACAJ,EAAAe,MAAA,CAAAK,EAAA,IAAAF,GAAA,EACA,GAAAP,CAAA,EAEAS,IAIM,GAAAC,EAAAC,CAAA,EAAMtB,EAAAe,MAAA,CAAAC,EAAA,IAAAhB,EAAAe,MAAA,CAAAQ,KAAA,CAAAT,IAGZd,EAAAe,MAAA,CAAAX,MAAA,CAAAgB,CACA,CACA,CAQA,SAAAD,EAAA+B,CAAA,EACA,IAAA9B,EAAAnB,EAAAG,MAAA,CAGA,KAAAgB,KAAA8B,GAAA,CACA,IAAAC,EAAAlD,CAAA,CAAAmB,EAAA,CACApB,EAAAM,cAAA,CAAA6C,CAAA,IACAA,CAAA,IAAAjE,IAAA,CAAAkE,IAAA,CAAApD,EAAAxB,EACA,CACAyB,EAAAG,MAAA,CAAA8C,CACA,CACA,SAAArC,IACAhB,EAAAmD,KAAA,SACAlD,EAAAc,KAAAA,EACAf,EAAAe,KAAAA,EACAZ,EAAAM,cAAA,CAAAI,UAAA,CAAAE,KAAAA,CACA,CACA,CApUA,EAGAoB,EAAA,CACAzD,SAwUA,SAAAC,CAAA,CAAA6E,CAAA,CAAAC,CAAA,EAGA,MAAS,GAAAnE,EAAAC,CAAA,EAAYZ,EAAAA,EAAAG,OAAA,MAAAC,MAAA,CAAAC,UAAA,CAAAjC,QAAA,CAAAyG,EAAAC,GAAA,kBAAA1E,MAAA,CAAAC,UAAA,CAAA0E,OAAA,CAAAC,IAAA,CAAAC,QAAA,iBAAA7C,KAAAA,EAAA,EACrB,CA3UA,2BCjBO,IAAA4B,EAAA,CACPjE,SASA,SAAAC,CAAA,EACA,IAAAwB,EAAA,KACA0D,EAAAlF,EAAAG,OAAA,CAEEgF,EAAAC,CAAS,CAMX,SAAA7E,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAP,EAAAQ,OAAA,CAAAD,GACA,MACA,CAKA,OAJAP,EAAAS,KAAA,oBACAT,EAAAQ,OAAA,CAAAD,GACAP,EAAAU,IAAA,oBACAc,EAAAyB,gBAAA,CAAAb,KAAAA,EACA8C,CACA,EAdAlF,EAAAG,OAAA,MAAAC,MAAA,CAAAC,UAAA,CAAAgF,WAAA,CAAAC,EAAsE,GAAA3E,EAAAC,CAAA,EAAYZ,EAAAA,EAAAG,OAAA,MAAAC,MAAA,CAAAC,UAAA,CAAA2D,IAAA,CAAAsB,EAAAtF,EAAAG,OAAA,CAAuFoF,EAAAC,CAAO,CAAAF,IAAA,gBAChL,OAAAJ,EAgBA,SAAAI,EAAA/E,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAP,EAAAQ,OAAA,CAAAD,GACA,MACA,CAKA,OAJAP,EAAAS,KAAA,eACAT,EAAAQ,OAAA,CAAAD,GACAP,EAAAU,IAAA,eACAc,EAAAyB,gBAAA,CAAAb,KAAAA,EACA8C,CACA,CACA,CA1CA,ECJOO,EAAA,CACPC,WAAAC,GACA,EACOC,EAAAC,EAAA,UACMC,EAAID,EAAA,QAQjB,SAAAA,EAAAE,CAAA,EACA,OACAL,WAAAC,EAAAI,SAAAA,EAAAC,EAAA5D,KAAAA,GACArC,SAQA,SAAAC,CAAA,EACA,IAAAwB,EAAA,KACAnB,EAAA,KAAAD,MAAA,CAAAC,UAAA,CAAA0F,EAAA,CACAE,EAAAjG,EAAAG,OAAA,CAAAE,EAAAsB,EAAAuE,GACA,OAAAvE,EAGA,SAAAA,EAAApB,CAAA,EACA,OAAA4F,EAAA5F,GAAA0F,EAAA1F,GAAA2F,EAAA3F,EACA,CAGA,SAAA2F,EAAA3F,CAAA,EACA,GAAAA,OAAAA,EAAA,CACAP,EAAAQ,OAAA,CAAAD,GACA,MACA,CAGA,OAFAP,EAAAS,KAAA,SACAT,EAAAQ,OAAA,CAAAD,GACAU,CACA,CAGA,SAAAA,EAAAV,CAAA,SACA,EAAAA,IACAP,EAAAU,IAAA,SACAuF,EAAA1F,KAIAP,EAAAQ,OAAA,CAAAD,GACAU,EACA,CAQA,SAAAkF,EAAA5F,CAAA,EACA,GAAAA,OAAAA,EACA,SAEA,IAAA6F,EAAA/F,CAAA,CAAAE,EAAA,CACAqC,EAAA,GACA,GAAAwD,EAGA,OAAAxD,EAAAwD,EAAAxE,MAAA,GACA,IAAAC,EAAAuE,CAAA,CAAAxD,EAAA,CACA,IAAAf,EAAA5B,QAAA,EAAA4B,EAAA5B,QAAA,CAAA2E,IAAA,CAAApD,EAAAA,EAAAvB,QAAA,EACA,QAEA,CAEA,QACA,CACA,CAjEA,CAkEA,CAQA,SAAA0F,EAAAU,CAAA,EACA,OAGA,SAAA9D,CAAA,CAAA+D,CAAA,EACA,IAEA7F,EAFAmC,EAAA,GAMA,OAAAA,GAAAL,EAAAX,MAAA,EACAnB,KAAA2B,IAAA3B,EACA8B,CAAA,CAAAK,EAAA,EAAAL,SAAAA,CAAA,CAAAK,EAAA,IAAAH,IAAA,GACAhC,EAAAmC,EACAA,KAEQL,CAAA,CAAAK,EAAA,EAAAL,SAAAA,CAAA,CAAAK,EAAA,IAAAH,IAAA,GAERG,IAAAnC,EAAA,IACA8B,CAAA,CAAA9B,EAAA,IAAAiC,GAAA,CAAAH,CAAA,CAAAK,EAAA,MAAAF,GAAA,CACAH,EAAAgE,MAAA,CAAA9F,EAAA,EAAAmC,EAAAnC,EAAA,GACAmC,EAAAnC,EAAA,GAEAA,EAAA2B,KAAAA,GAGA,OAAAiE,EAAAA,EAAA9D,EAAA+D,GAAA/D,CACA,CACA,CAaA,SAAAyD,EAAAzD,CAAA,CAAA+D,CAAA,EACA,IAAAE,EAAA,EAEA,OAAAA,GAAAjE,EAAAX,MAAA,EACA,IAAA4E,IAAAjE,EAAAX,MAAA,EAAAW,eAAAA,CAAA,CAAAiE,EAAA,IAAA/D,IAAA,GAAAF,SAAAA,CAAA,CAAAiE,EAAA,MAAA/D,IAAA,MAOAgE,EANA,IAAAxF,EAAAsB,CAAA,CAAAiE,EAAA,MACAE,EAAAJ,EAAAhC,WAAA,CAAArD,GACA2B,EAAA8D,EAAA9E,MAAA,CACA+E,EAAA,GACAjC,EAAA,EAGA,KAAA9B,KAAA,CACA,IAAAgE,EAAAF,CAAA,CAAA9D,EAAA,CACA,oBAAAgE,EAAA,CAEA,IADAD,EAAAC,EAAAhF,MAAA,CACAgF,KAAAA,EAAAC,UAAA,CAAAF,EAAA,IACAjC,IACAiC,IAEA,GAAAA,EAAA,MACAA,EAAA,EACA,MAEA,GAAAC,KAAAA,EACAH,EAAA,GACA/B,SACU,GAAAkC,KAAAA,OAEA,CAEVhE,IACA,KACA,CACA,CAMA,GAHA0D,EAAAQ,wBAAA,EAAAN,IAAAjE,EAAAX,MAAA,EACA8C,CAAAA,EAAA,GAEAA,EAAA,CACA,IAAA5D,EAAA,CACA2B,KAAA+D,IAAAjE,EAAAX,MAAA,EAAA6E,GAAA/B,EAAA,mCACA/C,MAAA,CACAoF,aAAAnE,EAAA+D,EAAA1F,EAAAU,KAAA,CAAAoF,YAAA,CAAAJ,EACAK,OAAA/F,EAAAU,KAAA,CAAAqF,MAAA,CAAApE,EACAiB,KAAA5C,EAAAyB,GAAA,CAAAmB,IAAA,CACAoD,OAAAhG,EAAAyB,GAAA,CAAAuE,MAAA,CAAAvC,EACAZ,OAAA7C,EAAAyB,GAAA,CAAAoB,MAAA,CAAAY,CACA,EACAhC,IAAA,CACA,GAAAzB,EAAAyB,GAAA,CAEA,CACAzB,CAAAA,EAAAyB,GAAA,EACA,GAAA5B,EAAAa,KAAA,EAEAV,EAAAU,KAAA,CAAAmC,MAAA,GAAA7C,EAAAyB,GAAA,CAAAoB,MAAA,CACAoD,OAAAC,MAAA,CAAAlG,EAAAH,IAEAyB,EAAAgE,MAAA,CAAAC,EAAA,WAAA1F,EAAAwF,EAAA,SAAAxF,EAAAwF,EAAA,EACAE,GAAA,EAEA,CACAA,GACA,CAEA,OAAAjE,CACA,iOC3MO,IAAM6E,EAAQ,CACrB,GAAQhB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQjB,EAAAiB,CAAI,CACZ,GAAQC,EAAAC,CAAU,EAIXjH,EAAA,CACP,GAAQkH,EAAAC,CAAU,EAIXpC,EAAA,CACP,KAAQqC,EAAAC,CAAY,CACpB,KAAQD,EAAAC,CAAY,CACpB,GAAQD,EAAAC,CAAY,EAIPC,EAAI,CACjB,GAAQC,EAAAC,CAAU,CAClB,GAAQC,EAAAC,CAAa,CACrB,IAASC,EAAAD,CAAe,CAAED,EAAAC,CAAa,EACvC,GAAQE,EAAAC,CAAQ,CAChB,GAAQF,EAAAD,CAAe,CACvB,GAAQD,EAAAC,CAAa,CACrB,GAAQI,EAAAC,CAAU,CAClB,IAASD,EAAAC,CAAU,EAINC,EAAM,CACnB,GAAQC,EAAAC,CAAkB,CAC1B,GAAQC,EAAAC,CAAe,EAIVC,EAAI,CACjB,KAAQC,EAAAC,CAAU,CAClB,KAAQD,EAAAC,CAAU,CAClB,KAAQD,EAAAC,CAAU,CAClB,GAAQC,EAAAd,CAAe,CACvB,GAAQO,EAAAC,CAAkB,CAC1B,GAAQO,EAAAC,CAAS,CACjB,IAASC,EAAAC,CAAQ,CAAEC,EAAAT,CAAQ,EAC3B,GAAQU,EAAAC,CAAc,CACtB,IAASC,EAAAC,CAAe,CAAEd,EAAAC,CAAe,EACzC,GAAQc,EAAA1B,CAAQ,CAChB,GAAQiB,EAAAC,CAAS,CACjB,GAAQS,EAAAC,CAAQ,EAITC,EAAA,CACP3E,KAAA,CAAS+D,EAAAC,CAAS,CAAEvD,EAAW,EAIxBmE,EAAA,CACP5E,KAAA,SAIOD,EAAA,CACPC,KAAA,mBE3DO,SAAA6E,EAAAC,CAAA,EAMP,IAAA1J,EAAA,CACAC,WAJE,GAAA0J,EAAAC,CAAA,EAAiB,CAAEC,KAAiBC,CAFtCJ,GAAA,IAEsCK,UAAA,OAKtCrK,QAAAsK,EAAoBtK,GACpBuK,QAAA,GACAjM,SAAAgM,EAAqBhJ,GACrB4C,KAAAoG,EAAiBpG,GACjBL,KAAA,GACAiC,OAAAwE,EAAmBxE,GACnBK,KAAAmE,EAAiBtE,EACjB,EACA,OAAA1F,EAQA,SAAAgK,EAAAlF,CAAA,EACA,OAEA,SAAAoF,CAAA,EACA,OAAaC,SDQNnK,CAAA,CAAAoK,CAAA,CAAAF,CAAA,EAEP,IAAAnI,EAAA,CACA4E,aAAA,GACAC,OAAA,EACAnD,KAAAyG,GAAAA,EAAAzG,IAAA,IACAoD,OAAAqD,GAAAA,EAAArD,MAAA,IACAnD,OAAAwG,GAAAA,EAAAxG,MAAA,GACA,EAEA2G,EAAA,GAEAC,EAAA,GAEAhE,EAAA,GAEAjF,EAAA,GASAzB,EAAA,CACAG,QAAAwK,EAoNA,SAAAC,CAAA,CAAAC,CAAA,EACAC,EAAAF,EAAAC,EAAAP,IAAA,CACA,GArNA/G,MAAAoH,EAAAI,GACAvK,QAsJA,SAAAD,CAAA,EACQ,GAAAW,EAAAC,EAAA,EAAkBZ,IAC1B4B,EAAA0B,IAAA,GACA1B,EAAA8E,MAAA,GACA9E,EAAA2B,MAAA,EAAAvD,KAAAA,EAAA,IACAyK,KACM,KAAAzK,IACN4B,EAAA8E,MAAA,GACA9E,EAAA2B,MAAA,IAIA3B,EAAA4E,YAAA,GACA5E,EAAA6E,MAAA,IAEA7E,EAAA4E,YAAA,GAGA5E,EAAA4E,YAAA,GAIAL,CAAA,CAAAvE,EAAA6E,MAAA,EAAApF,MAAA,GACAO,EAAA4E,YAAA,IACA5E,EAAA6E,MAAA,KAKAV,EAAArG,QAAA,CAAAM,CAIA,EAtLAE,MAyLA,SAAAgC,CAAA,CAAAwI,CAAA,EAGA,IAAAnK,EAAAmK,GAAA,GAKA,OAJAnK,EAAA2B,IAAA,CAAAA,EACA3B,EAAAa,KAAA,CAAAiC,IACA0C,EAAA/D,MAAA,CAAAhE,IAAA,UAAAuC,EAAAwF,EAAA,EACA7E,EAAAlD,IAAA,CAAAuC,GACAA,CACA,EAjMAJ,KAoMA,SAAA+B,CAAA,EACA,IAAA3B,EAAAW,EAAAyJ,GAAA,GAGA,OAFApK,EAAA4B,GAAA,CAAAkB,IACA0C,EAAA/D,MAAA,CAAAhE,IAAA,SAAAuC,EAAAwF,EAAA,EACAxF,CACA,EAxMAsC,UAAAuH,EAAAI,EAAA,CACA3H,UAAA,EACA,EACA,EAOAkD,EAAA,CACA/F,KAAA,KACAuB,eAAA,GACAyC,WA8EA,SAAApF,CAAA,EACAsL,CAAA,CAAAtL,EAAA0E,IAAA,EAAA1E,EAAA8H,MAAA,CACA+D,GACA,EAhFAzI,OAAA,GACAqB,IAAAA,EACAxD,OAAAA,EACAH,SAAA,KACAkL,eA4CA,SAAArK,CAAA,CAAAsK,CAAA,EACA,OAAAC,SAuZA3E,CAAA,CAAA0E,CAAA,EACA,IAIAE,EAJA1I,EAAA,GAEA2I,EAAA,GAGA,OAAA3I,EAAA8D,EAAA9E,MAAA,OAGAzC,EAFA,IAAAyH,EAAAF,CAAA,CAAA9D,EAAA,CAGA,oBAAAgE,EACAzH,EAAAyH,OACM,OAAAA,GACN,QAEAzH,EAAA,KACA,KAEA,SAEAA,EAAA,KACA,KAEA,SAEAA,EAAA,OACA,KAEA,SAEAA,EAAAiM,EAAA,QACA,KAEA,SAEA,IAAAA,GAAAE,EAAA,SACAnM,EAAA,IACA,KAEA,SAGAA,EAAAqM,OAAAC,YAAA,CAAA7E,EAEA,CACA0E,EAAA1E,KAAAA,EACA2E,EAAAhN,IAAA,CAAAY,EACA,CACA,OAAAoM,EAAAG,IAAA,IACA,EAxcApH,EAAAxD,GAAAsK,EACA,EA7CA9G,YAAAA,EACAE,MAsBA,SAAAzB,CAAA,QAKA,CAJA2D,EAAa,GAAA7D,EAAA8I,CAAA,EAAIjF,EAAA3D,GACjB6I,WAqEA,IAAAC,EACA,KAAA1J,EAAA6E,MAAA,CAAAN,EAAA9E,MAAA,GACA,IAAAgF,EAAAF,CAAA,CAAAvE,EAAA6E,MAAA,EAGA,oBAAAJ,EAKA,IAJAiF,EAAA1J,EAAA6E,MAAA,CACA7E,EAAA4E,YAAA,IACA5E,CAAAA,EAAA4E,YAAA,IAEA5E,EAAA6E,MAAA,GAAA6E,GAAA1J,EAAA4E,YAAA,CAAAH,EAAAhF,MAAA,EACAkK,KAgBAvL,EAAAA,EAhBAqG,EAAAC,UAAA,CAAA1E,EAAA4E,YAAA,EAmBAgF,EAAAA,EAAAxL,EAnBA,MAmBAwL,EAAAA,EAhBAnF,EAEA,CACA,IAnFAF,OAAAA,CAAA,CAAAA,EAAA9E,MAAA,KACA,IAEAkJ,EAAAN,EAAA,GAGAlE,EAAA/D,MAAA,CAAqB,GAAAyJ,EAAAhE,CAAA,EAAU0C,EAAApE,EAAA/D,MAAA,CAAA+D,GAC/BA,EAAA/D,MAAA,CACA,CAlCA,EAOAwJ,EAAAvB,EAAAzK,QAAA,CAAA6E,IAAA,CAAA0B,EAAAtG,GAWA,OAHAwK,EAAA9E,UAAA,EACAgF,EAAAnM,IAAA,CAAAiM,GAEAlE,EA4BA,SAAAhC,EAAAxD,CAAA,EACA,OAAAmL,SA2WAvF,CAAA,CAAA5F,CAAA,MAMAoL,EALA,IAAAC,EAAArL,EAAAa,KAAA,CAAAqF,MAAA,CACAoF,EAAAtL,EAAAa,KAAA,CAAAoF,YAAA,CACAsF,EAAAvL,EAAA4B,GAAA,CAAAsE,MAAA,CACAsF,EAAAxL,EAAA4B,GAAA,CAAAqE,YAAA,CAGA,GAAAoF,IAAAE,EAEAH,EAAA,CAAAxF,CAAA,CAAAyF,EAAA,CAAApJ,KAAA,CAAAqJ,EAAAE,GAAA,KACI,CAEJ,GADAJ,EAAAxF,EAAA3D,KAAA,CAAAoJ,EAAAE,GACAD,EAAA,IACA,IAAAG,EAAAL,CAAA,IACA,iBAAAK,EACAL,CAAA,IAAAK,EAAAxJ,KAAA,CAAAqJ,GAGAF,EAAAM,KAAA,EAEA,CACAF,EAAA,GAEAJ,EAAA3N,IAAA,CAAAmI,CAAA,CAAA2F,EAAA,CAAAtJ,KAAA,GAAAuJ,GAEA,CACA,OAAAJ,CACA,EAtYAxF,EAAA5F,EACA,CAGA,SAAA8C,IAEA,IACAmD,aAAAA,CAAA,CACAC,OAAAA,CAAA,CACAnD,KAAAA,CAAA,CACAoD,OAAAA,CAAA,CACAnD,OAAAA,CAAA,CACA,CAAM3B,EACN,OACA4E,aAAAA,EACAC,OAAAA,EACAnD,KAAAA,EACAoD,OAAAA,EACAnD,OAAAA,CACA,CACA,CAgIA,SAAAiH,EAAA1C,CAAA,CAAAwC,CAAA,EACAA,EAAA4B,OAAA,EACA,CAUA,SAAA9B,EAAA+B,CAAA,CAAAzB,CAAA,EACA,OAeA,SAAA5K,CAAA,CAAAsM,CAAA,CAAAC,CAAA,EAEA,IAAAC,EAEAC,EAEA7J,EAEA4H,EACA,OAAAkC,MAAAC,OAAA,CAAA3M,GACA4M,EAAA5M,GAAA,aAAAA,EAEA4M,EAAA,CAAqD5M,EAAA,EAcrD,SAAAE,CAAA,EACA,IAAA2M,EAAA3M,OAAAA,GAAA4M,CAAA,CAAA5M,EAAA,CACA6M,EAAA7M,OAAAA,GAAA4M,EAAAnI,IAAA,CAKA,OAAAiI,EAJA,IAGAF,MAAAC,OAAA,CAAAE,GAAAA,EAAAA,EAAA,CAAAA,EAAA,OAAAH,MAAAC,OAAA,CAAAI,GAAAA,EAAAA,EAAA,CAAAA,EAAA,MACA7M,EACA,EAWA,SAAA0M,EAAA7G,CAAA,QAGA,CAFAyG,EAAAzG,EACA0G,EAAA,EACA1G,IAAAA,EAAAxE,MAAA,EACAgL,EAEAS,EAAAjH,CAAA,CAAA0G,EAAA,CACA,CAUA,SAAAO,EAAAzC,CAAA,EACA,OAGA,SAAArK,CAAA,QAaA,CARAsK,EAAAyC,WAiEA,IAAAC,EAAA3J,IACA4J,EAAAlH,EAAArG,QAAA,CACAwN,EAAAnH,EAAArD,gBAAA,CACAyK,EAAApH,EAAA/D,MAAA,CAAAX,MAAA,CACA+L,EAAAZ,MAAAzC,IAAA,CAAA7I,GACA,OACA6I,KAAAoD,EACAjB,QASA,WACAtK,EAAAoL,EACAjH,EAAArG,QAAA,CAAAuN,EACAlH,EAAArD,gBAAA,CAAAwK,EACAnH,EAAA/D,MAAA,CAAAX,MAAA,CAAA8L,EACAjM,EAAAkM,EACA3C,GACA,CAfA,CAgBA,IAxFA/H,EAAA2H,EACAA,EAAAgD,OAAA,EACAtH,CAAAA,EAAArD,gBAAA,CAAA2H,CAAA,EAKAA,EAAAiD,IAAA,EAAAvH,EAAAlG,MAAA,CAAAC,UAAA,CAAA0E,OAAA,CAAAC,IAAA,CAAAC,QAAA,CAAA2F,EAAAiD,IAAA,GACA/I,EAAAvE,GAEAqK,EAAA7K,QAAA,CAAA6E,IAAA,CAIAqG,EAAA/D,OAAAC,MAAA,CAAAD,OAAAkD,MAAA,CAAA9D,GAAA2E,GAAA3E,EAAAtG,EAAA6E,EAAAC,GAAAvE,EACA,CACA,CAGA,SAAAsE,EAAAtE,CAAA,EAGA,OADAmM,EAAAzJ,EAAA4H,GACA8B,CACA,CAGA,SAAA7H,EAAAvE,CAAA,QAGA,CADAsK,EAAA4B,OAAA,GACA,EAAAK,EAAAD,EAAAjL,MAAA,EACAyL,EAAAR,CAAA,CAAAC,EAAA,EAEAF,CACA,CACA,CACA,CAUA,SAAA9B,EAAAF,CAAA,CAAAN,CAAA,EACAM,EAAAlF,UAAA,GAAAgF,EAAAzF,QAAA,CAAA2F,IACAF,EAAAnM,IAAA,CAAAqM,GAEAA,EAAAkD,OAAA,EACM,GAAAjL,EAAAC,CAAA,EAAMwD,EAAA/D,MAAA,CAAA+H,EAAAhE,EAAA/D,MAAA,CAAAX,MAAA,CAAA0I,EAAAM,EAAAkD,OAAA,CAAAxH,EAAA/D,MAAA,CAAAQ,KAAA,CAAAuH,GAAAhE,IAEZsE,EAAAmD,SAAA,EACAzH,CAAAA,EAAA/D,MAAA,CAAAqI,EAAAmD,SAAA,CAAAzH,EAAA/D,MAAA,CAAA+D,EAAA,CAEA,CA0CA,SAAA0E,IACA7I,EAAA0B,IAAA,IAAA4G,GAAAtI,EAAA8E,MAAA,KACA9E,EAAA8E,MAAA,CAAAwD,CAAA,CAAAtI,EAAA0B,IAAA,EACA1B,EAAA2B,MAAA,EAAA2G,CAAA,CAAAtI,EAAA0B,IAAA,IAEA,CACA,EC1c4BzD,EAAA8E,EAAAoF,EAC5B,CACA,CACA,uEC3CO,SAAA0D,EAAAzL,CAAA,EACP,MAAU,GAAA0L,EAAA5F,CAAA,EAAW9F,KAGrB,OAAAA,CACA,yDCAA,IAAA2L,EAAA,cAMO,SAAAC,IACP,IAKAC,EALAnH,EAAA,EACAoH,EAAA,GAEA1M,EAAA,GAGA,OAIA,SAAAxC,CAAA,CAAAmP,CAAA,CAAA5L,CAAA,MAIA6L,EAEAvN,EAEAwN,EAEAC,EAEAlO,EAVA,IAAAmG,EAAA,GAqBA,IAVAvH,EAAAkP,EAAA,kBAAAlP,EAAAA,EAAAuP,QAAA,OAAAC,YAAAL,GAAAlM,KAAAA,GAAAwM,MAAA,CAAAzP,EAAA,EACAqP,EAAA,EACAH,EAAA,GACA1M,IAEA,QAAAxC,EAAA0H,UAAA,KACA2H,IAEA7M,EAAAS,KAAAA,GAEAoM,EAAArP,EAAAyC,MAAA,GAKA,GAJAsM,EAAAW,SAAA,CAAAL,EAEAC,EAAAF,CADAA,EAAAL,EAAAY,IAAA,CAAA3P,EAAA,GACAoP,KAAAnM,IAAAmM,EAAA3L,KAAA,CAAA2L,EAAA3L,KAAA,CAAAzD,EAAAyC,MAAA,CACArB,EAAApB,EAAA0H,UAAA,CAAA4H,GACA,CAAAF,EAAA,CACAF,EAAAlP,EAAA4D,KAAA,CAAAyL,GACA,KACA,CACA,GAAAjO,KAAAA,GAAAiO,IAAAC,GAAAL,EACA1H,EAAAnI,IAAA,KACA6P,EAAAhM,KAAAA,OAUA,OARAgM,IACA1H,EAAAnI,IAAA,KACA6P,EAAAhM,KAAAA,GAEAoM,EAAAC,IACA/H,EAAAnI,IAAA,CAAAY,EAAA4D,KAAA,CAAAyL,EAAAC,IACAxH,GAAAwH,EAAAD,GAEAjO,GACA,OAEAmG,EAAAnI,IAAA,QACA0I,IACA,KAEA,QAIA,IAFAjG,EAAA+N,EAAAA,KAAAC,IAAA,CAAA/H,EAAA,GACAP,EAAAnI,IAAA,KACA0I,IAAAjG,GAAA0F,EAAAnI,IAAA,KACA,KAEA,SAEAmI,EAAAnI,IAAA,KACA0I,EAAA,EACA,KAEA,SAEAmH,EAAA,GACAnH,EAAA,CAEA,CAEAuH,EAAAC,EAAA,CACA,CAMA,OALA/L,IACA0L,GAAA1H,EAAAnI,IAAA,KACA8P,GAAA3H,EAAAnI,IAAA,CAAA8P,GACA3H,EAAAnI,IAAA,QAEAmI,CACA,CACA","sources":["webpack://_N_E/./node_modules/@microsoft/clarity/src/utils.js","webpack://_N_E/./node_modules/@microsoft/clarity/index.js","webpack://_N_E/./node_modules/micromark/lib/initialize/content.js","webpack://_N_E/./node_modules/micromark/lib/initialize/document.js","webpack://_N_E/./node_modules/micromark/lib/initialize/flow.js","webpack://_N_E/./node_modules/micromark/lib/initialize/text.js","webpack://_N_E/./node_modules/micromark/lib/constructs.js","webpack://_N_E/./node_modules/micromark/lib/create-tokenizer.js","webpack://_N_E/./node_modules/micromark/lib/parse.js","webpack://_N_E/./node_modules/micromark/lib/postprocess.js","webpack://_N_E/./node_modules/micromark/lib/preprocess.js"],"sourcesContent":["export function injectScript(projectId){\r\n  try{\r\n      (function (c, l, a, r, i, t, y) {\r\n          if(l.getElementById(\"clarity-script\")){\r\n            return;\r\n          }\r\n          c[a] = c[a] ||\r\n            function () {\r\n              (c[a].q = c[a].q || []).push(arguments);\r\n            };\r\n          t = l.createElement(r);\r\n          t.async = 1;\r\n          t.src = \"https://www.clarity.ms/tag/\" + i + \"?ref=npm\";\r\n          t.id = \"clarity-script\"\r\n          y = l.getElementsByTagName(r)[0];\r\n          y.parentNode.insertBefore(t, y);\r\n        })(window, document, \"clarity\", \"script\", projectId);\r\n      return;\r\n  }catch(error){\r\n      return;\r\n  }\r\n};","import { injectScript } from \"./src/utils\"\n\nconst Clarity = {\n    init(projectId) {\n        injectScript(projectId, 'clarity-script');\n    },\n\n    setTag(key, value) {\n        window.clarity('set', key, value);\n    },\n\n    identify(customerId, customSessionId, customPageId, friendlyName) {\n        window.clarity('identify', customerId, customSessionId, customPageId, friendlyName);\n    },\n\n    consent(consent = true) {\n        window.clarity('consent', consent);\n    },\n\n    upgrade(reason) {\n        window.clarity('upgrade', reason);\n    },\n\n    event(eventName) {\n        window.clarity('event', eventName);\n    },\n};\n\nexport default Clarity;","/**\n * @import {\n *   InitialConstruct,\n *   Initializer,\n *   State,\n *   TokenizeContext,\n *   Token\n * } from 'micromark-util-types'\n */\n\nimport { factorySpace } from 'micromark-factory-space';\nimport { markdownLineEnding } from 'micromark-util-character';\n/** @type {InitialConstruct} */\nexport const content = {\n  tokenize: initializeContent\n};\n\n/**\n * @this {TokenizeContext}\n *   Context.\n * @type {Initializer}\n *   Content.\n */\nfunction initializeContent(effects) {\n  const contentStart = effects.attempt(this.parser.constructs.contentInitial, afterContentStartConstruct, paragraphInitial);\n  /** @type {Token} */\n  let previous;\n  return contentStart;\n\n  /** @type {State} */\n  function afterContentStartConstruct(code) {\n    if (code === null) {\n      effects.consume(code);\n      return;\n    }\n    effects.enter(\"lineEnding\");\n    effects.consume(code);\n    effects.exit(\"lineEnding\");\n    return factorySpace(effects, contentStart, \"linePrefix\");\n  }\n\n  /** @type {State} */\n  function paragraphInitial(code) {\n    effects.enter(\"paragraph\");\n    return lineStart(code);\n  }\n\n  /** @type {State} */\n  function lineStart(code) {\n    const token = effects.enter(\"chunkText\", {\n      contentType: \"text\",\n      previous\n    });\n    if (previous) {\n      previous.next = token;\n    }\n    previous = token;\n    return data(code);\n  }\n\n  /** @type {State} */\n  function data(code) {\n    if (code === null) {\n      effects.exit(\"chunkText\");\n      effects.exit(\"paragraph\");\n      effects.consume(code);\n      return;\n    }\n    if (markdownLineEnding(code)) {\n      effects.consume(code);\n      effects.exit(\"chunkText\");\n      return lineStart;\n    }\n\n    // Data.\n    effects.consume(code);\n    return data;\n  }\n}","/**\n * @import {\n *   Construct,\n *   ContainerState,\n *   InitialConstruct,\n *   Initializer,\n *   Point,\n *   State,\n *   TokenizeContext,\n *   Tokenizer,\n *   Token\n * } from 'micromark-util-types'\n */\n\n/**\n * @typedef {[Construct, ContainerState]} StackItem\n *   Construct and its state.\n */\n\nimport { factorySpace } from 'micromark-factory-space';\nimport { markdownLineEnding } from 'micromark-util-character';\nimport { splice } from 'micromark-util-chunked';\n/** @type {InitialConstruct} */\nexport const document = {\n  tokenize: initializeDocument\n};\n\n/** @type {Construct} */\nconst containerConstruct = {\n  tokenize: tokenizeContainer\n};\n\n/**\n * @this {TokenizeContext}\n *   Self.\n * @type {Initializer}\n *   Initializer.\n */\nfunction initializeDocument(effects) {\n  const self = this;\n  /** @type {Array<StackItem>} */\n  const stack = [];\n  let continued = 0;\n  /** @type {TokenizeContext | undefined} */\n  let childFlow;\n  /** @type {Token | undefined} */\n  let childToken;\n  /** @type {number} */\n  let lineStartOffset;\n  return start;\n\n  /** @type {State} */\n  function start(code) {\n    // First we iterate through the open blocks, starting with the root\n    // document, and descending through last children down to the last open\n    // block.\n    // Each block imposes a condition that the line must satisfy if the block is\n    // to remain open.\n    // For example, a block quote requires a `>` character.\n    // A paragraph requires a non-blank line.\n    // In this phase we may match all or just some of the open blocks.\n    // But we cannot close unmatched blocks yet, because we may have a lazy\n    // continuation line.\n    if (continued < stack.length) {\n      const item = stack[continued];\n      self.containerState = item[1];\n      return effects.attempt(item[0].continuation, documentContinue, checkNewContainers)(code);\n    }\n\n    // Done.\n    return checkNewContainers(code);\n  }\n\n  /** @type {State} */\n  function documentContinue(code) {\n    continued++;\n\n    // Note: this field is called `_closeFlow` but it also closes containers.\n    // Perhaps a good idea to rename it but it’s already used in the wild by\n    // extensions.\n    if (self.containerState._closeFlow) {\n      self.containerState._closeFlow = undefined;\n      if (childFlow) {\n        closeFlow();\n      }\n\n      // Note: this algorithm for moving events around is similar to the\n      // algorithm when dealing with lazy lines in `writeToChild`.\n      const indexBeforeExits = self.events.length;\n      let indexBeforeFlow = indexBeforeExits;\n      /** @type {Point | undefined} */\n      let point;\n\n      // Find the flow chunk.\n      while (indexBeforeFlow--) {\n        if (self.events[indexBeforeFlow][0] === 'exit' && self.events[indexBeforeFlow][1].type === \"chunkFlow\") {\n          point = self.events[indexBeforeFlow][1].end;\n          break;\n        }\n      }\n      exitContainers(continued);\n\n      // Fix positions.\n      let index = indexBeforeExits;\n      while (index < self.events.length) {\n        self.events[index][1].end = {\n          ...point\n        };\n        index++;\n      }\n\n      // Inject the exits earlier (they’re still also at the end).\n      splice(self.events, indexBeforeFlow + 1, 0, self.events.slice(indexBeforeExits));\n\n      // Discard the duplicate exits.\n      self.events.length = index;\n      return checkNewContainers(code);\n    }\n    return start(code);\n  }\n\n  /** @type {State} */\n  function checkNewContainers(code) {\n    // Next, after consuming the continuation markers for existing blocks, we\n    // look for new block starts (e.g. `>` for a block quote).\n    // If we encounter a new block start, we close any blocks unmatched in\n    // step 1 before creating the new block as a child of the last matched\n    // block.\n    if (continued === stack.length) {\n      // No need to `check` whether there’s a container, of `exitContainers`\n      // would be moot.\n      // We can instead immediately `attempt` to parse one.\n      if (!childFlow) {\n        return documentContinued(code);\n      }\n\n      // If we have concrete content, such as block HTML or fenced code,\n      // we can’t have containers “pierce” into them, so we can immediately\n      // start.\n      if (childFlow.currentConstruct && childFlow.currentConstruct.concrete) {\n        return flowStart(code);\n      }\n\n      // If we do have flow, it could still be a blank line,\n      // but we’d be interrupting it w/ a new container if there’s a current\n      // construct.\n      // To do: next major: remove `_gfmTableDynamicInterruptHack` (no longer\n      // needed in micromark-extension-gfm-table@1.0.6).\n      self.interrupt = Boolean(childFlow.currentConstruct && !childFlow._gfmTableDynamicInterruptHack);\n    }\n\n    // Check if there is a new container.\n    self.containerState = {};\n    return effects.check(containerConstruct, thereIsANewContainer, thereIsNoNewContainer)(code);\n  }\n\n  /** @type {State} */\n  function thereIsANewContainer(code) {\n    if (childFlow) closeFlow();\n    exitContainers(continued);\n    return documentContinued(code);\n  }\n\n  /** @type {State} */\n  function thereIsNoNewContainer(code) {\n    self.parser.lazy[self.now().line] = continued !== stack.length;\n    lineStartOffset = self.now().offset;\n    return flowStart(code);\n  }\n\n  /** @type {State} */\n  function documentContinued(code) {\n    // Try new containers.\n    self.containerState = {};\n    return effects.attempt(containerConstruct, containerContinue, flowStart)(code);\n  }\n\n  /** @type {State} */\n  function containerContinue(code) {\n    continued++;\n    stack.push([self.currentConstruct, self.containerState]);\n    // Try another.\n    return documentContinued(code);\n  }\n\n  /** @type {State} */\n  function flowStart(code) {\n    if (code === null) {\n      if (childFlow) closeFlow();\n      exitContainers(0);\n      effects.consume(code);\n      return;\n    }\n    childFlow = childFlow || self.parser.flow(self.now());\n    effects.enter(\"chunkFlow\", {\n      _tokenizer: childFlow,\n      contentType: \"flow\",\n      previous: childToken\n    });\n    return flowContinue(code);\n  }\n\n  /** @type {State} */\n  function flowContinue(code) {\n    if (code === null) {\n      writeToChild(effects.exit(\"chunkFlow\"), true);\n      exitContainers(0);\n      effects.consume(code);\n      return;\n    }\n    if (markdownLineEnding(code)) {\n      effects.consume(code);\n      writeToChild(effects.exit(\"chunkFlow\"));\n      // Get ready for the next line.\n      continued = 0;\n      self.interrupt = undefined;\n      return start;\n    }\n    effects.consume(code);\n    return flowContinue;\n  }\n\n  /**\n   * @param {Token} token\n   *   Token.\n   * @param {boolean | undefined} [endOfFile]\n   *   Whether the token is at the end of the file (default: `false`).\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function writeToChild(token, endOfFile) {\n    const stream = self.sliceStream(token);\n    if (endOfFile) stream.push(null);\n    token.previous = childToken;\n    if (childToken) childToken.next = token;\n    childToken = token;\n    childFlow.defineSkip(token.start);\n    childFlow.write(stream);\n\n    // Alright, so we just added a lazy line:\n    //\n    // ```markdown\n    // > a\n    // b.\n    //\n    // Or:\n    //\n    // > ~~~c\n    // d\n    //\n    // Or:\n    //\n    // > | e |\n    // f\n    // ```\n    //\n    // The construct in the second example (fenced code) does not accept lazy\n    // lines, so it marked itself as done at the end of its first line, and\n    // then the content construct parses `d`.\n    // Most constructs in markdown match on the first line: if the first line\n    // forms a construct, a non-lazy line can’t “unmake” it.\n    //\n    // The construct in the third example is potentially a GFM table, and\n    // those are *weird*.\n    // It *could* be a table, from the first line, if the following line\n    // matches a condition.\n    // In this case, that second line is lazy, which “unmakes” the first line\n    // and turns the whole into one content block.\n    //\n    // We’ve now parsed the non-lazy and the lazy line, and can figure out\n    // whether the lazy line started a new flow block.\n    // If it did, we exit the current containers between the two flow blocks.\n    if (self.parser.lazy[token.start.line]) {\n      let index = childFlow.events.length;\n      while (index--) {\n        if (\n        // The token starts before the line ending…\n        childFlow.events[index][1].start.offset < lineStartOffset && (\n        // …and either is not ended yet…\n        !childFlow.events[index][1].end ||\n        // …or ends after it.\n        childFlow.events[index][1].end.offset > lineStartOffset)) {\n          // Exit: there’s still something open, which means it’s a lazy line\n          // part of something.\n          return;\n        }\n      }\n\n      // Note: this algorithm for moving events around is similar to the\n      // algorithm when closing flow in `documentContinue`.\n      const indexBeforeExits = self.events.length;\n      let indexBeforeFlow = indexBeforeExits;\n      /** @type {boolean | undefined} */\n      let seen;\n      /** @type {Point | undefined} */\n      let point;\n\n      // Find the previous chunk (the one before the lazy line).\n      while (indexBeforeFlow--) {\n        if (self.events[indexBeforeFlow][0] === 'exit' && self.events[indexBeforeFlow][1].type === \"chunkFlow\") {\n          if (seen) {\n            point = self.events[indexBeforeFlow][1].end;\n            break;\n          }\n          seen = true;\n        }\n      }\n      exitContainers(continued);\n\n      // Fix positions.\n      index = indexBeforeExits;\n      while (index < self.events.length) {\n        self.events[index][1].end = {\n          ...point\n        };\n        index++;\n      }\n\n      // Inject the exits earlier (they’re still also at the end).\n      splice(self.events, indexBeforeFlow + 1, 0, self.events.slice(indexBeforeExits));\n\n      // Discard the duplicate exits.\n      self.events.length = index;\n    }\n  }\n\n  /**\n   * @param {number} size\n   *   Size.\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function exitContainers(size) {\n    let index = stack.length;\n\n    // Exit open containers.\n    while (index-- > size) {\n      const entry = stack[index];\n      self.containerState = entry[1];\n      entry[0].exit.call(self, effects);\n    }\n    stack.length = size;\n  }\n  function closeFlow() {\n    childFlow.write([null]);\n    childToken = undefined;\n    childFlow = undefined;\n    self.containerState._closeFlow = undefined;\n  }\n}\n\n/**\n * @this {TokenizeContext}\n *   Context.\n * @type {Tokenizer}\n *   Tokenizer.\n */\nfunction tokenizeContainer(effects, ok, nok) {\n  // Always populated by defaults.\n\n  return factorySpace(effects, effects.attempt(this.parser.constructs.document, ok, nok), \"linePrefix\", this.parser.constructs.disable.null.includes('codeIndented') ? undefined : 4);\n}","/**\n * @import {\n *   InitialConstruct,\n *   Initializer,\n *   State,\n *   TokenizeContext\n * } from 'micromark-util-types'\n */\n\nimport { blankLine, content } from 'micromark-core-commonmark';\nimport { factorySpace } from 'micromark-factory-space';\nimport { markdownLineEnding } from 'micromark-util-character';\n/** @type {InitialConstruct} */\nexport const flow = {\n  tokenize: initializeFlow\n};\n\n/**\n * @this {TokenizeContext}\n *   Self.\n * @type {Initializer}\n *   Initializer.\n */\nfunction initializeFlow(effects) {\n  const self = this;\n  const initial = effects.attempt(\n  // Try to parse a blank line.\n  blankLine, atBlankEnding,\n  // Try to parse initial flow (essentially, only code).\n  effects.attempt(this.parser.constructs.flowInitial, afterConstruct, factorySpace(effects, effects.attempt(this.parser.constructs.flow, afterConstruct, effects.attempt(content, afterConstruct)), \"linePrefix\")));\n  return initial;\n\n  /** @type {State} */\n  function atBlankEnding(code) {\n    if (code === null) {\n      effects.consume(code);\n      return;\n    }\n    effects.enter(\"lineEndingBlank\");\n    effects.consume(code);\n    effects.exit(\"lineEndingBlank\");\n    self.currentConstruct = undefined;\n    return initial;\n  }\n\n  /** @type {State} */\n  function afterConstruct(code) {\n    if (code === null) {\n      effects.consume(code);\n      return;\n    }\n    effects.enter(\"lineEnding\");\n    effects.consume(code);\n    effects.exit(\"lineEnding\");\n    self.currentConstruct = undefined;\n    return initial;\n  }\n}","/**\n * @import {\n *   Code,\n *   InitialConstruct,\n *   Initializer,\n *   Resolver,\n *   State,\n *   TokenizeContext\n * } from 'micromark-util-types'\n */\n\nexport const resolver = {\n  resolveAll: createResolver()\n};\nexport const string = initializeFactory('string');\nexport const text = initializeFactory('text');\n\n/**\n * @param {'string' | 'text'} field\n *   Field.\n * @returns {InitialConstruct}\n *   Construct.\n */\nfunction initializeFactory(field) {\n  return {\n    resolveAll: createResolver(field === 'text' ? resolveAllLineSuffixes : undefined),\n    tokenize: initializeText\n  };\n\n  /**\n   * @this {TokenizeContext}\n   *   Context.\n   * @type {Initializer}\n   */\n  function initializeText(effects) {\n    const self = this;\n    const constructs = this.parser.constructs[field];\n    const text = effects.attempt(constructs, start, notText);\n    return start;\n\n    /** @type {State} */\n    function start(code) {\n      return atBreak(code) ? text(code) : notText(code);\n    }\n\n    /** @type {State} */\n    function notText(code) {\n      if (code === null) {\n        effects.consume(code);\n        return;\n      }\n      effects.enter(\"data\");\n      effects.consume(code);\n      return data;\n    }\n\n    /** @type {State} */\n    function data(code) {\n      if (atBreak(code)) {\n        effects.exit(\"data\");\n        return text(code);\n      }\n\n      // Data.\n      effects.consume(code);\n      return data;\n    }\n\n    /**\n     * @param {Code} code\n     *   Code.\n     * @returns {boolean}\n     *   Whether the code is a break.\n     */\n    function atBreak(code) {\n      if (code === null) {\n        return true;\n      }\n      const list = constructs[code];\n      let index = -1;\n      if (list) {\n        // Always populated by defaults.\n\n        while (++index < list.length) {\n          const item = list[index];\n          if (!item.previous || item.previous.call(self, self.previous)) {\n            return true;\n          }\n        }\n      }\n      return false;\n    }\n  }\n}\n\n/**\n * @param {Resolver | undefined} [extraResolver]\n *   Resolver.\n * @returns {Resolver}\n *   Resolver.\n */\nfunction createResolver(extraResolver) {\n  return resolveAllText;\n\n  /** @type {Resolver} */\n  function resolveAllText(events, context) {\n    let index = -1;\n    /** @type {number | undefined} */\n    let enter;\n\n    // A rather boring computation (to merge adjacent `data` events) which\n    // improves mm performance by 29%.\n    while (++index <= events.length) {\n      if (enter === undefined) {\n        if (events[index] && events[index][1].type === \"data\") {\n          enter = index;\n          index++;\n        }\n      } else if (!events[index] || events[index][1].type !== \"data\") {\n        // Don’t do anything if there is one data token.\n        if (index !== enter + 2) {\n          events[enter][1].end = events[index - 1][1].end;\n          events.splice(enter + 2, index - enter - 2);\n          index = enter + 2;\n        }\n        enter = undefined;\n      }\n    }\n    return extraResolver ? extraResolver(events, context) : events;\n  }\n}\n\n/**\n * A rather ugly set of instructions which again looks at chunks in the input\n * stream.\n * The reason to do this here is that it is *much* faster to parse in reverse.\n * And that we can’t hook into `null` to split the line suffix before an EOF.\n * To do: figure out if we can make this into a clean utility, or even in core.\n * As it will be useful for GFMs literal autolink extension (and maybe even\n * tables?)\n *\n * @type {Resolver}\n */\nfunction resolveAllLineSuffixes(events, context) {\n  let eventIndex = 0; // Skip first.\n\n  while (++eventIndex <= events.length) {\n    if ((eventIndex === events.length || events[eventIndex][1].type === \"lineEnding\") && events[eventIndex - 1][1].type === \"data\") {\n      const data = events[eventIndex - 1][1];\n      const chunks = context.sliceStream(data);\n      let index = chunks.length;\n      let bufferIndex = -1;\n      let size = 0;\n      /** @type {boolean | undefined} */\n      let tabs;\n      while (index--) {\n        const chunk = chunks[index];\n        if (typeof chunk === 'string') {\n          bufferIndex = chunk.length;\n          while (chunk.charCodeAt(bufferIndex - 1) === 32) {\n            size++;\n            bufferIndex--;\n          }\n          if (bufferIndex) break;\n          bufferIndex = -1;\n        }\n        // Number\n        else if (chunk === -2) {\n          tabs = true;\n          size++;\n        } else if (chunk === -1) {\n          // Empty\n        } else {\n          // Replacement character, exit.\n          index++;\n          break;\n        }\n      }\n\n      // Allow final trailing whitespace.\n      if (context._contentTypeTextTrailing && eventIndex === events.length) {\n        size = 0;\n      }\n      if (size) {\n        const token = {\n          type: eventIndex === events.length || tabs || size < 2 ? \"lineSuffix\" : \"hardBreakTrailing\",\n          start: {\n            _bufferIndex: index ? bufferIndex : data.start._bufferIndex + bufferIndex,\n            _index: data.start._index + index,\n            line: data.end.line,\n            column: data.end.column - size,\n            offset: data.end.offset - size\n          },\n          end: {\n            ...data.end\n          }\n        };\n        data.end = {\n          ...token.start\n        };\n        if (data.start.offset === data.end.offset) {\n          Object.assign(data, token);\n        } else {\n          events.splice(eventIndex, 0, ['enter', token, context], ['exit', token, context]);\n          eventIndex += 2;\n        }\n      }\n      eventIndex++;\n    }\n  }\n  return events;\n}","/**\n * @import {Extension} from 'micromark-util-types'\n */\n\nimport { attention, autolink, blockQuote, characterEscape, characterReference, codeFenced, codeIndented, codeText, definition, hardBreakEscape, headingAtx, htmlFlow, htmlText, labelEnd, labelStartImage, labelStartLink, lineEnding, list, setextUnderline, thematicBreak } from 'micromark-core-commonmark';\nimport { resolver as resolveText } from './initialize/text.js';\n\n/** @satisfies {Extension['document']} */\nexport const document = {\n  [42]: list,\n  [43]: list,\n  [45]: list,\n  [48]: list,\n  [49]: list,\n  [50]: list,\n  [51]: list,\n  [52]: list,\n  [53]: list,\n  [54]: list,\n  [55]: list,\n  [56]: list,\n  [57]: list,\n  [62]: blockQuote\n};\n\n/** @satisfies {Extension['contentInitial']} */\nexport const contentInitial = {\n  [91]: definition\n};\n\n/** @satisfies {Extension['flowInitial']} */\nexport const flowInitial = {\n  [-2]: codeIndented,\n  [-1]: codeIndented,\n  [32]: codeIndented\n};\n\n/** @satisfies {Extension['flow']} */\nexport const flow = {\n  [35]: headingAtx,\n  [42]: thematicBreak,\n  [45]: [setextUnderline, thematicBreak],\n  [60]: htmlFlow,\n  [61]: setextUnderline,\n  [95]: thematicBreak,\n  [96]: codeFenced,\n  [126]: codeFenced\n};\n\n/** @satisfies {Extension['string']} */\nexport const string = {\n  [38]: characterReference,\n  [92]: characterEscape\n};\n\n/** @satisfies {Extension['text']} */\nexport const text = {\n  [-5]: lineEnding,\n  [-4]: lineEnding,\n  [-3]: lineEnding,\n  [33]: labelStartImage,\n  [38]: characterReference,\n  [42]: attention,\n  [60]: [autolink, htmlText],\n  [91]: labelStartLink,\n  [92]: [hardBreakEscape, characterEscape],\n  [93]: labelEnd,\n  [95]: attention,\n  [96]: codeText\n};\n\n/** @satisfies {Extension['insideSpan']} */\nexport const insideSpan = {\n  null: [attention, resolveText]\n};\n\n/** @satisfies {Extension['attentionMarkers']} */\nexport const attentionMarkers = {\n  null: [42, 95]\n};\n\n/** @satisfies {Extension['disable']} */\nexport const disable = {\n  null: []\n};","/**\n * @import {\n *   Chunk,\n *   Code,\n *   ConstructRecord,\n *   Construct,\n *   Effects,\n *   InitialConstruct,\n *   ParseContext,\n *   Point,\n *   State,\n *   TokenizeContext,\n *   Token\n * } from 'micromark-util-types'\n */\n\n/**\n * @callback Restore\n *   Restore the state.\n * @returns {undefined}\n *   Nothing.\n *\n * @typedef Info\n *   Info.\n * @property {Restore} restore\n *   Restore.\n * @property {number} from\n *   From.\n *\n * @callback ReturnHandle\n *   Handle a successful run.\n * @param {Construct} construct\n *   Construct.\n * @param {Info} info\n *   Info.\n * @returns {undefined}\n *   Nothing.\n */\n\nimport { markdownLineEnding } from 'micromark-util-character';\nimport { push, splice } from 'micromark-util-chunked';\nimport { resolveAll } from 'micromark-util-resolve-all';\n/**\n * Create a tokenizer.\n * Tokenizers deal with one type of data (e.g., containers, flow, text).\n * The parser is the object dealing with it all.\n * `initialize` works like other constructs, except that only its `tokenize`\n * function is used, in which case it doesn’t receive an `ok` or `nok`.\n * `from` can be given to set the point before the first character, although\n * when further lines are indented, they must be set with `defineSkip`.\n *\n * @param {ParseContext} parser\n *   Parser.\n * @param {InitialConstruct} initialize\n *   Construct.\n * @param {Omit<Point, '_bufferIndex' | '_index'> | undefined} [from]\n *   Point (optional).\n * @returns {TokenizeContext}\n *   Context.\n */\nexport function createTokenizer(parser, initialize, from) {\n  /** @type {Point} */\n  let point = {\n    _bufferIndex: -1,\n    _index: 0,\n    line: from && from.line || 1,\n    column: from && from.column || 1,\n    offset: from && from.offset || 0\n  };\n  /** @type {Record<string, number>} */\n  const columnStart = {};\n  /** @type {Array<Construct>} */\n  const resolveAllConstructs = [];\n  /** @type {Array<Chunk>} */\n  let chunks = [];\n  /** @type {Array<Token>} */\n  let stack = [];\n  /** @type {boolean | undefined} */\n  let consumed = true;\n\n  /**\n   * Tools used for tokenizing.\n   *\n   * @type {Effects}\n   */\n  const effects = {\n    attempt: constructFactory(onsuccessfulconstruct),\n    check: constructFactory(onsuccessfulcheck),\n    consume,\n    enter,\n    exit,\n    interrupt: constructFactory(onsuccessfulcheck, {\n      interrupt: true\n    })\n  };\n\n  /**\n   * State and tools for resolving and serializing.\n   *\n   * @type {TokenizeContext}\n   */\n  const context = {\n    code: null,\n    containerState: {},\n    defineSkip,\n    events: [],\n    now,\n    parser,\n    previous: null,\n    sliceSerialize,\n    sliceStream,\n    write\n  };\n\n  /**\n   * The state function.\n   *\n   * @type {State | undefined}\n   */\n  let state = initialize.tokenize.call(context, effects);\n\n  /**\n   * Track which character we expect to be consumed, to catch bugs.\n   *\n   * @type {Code}\n   */\n  let expectedCode;\n  if (initialize.resolveAll) {\n    resolveAllConstructs.push(initialize);\n  }\n  return context;\n\n  /** @type {TokenizeContext['write']} */\n  function write(slice) {\n    chunks = push(chunks, slice);\n    main();\n\n    // Exit if we’re not done, resolve might change stuff.\n    if (chunks[chunks.length - 1] !== null) {\n      return [];\n    }\n    addResult(initialize, 0);\n\n    // Otherwise, resolve, and exit.\n    context.events = resolveAll(resolveAllConstructs, context.events, context);\n    return context.events;\n  }\n\n  //\n  // Tools.\n  //\n\n  /** @type {TokenizeContext['sliceSerialize']} */\n  function sliceSerialize(token, expandTabs) {\n    return serializeChunks(sliceStream(token), expandTabs);\n  }\n\n  /** @type {TokenizeContext['sliceStream']} */\n  function sliceStream(token) {\n    return sliceChunks(chunks, token);\n  }\n\n  /** @type {TokenizeContext['now']} */\n  function now() {\n    // This is a hot path, so we clone manually instead of `Object.assign({}, point)`\n    const {\n      _bufferIndex,\n      _index,\n      line,\n      column,\n      offset\n    } = point;\n    return {\n      _bufferIndex,\n      _index,\n      line,\n      column,\n      offset\n    };\n  }\n\n  /** @type {TokenizeContext['defineSkip']} */\n  function defineSkip(value) {\n    columnStart[value.line] = value.column;\n    accountForPotentialSkip();\n  }\n\n  //\n  // State management.\n  //\n\n  /**\n   * Main loop (note that `_index` and `_bufferIndex` in `point` are modified by\n   * `consume`).\n   * Here is where we walk through the chunks, which either include strings of\n   * several characters, or numerical character codes.\n   * The reason to do this in a loop instead of a call is so the stack can\n   * drain.\n   *\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function main() {\n    /** @type {number} */\n    let chunkIndex;\n    while (point._index < chunks.length) {\n      const chunk = chunks[point._index];\n\n      // If we’re in a buffer chunk, loop through it.\n      if (typeof chunk === 'string') {\n        chunkIndex = point._index;\n        if (point._bufferIndex < 0) {\n          point._bufferIndex = 0;\n        }\n        while (point._index === chunkIndex && point._bufferIndex < chunk.length) {\n          go(chunk.charCodeAt(point._bufferIndex));\n        }\n      } else {\n        go(chunk);\n      }\n    }\n  }\n\n  /**\n   * Deal with one code.\n   *\n   * @param {Code} code\n   *   Code.\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function go(code) {\n    consumed = undefined;\n    expectedCode = code;\n    state = state(code);\n  }\n\n  /** @type {Effects['consume']} */\n  function consume(code) {\n    if (markdownLineEnding(code)) {\n      point.line++;\n      point.column = 1;\n      point.offset += code === -3 ? 2 : 1;\n      accountForPotentialSkip();\n    } else if (code !== -1) {\n      point.column++;\n      point.offset++;\n    }\n\n    // Not in a string chunk.\n    if (point._bufferIndex < 0) {\n      point._index++;\n    } else {\n      point._bufferIndex++;\n\n      // At end of string chunk.\n      if (point._bufferIndex ===\n      // Points w/ non-negative `_bufferIndex` reference\n      // strings.\n      /** @type {string} */\n      chunks[point._index].length) {\n        point._bufferIndex = -1;\n        point._index++;\n      }\n    }\n\n    // Expose the previous character.\n    context.previous = code;\n\n    // Mark as consumed.\n    consumed = true;\n  }\n\n  /** @type {Effects['enter']} */\n  function enter(type, fields) {\n    /** @type {Token} */\n    // @ts-expect-error Patch instead of assign required fields to help GC.\n    const token = fields || {};\n    token.type = type;\n    token.start = now();\n    context.events.push(['enter', token, context]);\n    stack.push(token);\n    return token;\n  }\n\n  /** @type {Effects['exit']} */\n  function exit(type) {\n    const token = stack.pop();\n    token.end = now();\n    context.events.push(['exit', token, context]);\n    return token;\n  }\n\n  /**\n   * Use results.\n   *\n   * @type {ReturnHandle}\n   */\n  function onsuccessfulconstruct(construct, info) {\n    addResult(construct, info.from);\n  }\n\n  /**\n   * Discard results.\n   *\n   * @type {ReturnHandle}\n   */\n  function onsuccessfulcheck(_, info) {\n    info.restore();\n  }\n\n  /**\n   * Factory to attempt/check/interrupt.\n   *\n   * @param {ReturnHandle} onreturn\n   *   Callback.\n   * @param {{interrupt?: boolean | undefined} | undefined} [fields]\n   *   Fields.\n   */\n  function constructFactory(onreturn, fields) {\n    return hook;\n\n    /**\n     * Handle either an object mapping codes to constructs, a list of\n     * constructs, or a single construct.\n     *\n     * @param {Array<Construct> | ConstructRecord | Construct} constructs\n     *   Constructs.\n     * @param {State} returnState\n     *   State.\n     * @param {State | undefined} [bogusState]\n     *   State.\n     * @returns {State}\n     *   State.\n     */\n    function hook(constructs, returnState, bogusState) {\n      /** @type {ReadonlyArray<Construct>} */\n      let listOfConstructs;\n      /** @type {number} */\n      let constructIndex;\n      /** @type {Construct} */\n      let currentConstruct;\n      /** @type {Info} */\n      let info;\n      return Array.isArray(constructs) ? /* c8 ignore next 1 */\n      handleListOfConstructs(constructs) : 'tokenize' in constructs ?\n      // Looks like a construct.\n      handleListOfConstructs([(/** @type {Construct} */constructs)]) : handleMapOfConstructs(constructs);\n\n      /**\n       * Handle a list of construct.\n       *\n       * @param {ConstructRecord} map\n       *   Constructs.\n       * @returns {State}\n       *   State.\n       */\n      function handleMapOfConstructs(map) {\n        return start;\n\n        /** @type {State} */\n        function start(code) {\n          const left = code !== null && map[code];\n          const all = code !== null && map.null;\n          const list = [\n          // To do: add more extension tests.\n          /* c8 ignore next 2 */\n          ...(Array.isArray(left) ? left : left ? [left] : []), ...(Array.isArray(all) ? all : all ? [all] : [])];\n          return handleListOfConstructs(list)(code);\n        }\n      }\n\n      /**\n       * Handle a list of construct.\n       *\n       * @param {ReadonlyArray<Construct>} list\n       *   Constructs.\n       * @returns {State}\n       *   State.\n       */\n      function handleListOfConstructs(list) {\n        listOfConstructs = list;\n        constructIndex = 0;\n        if (list.length === 0) {\n          return bogusState;\n        }\n        return handleConstruct(list[constructIndex]);\n      }\n\n      /**\n       * Handle a single construct.\n       *\n       * @param {Construct} construct\n       *   Construct.\n       * @returns {State}\n       *   State.\n       */\n      function handleConstruct(construct) {\n        return start;\n\n        /** @type {State} */\n        function start(code) {\n          // To do: not needed to store if there is no bogus state, probably?\n          // Currently doesn’t work because `inspect` in document does a check\n          // w/o a bogus, which doesn’t make sense. But it does seem to help perf\n          // by not storing.\n          info = store();\n          currentConstruct = construct;\n          if (!construct.partial) {\n            context.currentConstruct = construct;\n          }\n\n          // Always populated by defaults.\n\n          if (construct.name && context.parser.constructs.disable.null.includes(construct.name)) {\n            return nok(code);\n          }\n          return construct.tokenize.call(\n          // If we do have fields, create an object w/ `context` as its\n          // prototype.\n          // This allows a “live binding”, which is needed for `interrupt`.\n          fields ? Object.assign(Object.create(context), fields) : context, effects, ok, nok)(code);\n        }\n      }\n\n      /** @type {State} */\n      function ok(code) {\n        consumed = true;\n        onreturn(currentConstruct, info);\n        return returnState;\n      }\n\n      /** @type {State} */\n      function nok(code) {\n        consumed = true;\n        info.restore();\n        if (++constructIndex < listOfConstructs.length) {\n          return handleConstruct(listOfConstructs[constructIndex]);\n        }\n        return bogusState;\n      }\n    }\n  }\n\n  /**\n   * @param {Construct} construct\n   *   Construct.\n   * @param {number} from\n   *   From.\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function addResult(construct, from) {\n    if (construct.resolveAll && !resolveAllConstructs.includes(construct)) {\n      resolveAllConstructs.push(construct);\n    }\n    if (construct.resolve) {\n      splice(context.events, from, context.events.length - from, construct.resolve(context.events.slice(from), context));\n    }\n    if (construct.resolveTo) {\n      context.events = construct.resolveTo(context.events, context);\n    }\n  }\n\n  /**\n   * Store state.\n   *\n   * @returns {Info}\n   *   Info.\n   */\n  function store() {\n    const startPoint = now();\n    const startPrevious = context.previous;\n    const startCurrentConstruct = context.currentConstruct;\n    const startEventsIndex = context.events.length;\n    const startStack = Array.from(stack);\n    return {\n      from: startEventsIndex,\n      restore\n    };\n\n    /**\n     * Restore state.\n     *\n     * @returns {undefined}\n     *   Nothing.\n     */\n    function restore() {\n      point = startPoint;\n      context.previous = startPrevious;\n      context.currentConstruct = startCurrentConstruct;\n      context.events.length = startEventsIndex;\n      stack = startStack;\n      accountForPotentialSkip();\n    }\n  }\n\n  /**\n   * Move the current point a bit forward in the line when it’s on a column\n   * skip.\n   *\n   * @returns {undefined}\n   *   Nothing.\n   */\n  function accountForPotentialSkip() {\n    if (point.line in columnStart && point.column < 2) {\n      point.column = columnStart[point.line];\n      point.offset += columnStart[point.line] - 1;\n    }\n  }\n}\n\n/**\n * Get the chunks from a slice of chunks in the range of a token.\n *\n * @param {ReadonlyArray<Chunk>} chunks\n *   Chunks.\n * @param {Pick<Token, 'end' | 'start'>} token\n *   Token.\n * @returns {Array<Chunk>}\n *   Chunks.\n */\nfunction sliceChunks(chunks, token) {\n  const startIndex = token.start._index;\n  const startBufferIndex = token.start._bufferIndex;\n  const endIndex = token.end._index;\n  const endBufferIndex = token.end._bufferIndex;\n  /** @type {Array<Chunk>} */\n  let view;\n  if (startIndex === endIndex) {\n    // @ts-expect-error `_bufferIndex` is used on string chunks.\n    view = [chunks[startIndex].slice(startBufferIndex, endBufferIndex)];\n  } else {\n    view = chunks.slice(startIndex, endIndex);\n    if (startBufferIndex > -1) {\n      const head = view[0];\n      if (typeof head === 'string') {\n        view[0] = head.slice(startBufferIndex);\n        /* c8 ignore next 4 -- used to be used, no longer */\n      } else {\n        view.shift();\n      }\n    }\n    if (endBufferIndex > 0) {\n      // @ts-expect-error `_bufferIndex` is used on string chunks.\n      view.push(chunks[endIndex].slice(0, endBufferIndex));\n    }\n  }\n  return view;\n}\n\n/**\n * Get the string value of a slice of chunks.\n *\n * @param {ReadonlyArray<Chunk>} chunks\n *   Chunks.\n * @param {boolean | undefined} [expandTabs=false]\n *   Whether to expand tabs (default: `false`).\n * @returns {string}\n *   Result.\n */\nfunction serializeChunks(chunks, expandTabs) {\n  let index = -1;\n  /** @type {Array<string>} */\n  const result = [];\n  /** @type {boolean | undefined} */\n  let atTab;\n  while (++index < chunks.length) {\n    const chunk = chunks[index];\n    /** @type {string} */\n    let value;\n    if (typeof chunk === 'string') {\n      value = chunk;\n    } else switch (chunk) {\n      case -5:\n        {\n          value = \"\\r\";\n          break;\n        }\n      case -4:\n        {\n          value = \"\\n\";\n          break;\n        }\n      case -3:\n        {\n          value = \"\\r\" + \"\\n\";\n          break;\n        }\n      case -2:\n        {\n          value = expandTabs ? \" \" : \"\\t\";\n          break;\n        }\n      case -1:\n        {\n          if (!expandTabs && atTab) continue;\n          value = \" \";\n          break;\n        }\n      default:\n        {\n          // Currently only replacement character.\n          value = String.fromCharCode(chunk);\n        }\n    }\n    atTab = chunk === -2;\n    result.push(value);\n  }\n  return result.join('');\n}","/**\n * @import {\n *   Create,\n *   FullNormalizedExtension,\n *   InitialConstruct,\n *   ParseContext,\n *   ParseOptions\n * } from 'micromark-util-types'\n */\n\nimport { combineExtensions } from 'micromark-util-combine-extensions';\nimport { content } from './initialize/content.js';\nimport { document } from './initialize/document.js';\nimport { flow } from './initialize/flow.js';\nimport { string, text } from './initialize/text.js';\nimport * as defaultConstructs from './constructs.js';\nimport { createTokenizer } from './create-tokenizer.js';\n\n/**\n * @param {ParseOptions | null | undefined} [options]\n *   Configuration (optional).\n * @returns {ParseContext}\n *   Parser.\n */\nexport function parse(options) {\n  const settings = options || {};\n  const constructs = /** @type {FullNormalizedExtension} */\n  combineExtensions([defaultConstructs, ...(settings.extensions || [])]);\n\n  /** @type {ParseContext} */\n  const parser = {\n    constructs,\n    content: create(content),\n    defined: [],\n    document: create(document),\n    flow: create(flow),\n    lazy: {},\n    string: create(string),\n    text: create(text)\n  };\n  return parser;\n\n  /**\n   * @param {InitialConstruct} initial\n   *   Construct to start with.\n   * @returns {Create}\n   *   Create a tokenizer.\n   */\n  function create(initial) {\n    return creator;\n    /** @type {Create} */\n    function creator(from) {\n      return createTokenizer(parser, initial, from);\n    }\n  }\n}","/**\n * @import {Event} from 'micromark-util-types'\n */\n\nimport { subtokenize } from 'micromark-util-subtokenize';\n\n/**\n * @param {Array<Event>} events\n *   Events.\n * @returns {Array<Event>}\n *   Events.\n */\nexport function postprocess(events) {\n  while (!subtokenize(events)) {\n    // Empty\n  }\n  return events;\n}","/**\n * @import {Chunk, Code, Encoding, Value} from 'micromark-util-types'\n */\n\n/**\n * @callback Preprocessor\n *   Preprocess a value.\n * @param {Value} value\n *   Value.\n * @param {Encoding | null | undefined} [encoding]\n *   Encoding when `value` is a typed array (optional).\n * @param {boolean | null | undefined} [end=false]\n *   Whether this is the last chunk (default: `false`).\n * @returns {Array<Chunk>}\n *   Chunks.\n */\n\nconst search = /[\\0\\t\\n\\r]/g;\n\n/**\n * @returns {Preprocessor}\n *   Preprocess a value.\n */\nexport function preprocess() {\n  let column = 1;\n  let buffer = '';\n  /** @type {boolean | undefined} */\n  let start = true;\n  /** @type {boolean | undefined} */\n  let atCarriageReturn;\n  return preprocessor;\n\n  /** @type {Preprocessor} */\n  // eslint-disable-next-line complexity\n  function preprocessor(value, encoding, end) {\n    /** @type {Array<Chunk>} */\n    const chunks = [];\n    /** @type {RegExpMatchArray | null} */\n    let match;\n    /** @type {number} */\n    let next;\n    /** @type {number} */\n    let startPosition;\n    /** @type {number} */\n    let endPosition;\n    /** @type {Code} */\n    let code;\n    value = buffer + (typeof value === 'string' ? value.toString() : new TextDecoder(encoding || undefined).decode(value));\n    startPosition = 0;\n    buffer = '';\n    if (start) {\n      // To do: `markdown-rs` actually parses BOMs (byte order mark).\n      if (value.charCodeAt(0) === 65279) {\n        startPosition++;\n      }\n      start = undefined;\n    }\n    while (startPosition < value.length) {\n      search.lastIndex = startPosition;\n      match = search.exec(value);\n      endPosition = match && match.index !== undefined ? match.index : value.length;\n      code = value.charCodeAt(endPosition);\n      if (!match) {\n        buffer = value.slice(startPosition);\n        break;\n      }\n      if (code === 10 && startPosition === endPosition && atCarriageReturn) {\n        chunks.push(-3);\n        atCarriageReturn = undefined;\n      } else {\n        if (atCarriageReturn) {\n          chunks.push(-5);\n          atCarriageReturn = undefined;\n        }\n        if (startPosition < endPosition) {\n          chunks.push(value.slice(startPosition, endPosition));\n          column += endPosition - startPosition;\n        }\n        switch (code) {\n          case 0:\n            {\n              chunks.push(65533);\n              column++;\n              break;\n            }\n          case 9:\n            {\n              next = Math.ceil(column / 4) * 4;\n              chunks.push(-2);\n              while (column++ < next) chunks.push(-1);\n              break;\n            }\n          case 10:\n            {\n              chunks.push(-4);\n              column = 1;\n              break;\n            }\n          default:\n            {\n              atCarriageReturn = true;\n              column = 1;\n            }\n        }\n      }\n      startPosition = endPosition + 1;\n    }\n    if (end) {\n      if (atCarriageReturn) chunks.push(-5);\n      if (buffer) chunks.push(buffer);\n      chunks.push(null);\n    }\n    return chunks;\n  }\n}"],"names":["clarity","init","projectId","injectScript","c","l","a","r","t","y","window","document","getElementById","q","push","arguments","createElement","async","src","id","getElementsByTagName","parentNode","insertBefore","error","setTag","key","value","identify","customerId","customSessionId","customPageId","friendlyName","consent","upgrade","reason","event","eventName","content","tokenize","effects","previous","contentStart","attempt","parser","constructs","contentInitial","code","consume","enter","exit","micromark_factory_space","f","lineStart","token","contentType","next","data","micromark_util_character","Ch","document_document","childFlow","childToken","lineStartOffset","self","stack","continued","start","length","item","containerState","continuation","documentContinue","checkNewContainers","_closeFlow","point","undefined","closeFlow","indexBeforeExits","events","indexBeforeFlow","type","end","exitContainers","index","micromark_util_chunked","d","slice","documentContinued","currentConstruct","concrete","flowStart","interrupt","Boolean","_gfmTableDynamicInterruptHack","check","containerConstruct","thereIsANewContainer","thereIsNoNewContainer","lazy","now","line","offset","containerContinue","flow","_tokenizer","flowContinue","writeToChild","endOfFile","stream","sliceStream","defineSkip","write","seen","size","entry","call","ok","nok","disable","null","includes","initial","blank_line","w","flowInitial","afterConstruct","lib_content","k","resolver","resolveAll","createResolver","string","initializeFactory","text_text","field","resolveAllLineSuffixes","text","notText","atBreak","list","extraResolver","context","splice","eventIndex","tabs","chunks","bufferIndex","chunk","charCodeAt","_contentTypeTextTrailing","_bufferIndex","_index","column","Object","assign","constructs_document","p","block_quote","m","definition","D","code_indented","S","constructs_flow","heading_atx","Z","thematic_break","C","setext_underline","html_flow","P","code_fenced","_","constructs_string","character_reference","u","character_escape","L","constructs_text","line_ending","g","label_start_image","attention","v","autolink","j","html_text","label_start_link","F","hard_break_escape","R","label_end","code_text","h","insideSpan","attentionMarkers","parse","options","micromark_util_combine_extensions","W","constructs_namespaceObject","settings","extensions","create","defined","from","createTokenizer","initialize","columnStart","resolveAllConstructs","constructFactory","construct","info","addResult","onsuccessfulcheck","accountForPotentialSkip","fields","pop","sliceSerialize","expandTabs","serializeChunks","atTab","result","String","fromCharCode","join","V","main","chunkIndex","go","state","micromark_util_resolve_all","sliceChunks","view","startIndex","startBufferIndex","endIndex","endBufferIndex","head","shift","restore","onreturn","returnState","bogusState","listOfConstructs","constructIndex","Array","isArray","handleListOfConstructs","left","map","all","handleConstruct","store","startPoint","startPrevious","startCurrentConstruct","startEventsIndex","startStack","partial","name","resolve","resolveTo","postprocess","micromark_util_subtokenize__WEBPACK_IMPORTED_MODULE_0__","search","preprocess","atCarriageReturn","buffer","encoding","match","startPosition","endPosition","toString","TextDecoder","decode","lastIndex","exec","Math","ceil"],"sourceRoot":""}